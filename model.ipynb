{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1OC5IMqG59dZ9zl4TEVfI4UcmIZQIDOKl",
      "authorship_tag": "ABX9TyMbnXldzIwwbza2LPJiBept",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jellyXuuuuu/CovidNetDeepLearning/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t_69Gk24AUc",
        "outputId": "0e665937-d8ce-4d05-9d8a-e96d31211c07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 85 kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.19.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.50.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.38.4)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.1.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print( tf.__version__ )"
      ],
      "metadata": {
        "id": "UPEQR19K4EMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7800687-be98-4289-ff7f-dc01aa0f452d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# def"
      ],
      "metadata": {
        "id": "zLDhHe5f3ljR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GQFVCh963c-t"
      },
      "outputs": [],
      "source": [
        "def read_graph_from_ckpt(ckpt_path,input_names,output_name ):   \n",
        "    saver = tf.train.import_meta_graph(ckpt_path+'.meta',clear_devices=True)\n",
        "    graph = tf.get_default_graph()\n",
        "    with tf.Session( graph=graph) as sess:\n",
        "        sess.run(tf.global_variables_initializer()) \n",
        "        saver.restore(sess,ckpt_path) \n",
        "        output_tf =graph.get_tensor_by_name(output_name) \n",
        "        pb_graph = tf.graph_util.convert_variables_to_constants( sess, graph.as_graph_def(), [output_tf.op.name]) \n",
        "     \n",
        "    with tf.Graph().as_default() as g:\n",
        "        tf.import_graph_def(pb_graph, name='')  \n",
        "    with tf.Session(graph=g) as sess:\n",
        "        OPS=get_ops_from_pb(g,input_names,output_name)\n",
        "    return OPS\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_graph_from_pb(tf_model_path ,input_names,output_name):  \n",
        "    with open(tf_model_path, 'rb') as f:\n",
        "        serialized = f.read() \n",
        "    tf.reset_default_graph()\n",
        "    gdef = tf.GraphDef()\n",
        "    gdef.ParseFromString(serialized) \n",
        "    with tf.Graph().as_default() as g:\n",
        "        tf.import_graph_def(gdef, name='') \n",
        "    \n",
        "    with tf.Session(graph=g) as sess: \n",
        "        OPS=get_ops_from_pb(g,input_names,output_name)\n",
        "    return OPS"
      ],
      "metadata": {
        "id": "c5s06XHm3og_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ops_from_pb(graph,input_names,output_name,save_ori_network=True):\n",
        "    if save_ori_network:\n",
        "        with open('ori_network.txt','w+') as w: \n",
        "            OPS=graph.get_operations()\n",
        "            for op in OPS:\n",
        "                txt = str([v.name for v in op.inputs])+'---->'+op.type+'--->'+str([v.name for v in op.outputs])\n",
        "                w.write(txt+'\\n') \n",
        "    inputs_tf = [graph.get_tensor_by_name(input_name) for input_name in input_names]\n",
        "    output_tf =graph.get_tensor_by_name(output_name) \n",
        "    OPS =get_ops_from_inputs_outputs(graph, inputs_tf,[output_tf] ) \n",
        "    with open('network.txt','w+') as w: \n",
        "        for op in OPS:\n",
        "            txt = str([v.name for v in op.inputs])+'---->'+op.type+'--->'+str([v.name for v in op.outputs])\n",
        "            w.write(txt+'\\n') \n",
        "    OPS = sort_ops(OPS)\n",
        "    OPS = merge_layers(OPS)\n",
        "    return OPS"
      ],
      "metadata": {
        "id": "cQcVXB2q35fU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# from github"
      ],
      "metadata": {
        "id": "SmaQD5Dv4r3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# util.py\n",
        "\n",
        "def create_graph(ops):\n",
        "  \n",
        "  n = len(ops)\n",
        "  G = [[] for i in range(n)]\n",
        "  op_name_to_index = dict() \n",
        "  for i, op in enumerate(ops):\n",
        "    op_name_to_index[op.name] = i \n",
        "  for i, op in enumerate(ops):\n",
        "    for inp in op.inputs:\n",
        "      G[op_name_to_index[inp.op.name]].append(i)\n",
        "\n",
        "  return G \n",
        "def push_stack(stack, node, in_stack,ops):\n",
        "  stack.append(node)\n",
        "  if node in in_stack:\n",
        "    print('cycles---->',ops[node])\n",
        "    raise ValueError('Graph has cycles.')\n",
        "  else:\n",
        "    in_stack[node] = True\n",
        "\n",
        "def get_unvisited_child(G, node, not_visited):\n",
        "  for child in G[node]:\n",
        "    if child in not_visited:\n",
        "      return child\n",
        "  return -1\n",
        "\n",
        " \n",
        "#对计算节点排序，使得每个计算节点所依赖的计算节点在前面\n",
        "def  sort_ops(ops):  \n",
        "\n",
        "  G = create_graph(ops)\n",
        "  n = len(ops) \n",
        "  topological_label = [-1 for i in range(n)]\n",
        "  stack = []\n",
        "  in_stack = dict()\n",
        "  not_visited = dict.fromkeys([i for i in range(n)])\n",
        "  label_counter = n-1\n",
        "\n",
        "  while len(not_visited) > 0:\n",
        "    node = list(not_visited.keys())[0]\n",
        "    push_stack(stack, node, in_stack,ops)\n",
        "    while len(stack) > 0:\n",
        "      node = get_unvisited_child(G, stack[-1], not_visited)\n",
        "      if node != -1:\n",
        "        push_stack(stack, node, in_stack,ops)\n",
        "      else:\n",
        "        node = stack.pop()\n",
        "        in_stack.pop(node)\n",
        "        not_visited.pop(node)\n",
        "        topological_label[node] = label_counter\n",
        "        label_counter -= 1\n",
        "\n",
        "  return [x for _, x in sorted(zip(topological_label, ops))]\n",
        " \n",
        "def _get_ops_in_path(from_tensors,to_tensors,ops):\n",
        "    invalid_ops=[]\n",
        "    valid_ops=ops.copy()\n",
        "    removed=[]\n",
        "    find_invalid=True\n",
        "    in_tensors = [tensor.name for tensor in from_tensors] \n",
        "    out_tensors = [tensor.name for tensor in to_tensors]\n",
        "    while find_invalid:\n",
        "      find_invalid=False\n",
        "      for op in valid_ops:  \n",
        "          for input in op.inputs:\n",
        "              if input.name in out_tensors:\n",
        "                out_tensors=out_tensors+[out.name for out in op.outputs]\n",
        "                find_invalid=True\n",
        "                break\n",
        "          for output in op.outputs:\n",
        "              if output.name in in_tensors:\n",
        "                in_tensors=in_tensors+[input.name for input in op.inputs]\n",
        "                find_invalid=True\n",
        "                break\n",
        "          if find_invalid:\n",
        "             invalid_ops.append(op)\n",
        "    \n",
        "      valid_ops=[op for op in valid_ops if not op in invalid_ops]       \n",
        "    print('inputs===========================')\n",
        "    print([op.type for op in invalid_ops if 'Con' in op.name])\n",
        "    print('end inputs========================')\n",
        "    print(out_tensors)\n",
        "    return valid_ops"
      ],
      "metadata": {
        "id": "wOyp10pZ4lV9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nodeObj.py\n",
        "\n",
        "class OPNode:\n",
        "    def __init__(self,node):\n",
        "        if not node is None:\n",
        "            self.node=node \n",
        "            self.type=node.type\n",
        "            self.inputs=node.inputs\n",
        "            self.outputs=node.outputs\n",
        "            self.name = node.name \n",
        "    @classmethod\n",
        "    def my_init(cls,type,inputs,outputs,name):\n",
        "        thiz = cls(None)\n",
        "        thiz.node=None\n",
        "        thiz.type= type\n",
        "        thiz.inputs= inputs\n",
        "        thiz.outputs= outputs\n",
        "        thiz.name = name \n",
        "        return thiz\n",
        "    def __eq__(self,obj):\n",
        "        if obj is None:\n",
        "            return False\n",
        "        if obj.name:\n",
        "            return self.name== obj.name\n",
        "        return False\n",
        "    def __hash__(self):\n",
        "        return self.node.__hash__()\n",
        "class TSNode:\n",
        "    def __init__(self,node,op_node):\n",
        "        self.node=node  \n",
        "        self.op=op_node \n",
        "        self.dtype=node.dtype\n",
        "        self.name = node.name \n",
        "        self.shape=node.shape \n",
        "        self.get_shape=node.get_shape\n",
        "        self.next_ops=node.consumers()\n",
        "   \n",
        "    def consumers(self):\n",
        "        return self.next_ops      \n",
        "    def eval(self):\n",
        "        return self.node.eval()\n",
        "    def __eq__(self,obj):\n",
        "        if obj is None:\n",
        "            return False\n",
        "        if obj.name:\n",
        "            return self.name== obj.name\n",
        "        return False\n",
        "    def __hash__(self):\n",
        "        return self.node.__hash__()"
      ],
      "metadata": {
        "id": "ePeGv9r343YD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph_builder.py\n",
        "\n",
        "# from NodeObj import OPNode,TSNode\n",
        "class GNode:\n",
        "    def __init__(self,id,node):\n",
        "        self.node=node\n",
        "        self.name=node.name\n",
        "        self.id=id\n",
        "        self.pre_ids=None\n",
        "        self.next_ids=None\n",
        "        if isinstance(node,OPNode):\n",
        "            self.type=node.type\n",
        "            self.pre=node.inputs\n",
        "            self.next=node.outputs\n",
        "        else:\n",
        "            self.pre=[node.op]\n",
        "            self.next=node.consumers()\n",
        "            self.shape=node.shape\n",
        "    \n",
        "    def __str__(self):\n",
        "        if isinstance(self.node,OPNode):\n",
        "            return 'op☯'+str(self.id)+'☯'+self.type+'☯'+self.name+'☯'\\\n",
        "                   +' '.join([str(id) for id in self.pre_ids])+'☯'\\\n",
        "                   +' '.join([str(id) for id in self.next_ids])\n",
        "        else:\n",
        "             \n",
        "            return 'ts☯'+str(self.id)+'☯'+str(self.shape)+'☯'+self.name+'☯'\\\n",
        "                   +' '.join([str(id) for id in self.pre_ids])+'☯'\\\n",
        "                   +' '.join([str(id) for id in self.next_ids])\n",
        "class GraphBuilder:\n",
        "    def __init__(self,file_dst):\n",
        "        self.file_dst=file_dst  \n",
        "        self.layers=[]\n",
        "        self.nodes_in_layers=set()\n",
        "        self.node_map=dict()\n",
        "        self.id=0\n",
        "    def __next_id(self):\n",
        "        self.id=self.id+1\n",
        "        return self.id \n",
        "    def __add_node_at(self,g_node,layer_idx):\n",
        "       \n",
        "        max_layer=len(self.layers)\n",
        "        if layer_idx>=max_layer:\n",
        "            self.layers.append([g_node])\n",
        "        else:\n",
        "            nodes=self.layers[layer_idx]\n",
        "            nodes.append(g_node) \n",
        "        self.nodes_in_layers.add(g_node.name)\n",
        "    def __to_gnode(self,nodes):\n",
        "        gnodes=[]\n",
        "        for node in nodes:\n",
        "            g_node=self.node_map.get(node.name,None)\n",
        "            if g_node is None:\n",
        "                id=self.__next_id()\n",
        "                g_node=GNode(id,node)\n",
        "                self.node_map[node.name]=g_node\n",
        "            gnodes.append(g_node)\n",
        "        return gnodes\n",
        "    def __max_ts_idx(self,ts_list):\n",
        "        max_idx=-1\n",
        "        for ts in ts_list:\n",
        "            for idx,nodes in enumerate(self.layers):\n",
        "                if ts in nodes and (idx>max_idx):\n",
        "                    max_idx=idx\n",
        "        return max_idx\n",
        "    def __mv_node_to_layer(self,gnode,layer_idx):\n",
        "        for idx,nodes in enumerate(self.layers):\n",
        "            if gnode in nodes:\n",
        "                if not idx==layer_idx:\n",
        "                   nodes.remove(gnode)\n",
        "                   self.layers[layer_idx].append(gnode)\n",
        "                break\n",
        "    def __print_layers(self):\n",
        "        for idx,layer in enumerate(self.layers):\n",
        "            dnames=[]\n",
        "            for node in layer:\n",
        "                dnames.append(node.name.split('/')[-1])\n",
        "            print('layer-->',idx,','.join(dnames))          \n",
        "    def add_op(self,op_node):\n",
        "         \n",
        "        inputs = self.__to_gnode(op_node.inputs)\n",
        "        op_gnode = self.__to_gnode([op_node])[0]\n",
        "        outputs = self.__to_gnode(op_node.outputs)\n",
        "        max_idx=self.__max_ts_idx(inputs)\n",
        "        # print('max_idx-->',max_idx)\n",
        "        \n",
        "        if max_idx<0:\n",
        "            for input in inputs:\n",
        "                self.__add_node_at(input,0)\n",
        "            self.__add_node_at(op_gnode,1)\n",
        "            op_idx=1\n",
        "        else:\n",
        "            op_idx=max_idx+1\n",
        "            self.__add_node_at(op_gnode,max_idx+1)\n",
        "            for input in inputs: \n",
        "                if input.name in self.nodes_in_layers:#已经添加过\n",
        "                    continue\n",
        "                else:\n",
        "                    self.__add_node_at(input,max_idx)\n",
        "                     \n",
        "        if len(outputs)>0:\n",
        "            output=outputs[0]\n",
        "            if not output.name in self.nodes_in_layers:#未添加 \n",
        "                self.__add_node_at(output,op_idx+1)\n",
        "                \n",
        "        # print('==============================',len(outputs))\n",
        "        # self.__print_layers()\n",
        "    def __layers_to_str(self):\n",
        "        str_layers=[]\n",
        "        for layer in self.layers: \n",
        "            layer = [str(l) for l in layer]\n",
        "            str_layers.append('&'.join(layer))\n",
        "        return '卍'.join(str_layers)\n",
        "    def __set_ids(self):\n",
        "        for layer in self.layers:\n",
        "            for node in layer:\n",
        "                node.pre_ids=[]\n",
        "                node.next_ids=[]\n",
        "                for pre in node.pre:\n",
        "                    gnode = self.node_map.get(pre.name,None)\n",
        "                    if not gnode is None:\n",
        "                        node.pre_ids.append(gnode.id)\n",
        "                for next in node.next:\n",
        "                    gnode = self.node_map.get(next.name,None)\n",
        "                    if not gnode is None:\n",
        "                        node.next_ids.append(gnode.id)\n",
        "    def build(self):  \n",
        "        # self.__print_layers()\n",
        "        with open('html/show_graph.html','r',encoding='utf-8') as r:\n",
        "            tmp_html=r.read()\n",
        "        with open('html/NodeObj.js','r',encoding='utf-8') as r:\n",
        "            js1=r.read()\n",
        "        with open('html/ShowGraph.js','r',encoding='utf-8') as r:\n",
        "            js2=r.read()\n",
        "        self.__set_ids()\n",
        "        data=self.__layers_to_str()\n",
        "        data = '<script type=\"text/javascript\">data=\"'+data+'\";</script>'\n",
        "        with open(self.file_dst,'w+',encoding='utf-8') as w:\n",
        "            w.write(data+'\\n'+tmp_html)\n",
        "            w.write( '\\n<script type=\"text/javascript\">'+js1+'</script>')\n",
        "            w.write( '\\n<script type=\"text/javascript\">'+js2+'</script>') "
      ],
      "metadata": {
        "id": "CZnabHKw5Dqg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge_layers.py\n",
        "\n",
        "# from NodeObj import OPNode \n",
        "def get_dropout_op_from(ops,start_dropout_op): \n",
        "    op_queue=[start_dropout_op]\n",
        "    do_ops=set() \n",
        "    while len(op_queue)>0:\n",
        "        op = op_queue.pop(0) \n",
        "        for inp in op.inputs:\n",
        "            if (inp.op in ops) and ('dropout' in inp.op.name) and (not inp.op in do_ops):\n",
        "                op_queue.append(inp.op) \n",
        "        for output in op.outputs:\n",
        "            for next_op in output.consumers(): \n",
        "                if (next_op in ops) and ('dropout' in next_op.name) and (not next_op in do_ops): \n",
        "                    op_queue.append(next_op)\n",
        "        do_ops.add(op) \n",
        "    return   do_ops  \n",
        "\n",
        "def merge_dropout_ops(sorted_ops,dropout_ops):\n",
        "    dropout_idx=0\n",
        "    merged_dropout=[]\n",
        "    dp_input_ops=[] \n",
        "    for dropout in dropout_ops:\n",
        "        dropout_input_tensor=set()\n",
        "        dropout_output_tensor=set()\n",
        "        input_op  = set()\n",
        "        output_op= set()\n",
        "        keep_prob=None \n",
        "        for dp in dropout:\n",
        "            if 'keep_prob' in dp.name:\n",
        "                keep_prob=dp\n",
        "            for input in  dp.inputs:\n",
        "                if input.op in sorted_ops and (not input.op in dropout):\n",
        "                    dropout_input_tensor.add(input)\n",
        "                    input_op.add(dp)\n",
        "            for output in dp.outputs:\n",
        "                for op in output.consumers():\n",
        "                    if op in sorted_ops and (not op in dropout):\n",
        "                        dropout_output_tensor.add(output)  \n",
        "                        output_op.add(dp)\n",
        "        name = 'Merged_dropout:'+str(dropout_idx)+'_'+str(keep_prob.outputs[0].eval())\n",
        "        dp_op = OPNode.my_init('Dropout',list(dropout_input_tensor),list(dropout_output_tensor) , name)  \n",
        "        for inp in dropout_input_tensor:\n",
        "            consumer=set()\n",
        "            for c_op in inp.consumers():\n",
        "                if c_op.name in [op.name for op in input_op]:\n",
        "                   consumer.add(dp_op)\n",
        "                else:\n",
        "                    consumer.add(c_op)\n",
        "        for output in dropout_output_tensor:\n",
        "            output.op=dp_op\n",
        "        dropout_idx=dropout_idx+1  \n",
        "        merged_dropout.append(dp_op)\n",
        "        dp_input_ops.append(input_op) \n",
        "    return merged_dropout,dp_input_ops\n",
        "def get_dropout_op_index(dp_op,dp_input_ops):\n",
        "    for idx,dp_set in enumerate(dp_input_ops):\n",
        "        if dp_op in dp_set:\n",
        "            return idx\n",
        "    return -1\n",
        "def merge_dropout(sorted_ops):\n",
        "    visited_op_name=set()\n",
        "    dropout_ops = []\n",
        "    for op in sorted_ops:\n",
        "        if op.name in visited_op_name: \n",
        "            continue\n",
        "        if 'dropout' in op.name:\n",
        "            do_ops = get_dropout_op_from(sorted_ops,op)\n",
        "            for v_op in do_ops:\n",
        "                visited_op_name.add(v_op.name)\n",
        "            dropout_ops.append( do_ops )\n",
        "    merged_dp_ops,dp_input_ops = merge_dropout_ops(sorted_ops,dropout_ops)\n",
        "    new_sorted_ops=[]\n",
        "    visited_dropout_idx=set()\n",
        "    for op in sorted_ops:\n",
        "        if 'dropout' in op.name:\n",
        "            idx = get_dropout_op_index(op, dp_input_ops)\n",
        "            if idx>=0 and (not idx in visited_dropout_idx):\n",
        "                dp_op = merged_dp_ops[idx]\n",
        "                new_sorted_ops.append(dp_op)\n",
        "                visited_dropout_idx.add(idx)\n",
        "        else:\n",
        "            new_sorted_ops.append(op)\n",
        "    return new_sorted_ops\n",
        "def merge_identity_const(sorted_ops):\n",
        "    new_sorted_ops=[]\n",
        "    for op in sorted_ops:\n",
        "        if op.type=='Const':#忽略Const\n",
        "            continue\n",
        "        if op.type=='Identity':#去掉Identity  \n",
        "            op.outputs[0].name=op.inputs[0].name \n",
        "            continue \n",
        "        new_sorted_ops.append(op)\n",
        "    return new_sorted_ops\n",
        "def get_squeeze_end_op(sorted_ops,start_squeeze_op):\n",
        "    ops_in_path=set()\n",
        "    end_op=None \n",
        "    flag=False\n",
        "    for op in sorted_ops:\n",
        "        if op == start_squeeze_op:\n",
        "            flag=True\n",
        "        if flag:\n",
        "            ops_in_path.add(op)\n",
        "            if op.type=='Reshape':\n",
        "                end_op=op\n",
        "                break \n",
        "    visited_ops=set() \n",
        "    if not end_op is None:\n",
        "        ops_in_path.remove(start_squeeze_op)\n",
        "        op_queue=list(ops_in_path)\n",
        "       \n",
        "        while len(op_queue)>0:\n",
        "            op =op_queue.pop(0) \n",
        "            for inp in op.inputs:\n",
        "                if inp.op !=start_squeeze_op and (not inp.op in visited_ops):\n",
        "                    op_queue.append(inp.op)\n",
        "            visited_ops.add(op)\n",
        "    print(len(ops_in_path))\n",
        "    return end_op,visited_ops\n",
        "def merge_squeeze(sorted_ops):\n",
        "    new_sorted_ops=[]\n",
        "    need_remove_ops=set()\n",
        "    for op in sorted_ops:\n",
        "        if op.type=='Squeeze':\n",
        "           end_op,need_removed = get_squeeze_end_op(sorted_ops,op)\n",
        "           if not end_op is None: \n",
        "               ts = end_op.outputs[0]\n",
        "               ts.op=op\n",
        "               op.outputs=[ts]\n",
        "               need_remove_ops|=need_removed\n",
        "        new_sorted_ops.append(op) \n",
        "    new_sorted_ops=[op for op in new_sorted_ops if not op in need_remove_ops]\n",
        "    return new_sorted_ops\n",
        "def merge_layers(sorted_ops):\n",
        "    sorted_ops = merge_dropout(sorted_ops)\n",
        "    sorted_ops = merge_identity_const(sorted_ops)\n",
        "    sorted_ops = merge_squeeze(sorted_ops)\n",
        "    return sorted_ops"
      ],
      "metadata": {
        "id": "vWz8TAE85KfM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read_graph.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "# from utils import sort_ops ,_get_ops_in_path\n",
        "# from GraphBuilder import GraphBuilder\n",
        "# from MergeLayers import merge_layers\n",
        "# from NodeObj import OPNode,TSNode\n",
        "def remove_identity_const(sorted_ops):\n",
        "    ops=[]\n",
        "    for op in sorted_ops:\n",
        "        if op.type=='Const':\n",
        "            continue\n",
        "        elif op.type=='Identity':\n",
        "            output = op.outputs[0]\n",
        "            output.identity_from=op.inputs[0]\n",
        "            sorted_ops.remove(op)\n",
        "            # print('--->',output.identity_from)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return sorted_ops\n",
        "def read_graph_from_pb(tf_model_path ,input_names,output_name):  \n",
        "    with open(tf_model_path, 'rb') as f:\n",
        "        serialized = f.read() \n",
        "    tf.reset_default_graph()\n",
        "    gdef = tf.GraphDef()\n",
        "    gdef.ParseFromString(serialized) \n",
        "    with tf.Graph().as_default() as g:\n",
        "        tf.import_graph_def(gdef, name='') \n",
        "    \n",
        "    with tf.Session(graph=g) as sess: \n",
        "        OPS=get_ops_from_pb(g,input_names,output_name)\n",
        "    return OPS\n",
        " \n",
        "def remove_ops_before_inputs(inputs,ops):\n",
        "    tensor_queue=inputs.copy()  \n",
        "    visited_ts=set()\n",
        "    invalid_ops=set()\n",
        "    while len(tensor_queue)>0:\n",
        "        ts = tensor_queue.pop(0) \n",
        "        if not ts.op in invalid_ops: \n",
        "            invalid_ops.add(ts.op)\n",
        "            tensor_queue=tensor_queue+[inp for inp in  ts.op.inputs if not inp in visited_ts]  \n",
        "        visited_ts.add(ts) \n",
        "    ops = [op for op in ops if not op in invalid_ops]\n",
        "    ops = get_connected_ops(ops,inputs) \n",
        "    return ops\n",
        "def get_connected_ops(ops_set,start_tensors):\n",
        "    visited_ts = set()\n",
        "    visited_ops=set()\n",
        "    ts_queue=start_tensors \n",
        "    while len(ts_queue)>0:\n",
        "        ts = ts_queue.pop(0) \n",
        "        if ts.op in ops_set:\n",
        "            visited_ops.add(ts.op)\n",
        "            ts_queue=ts_queue+[input for input in ts.op.inputs if not input in visited_ts]\n",
        "        for op in ts.consumers():\n",
        "            if op in ops_set:\n",
        "                visited_ops.add(op) \n",
        "                ts_queue=ts_queue+[output for output in op.outputs if not output in visited_ts]\n",
        "        visited_ts.add(ts)\n",
        "    ops = [op for op in ops_set if op in visited_ops] \n",
        "    return ops\n",
        " \n",
        "def ops_to_OPNodes(ops,inputs):\n",
        "\n",
        "    ops_map = dict()\n",
        "    ts_set=set()\n",
        "    ts_map=dict()\n",
        "   \n",
        "    for op in ops:\n",
        "        op_node = OPNode(op)\n",
        "        ops_map[op]= op_node\n",
        "        ts_set |=set([ts for ts in op.inputs])\n",
        "        ts_set |=set([ts for ts in op.outputs])\n",
        "    for ts in ts_set:\n",
        "        ts_map[ts]=TSNode(ts,None) \n",
        "    for op,op_node in ops_map.items(): \n",
        "        inps=[]\n",
        "        for inp in op.inputs:#修改节点输入\n",
        "            inp = ts_map[inp]\n",
        "            inps.append(inp)\n",
        "        op_node.inputs=inps\n",
        "        outputs=[]\n",
        "        for output in op.outputs:#修改节点输出\n",
        "            output = ts_map[output]\n",
        "            outputs.append(output)\n",
        "        op_node.outputs=outputs\n",
        "    for ts,ts_node in ts_map.items():\n",
        "        consumers=[] \n",
        "        for op in ts.consumers():\n",
        "            if op in ops:\n",
        "                consumers.append(ops_map[op]) \n",
        "        ts_node.next_ops=consumers \n",
        "        ts_node.op = ops_map.get(ts.op,None)\n",
        "        if ts_node.op==None:\n",
        "            print('---->',ts_node.name)\n",
        "    print(inputs)\n",
        "    #将inputs用placeholder替换\n",
        "    replace_input=dict() \n",
        "    for input in inputs:#将input映射placeholder \n",
        "        # if not input.op.type=='Placeholder':\n",
        "        input_shape = input.get_shape()\n",
        "        if input_shape==None:\n",
        "            input_shape=[None,None,None,None]\n",
        "        ph = tf.placeholder(input.dtype,input_shape)\n",
        "        print(ph.get_shape())\n",
        "        replace_input[input.name] = ph\n",
        "        ph_node = OPNode(ph.op) \n",
        "        ops_map[ph.op]=ph_node\n",
        "   \n",
        "    for op,op_node in ops_map.items():\n",
        "        new_inputs=[] \n",
        "        for input in op_node.inputs:\n",
        "            input = replace_input.get(input.name,input) #placeholder output\n",
        "            new_inputs.append(input) \n",
        "        op_node.inputs=new_inputs \n",
        "        \n",
        "    return ops_map.values() \n",
        "\n",
        "def get_ops_from_inputs_outputs(graph, inputs,outputs):\n",
        "    ops = graph.get_operations() \n",
        "    ops=remove_ops_before_inputs(inputs.copy(),ops)\n",
        "    ops = ops_to_OPNodes(ops,inputs)\n",
        "    return ops\n",
        "def get_ops_from_pb(graph,input_names,output_name,save_ori_network=True):\n",
        "    if save_ori_network:\n",
        "        with open('ori_network.txt','w+') as w: \n",
        "            OPS=graph.get_operations()\n",
        "            for op in OPS:\n",
        "                txt = str([v.name for v in op.inputs])+'---->'+op.type+'--->'+str([v.name for v in op.outputs])\n",
        "                w.write(txt+'\\n') \n",
        "    inputs_tf = [graph.get_tensor_by_name(input_name) for input_name in input_names]\n",
        "    output_tf =graph.get_tensor_by_name(output_name) \n",
        "    OPS =get_ops_from_inputs_outputs(graph, inputs_tf,[output_tf] ) \n",
        "    with open('network.txt','w+') as w: \n",
        "        for op in OPS:\n",
        "            txt = str([v.name for v in op.inputs])+'---->'+op.type+'--->'+str([v.name for v in op.outputs])\n",
        "            w.write(txt+'\\n') \n",
        "    OPS = sort_ops(OPS)\n",
        "    OPS = merge_layers(OPS)\n",
        "    return OPS\n",
        "def read_graph_from_ckpt(ckpt_path,input_names,output_name ):   \n",
        "    saver = tf.train.import_meta_graph(ckpt_path+'.meta',clear_devices=True)\n",
        "    graph = tf.get_default_graph()\n",
        "    with tf.Session( graph=graph) as sess:\n",
        "        sess.run(tf.global_variables_initializer()) \n",
        "        saver.restore(sess,ckpt_path) \n",
        "        output_tf =graph.get_tensor_by_name(output_name) \n",
        "        pb_graph = tf.graph_util.convert_variables_to_constants( sess, graph.as_graph_def(), [output_tf.op.name]) \n",
        "     \n",
        "    with tf.Graph().as_default() as g:\n",
        "        tf.import_graph_def(pb_graph, name='')  \n",
        "    with tf.Session(graph=g) as sess:\n",
        "        OPS=get_ops_from_pb(g,input_names,output_name)\n",
        "    return OPS\n",
        "\n",
        "def gen_graph(ops,html_dst):\n",
        "    gb = GraphBuilder(html_dst) \n",
        "    for op in ops:\n",
        "        if not len(op.outputs)>0:\n",
        "            continue  \n",
        "        if(op.type=='Placeholder'):\n",
        "            continue\n",
        "        gb.add_op(op )\n",
        "    gb.build()\n",
        "def print_graph(ops):\n",
        "    \n",
        "    for op in ops:\n",
        "        output = op.outputs[0] \n",
        "        print(op.inputs,output)\n",
        "def read_graph(model_path,input_names,output_name,html_dst):\n",
        "    dir_path = os.path.dirname(html_dst)\n",
        "    if len(dir_path)>0 and not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "    if model_path.endswith('pb'):\n",
        "        ops = read_graph_from_pb( model_path ,input_names,output_name)\t\n",
        "    else:\n",
        "        ops = read_graph_from_ckpt(model_path ,input_names,output_name)\n",
        "    \n",
        "    print(dir_path)\n",
        "    gen_graph(ops,html_dst)\n",
        "# if __name__=='__main__':\n",
        "#     model_path = sys.argv[1]\n",
        "#     input_names = sys.argv[2]\n",
        "#     output_name = sys.argv[3]\n",
        "#     html_dst = sys.argv[4]\n",
        "#     input_names=input_names.split(',')\n",
        "#     read_graph(model_path,input_names,output_name,html_dst)\n",
        "  \n",
        "# read_graph('../../mobilenet_v1_1.0_192.ckpt',['batch:0'],'MobilenetV1/Predictions/Reshape_1:0','output/html_dst3.html')\n",
        "# read_graph( '../../mobilenet_v1_1.0_192_frozen.pb' ,['input:0'],'MobilenetV1/Predictions/Reshape_1:0','output/html_dst1.html')\n"
      ],
      "metadata": {
        "id": "K7KcRcCJ356o"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Io40SlE65XzP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load files"
      ],
      "metadata": {
        "id": "RTxCsowRceoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r 'drive/MyDrive/covid/models/' ."
      ],
      "metadata": {
        "id": "fNkPbQqm5oFi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r 'drive/MyDrive/covid/labels/' ."
      ],
      "metadata": {
        "id": "p0FvcwGdcb2L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r 'drive/MyDrive/covid/assets/' ."
      ],
      "metadata": {
        "id": "zu-vmP-YczuS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# operation"
      ],
      "metadata": {
        "id": "EjkcrAWS5Yqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_names = ['input_1:0']"
      ],
      "metadata": {
        "id": "PMphIzc-cc59"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_graph_from_ckpt(\"models/COVIDNet-CXR-2/model\", input_names, 'norm_dense_2/Softmax:0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePLMhlTj5b3W",
        "outputId": "7a8adf85-d128-4cc8-9e36-0b0a1a4e2ce4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From <ipython-input-10-b105af89481f>:152: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.extract_sub_graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----> input_1:0\n",
            "[<tf.Tensor 'input_1:0' shape=(?, 480, 480, 3) dtype=float32>]\n",
            "(?, 480, 480, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.OPNode at 0x7f873d120590>,\n",
              " <__main__.OPNode at 0x7f873d423e90>,\n",
              " <__main__.OPNode at 0x7f873d423910>,\n",
              " <__main__.OPNode at 0x7f873d433cd0>,\n",
              " <__main__.OPNode at 0x7f873d42db50>,\n",
              " <__main__.OPNode at 0x7f873d42d0d0>,\n",
              " <__main__.OPNode at 0x7f873d43ce10>,\n",
              " <__main__.OPNode at 0x7f873d42d550>,\n",
              " <__main__.OPNode at 0x7f873d43c590>,\n",
              " <__main__.OPNode at 0x7f873d43c310>,\n",
              " <__main__.OPNode at 0x7f873d43cd90>,\n",
              " <__main__.OPNode at 0x7f873d42b110>,\n",
              " <__main__.OPNode at 0x7f873d430b90>,\n",
              " <__main__.OPNode at 0x7f873d433a50>,\n",
              " <__main__.OPNode at 0x7f873d430710>,\n",
              " <__main__.OPNode at 0x7f873d430390>,\n",
              " <__main__.OPNode at 0x7f873d440090>,\n",
              " <__main__.OPNode at 0x7f873d4296d0>,\n",
              " <__main__.OPNode at 0x7f873d430410>,\n",
              " <__main__.OPNode at 0x7f873d42d150>,\n",
              " <__main__.OPNode at 0x7f873d42b590>,\n",
              " <__main__.OPNode at 0x7f873d43e950>,\n",
              " <__main__.OPNode at 0x7f873d42bc10>,\n",
              " <__main__.OPNode at 0x7f873d4432d0>,\n",
              " <__main__.OPNode at 0x7f873d4339d0>,\n",
              " <__main__.OPNode at 0x7f873d430f10>,\n",
              " <__main__.OPNode at 0x7f873d43e5d0>,\n",
              " <__main__.OPNode at 0x7f873d430e90>,\n",
              " <__main__.OPNode at 0x7f873d43cb10>,\n",
              " <__main__.OPNode at 0x7f873d439050>,\n",
              " <__main__.OPNode at 0x7f873d425f90>,\n",
              " <__main__.OPNode at 0x7f873d433e50>,\n",
              " <__main__.OPNode at 0x7f873d42b690>,\n",
              " <__main__.OPNode at 0x7f873d43e0d0>,\n",
              " <__main__.OPNode at 0x7f873d42bd90>,\n",
              " <__main__.OPNode at 0x7f873d4335d0>,\n",
              " <__main__.OPNode at 0x7f873d43e9d0>,\n",
              " <__main__.OPNode at 0x7f873d4292d0>,\n",
              " <__main__.OPNode at 0x7f873d437810>,\n",
              " <__main__.OPNode at 0x7f873d43c710>,\n",
              " <__main__.OPNode at 0x7f873d439350>,\n",
              " <__main__.OPNode at 0x7f873d437d10>,\n",
              " <__main__.OPNode at 0x7f873d440f10>,\n",
              " <__main__.OPNode at 0x7f873d43ebd0>,\n",
              " <__main__.OPNode at 0x7f873d42b890>,\n",
              " <__main__.OPNode at 0x7f873d439750>,\n",
              " <__main__.OPNode at 0x7f873d425090>,\n",
              " <__main__.OPNode at 0x7f873d437110>,\n",
              " <__main__.OPNode at 0x7f873d425110>,\n",
              " <__main__.OPNode at 0x7f873d429250>,\n",
              " <__main__.OPNode at 0x7f873d439a50>,\n",
              " <__main__.OPNode at 0x7f873d4396d0>,\n",
              " <__main__.OPNode at 0x7f873d4433d0>,\n",
              " <__main__.OPNode at 0x7f873d4437d0>,\n",
              " <__main__.OPNode at 0x7f873d42d450>,\n",
              " <__main__.OPNode at 0x7f873d430d90>,\n",
              " <__main__.OPNode at 0x7f873d4293d0>,\n",
              " <__main__.OPNode at 0x7f873d429a50>,\n",
              " <__main__.OPNode at 0x7f873d43e150>,\n",
              " <__main__.OPNode at 0x7f873d429050>,\n",
              " <__main__.OPNode at 0x7f873d437490>,\n",
              " <__main__.OPNode at 0x7f873d42b510>,\n",
              " <__main__.OPNode at 0x7f873d430a90>,\n",
              " <__main__.OPNode at 0x7f873d42df50>,\n",
              " <__main__.OPNode at 0x7f873d429ed0>,\n",
              " <__main__.OPNode at 0x7f873d429750>,\n",
              " <__main__.OPNode at 0x7f873d440c90>,\n",
              " <__main__.OPNode at 0x7f873d440110>,\n",
              " <__main__.OPNode at 0x7f873d433d50>,\n",
              " <__main__.OPNode at 0x7f873d440410>,\n",
              " <__main__.OPNode at 0x7f873d42ba10>,\n",
              " <__main__.OPNode at 0x7f873d440810>,\n",
              " <__main__.OPNode at 0x7f873d439bd0>,\n",
              " <__main__.OPNode at 0x7f873d443650>,\n",
              " <__main__.OPNode at 0x7f873d440b10>,\n",
              " <__main__.OPNode at 0x7f873d425210>,\n",
              " <__main__.OPNode at 0x7f873d43e1d0>,\n",
              " <__main__.OPNode at 0x7f873d42b610>,\n",
              " <__main__.OPNode at 0x7f873d439850>,\n",
              " <__main__.OPNode at 0x7f873d43c290>,\n",
              " <__main__.OPNode at 0x7f873d42ded0>,\n",
              " <__main__.OPNode at 0x7f873d443a50>,\n",
              " <__main__.OPNode at 0x7f873d43c990>,\n",
              " <__main__.OPNode at 0x7f873d430310>,\n",
              " <__main__.OPNode at 0x7f873d429d50>,\n",
              " <__main__.OPNode at 0x7f873d440190>,\n",
              " <__main__.OPNode at 0x7f873d42b990>,\n",
              " <__main__.OPNode at 0x7f873d4336d0>,\n",
              " <__main__.OPNode at 0x7f873d43c690>,\n",
              " <__main__.OPNode at 0x7f873d437b90>,\n",
              " <__main__.OPNode at 0x7f873d437090>,\n",
              " <__main__.OPNode at 0x7f873d43ec50>,\n",
              " <__main__.OPNode at 0x7f873d43ca90>,\n",
              " <__main__.OPNode at 0x7f873d425f10>,\n",
              " <__main__.OPNode at 0x7f873d42b190>,\n",
              " <__main__.OPNode at 0x7f873d430b10>,\n",
              " <__main__.OPNode at 0x7f873d443ad0>,\n",
              " <__main__.OPNode at 0x7f873d4397d0>,\n",
              " <__main__.OPNode at 0x7f873d43e8d0>,\n",
              " <__main__.OPNode at 0x7f873d439e50>,\n",
              " <__main__.OPNode at 0x7f873d437410>,\n",
              " <__main__.OPNode at 0x7f873d43e850>,\n",
              " <__main__.OPNode at 0x7f873d4331d0>,\n",
              " <__main__.OPNode at 0x7f873d433250>,\n",
              " <__main__.OPNode at 0x7f873d42dc50>,\n",
              " <__main__.OPNode at 0x7f873d433550>,\n",
              " <__main__.OPNode at 0x7f873d42dcd0>,\n",
              " <__main__.OPNode at 0x7f873d43e650>,\n",
              " <__main__.OPNode at 0x7f873d440510>,\n",
              " <__main__.OPNode at 0x7f873d433dd0>,\n",
              " <__main__.OPNode at 0x7f873d42dfd0>,\n",
              " <__main__.OPNode at 0x7f873d439ad0>,\n",
              " <__main__.OPNode at 0x7f873d439ed0>,\n",
              " <__main__.OPNode at 0x7f873d440890>,\n",
              " <__main__.OPNode at 0x7f873d42b910>,\n",
              " <__main__.OPNode at 0x7f873d42d3d0>,\n",
              " <__main__.OPNode at 0x7f873d433650>,\n",
              " <__main__.OPNode at 0x7f873d443050>,\n",
              " <__main__.OPNode at 0x7f873d430690>,\n",
              " <__main__.OPNode at 0x7f873d440390>,\n",
              " <__main__.OPNode at 0x7f873d440e90>,\n",
              " <__main__.OPNode at 0x7f873d437190>,\n",
              " <__main__.OPNode at 0x7f873d43c390>,\n",
              " <__main__.OPNode at 0x7f873d4332d0>,\n",
              " <__main__.OPNode at 0x7f873d430e10>,\n",
              " <__main__.OPNode at 0x7f873d42d8d0>,\n",
              " <__main__.OPNode at 0x7f873d4299d0>,\n",
              " <__main__.OPNode at 0x7f873d443bd0>,\n",
              " <__main__.OPNode at 0x7f873d425990>,\n",
              " <__main__.OPNode at 0x7f873d437f90>,\n",
              " <__main__.OPNode at 0x7f873d440c10>,\n",
              " <__main__.OPNode at 0x7f873d437c90>,\n",
              " <__main__.OPNode at 0x7f873d430810>,\n",
              " <__main__.OPNode at 0x7f873d439b50>,\n",
              " <__main__.OPNode at 0x7f873d4239d0>,\n",
              " <__main__.OPNode at 0x7f873d423a50>,\n",
              " <__main__.OPNode at 0x7f873d423b50>,\n",
              " <__main__.OPNode at 0x7f873d423c10>,\n",
              " <__main__.OPNode at 0x7f873d423fd0>,\n",
              " <__main__.OPNode at 0x7f873d423d90>,\n",
              " <__main__.OPNode at 0x7f873d423f50>,\n",
              " <__main__.OPNode at 0x7f873d43ca10>,\n",
              " <__main__.OPNode at 0x7f873d43cd10>,\n",
              " <__main__.OPNode at 0x7f873d43e4d0>,\n",
              " <__main__.OPNode at 0x7f873d433950>,\n",
              " <__main__.OPNode at 0x7f873d429650>,\n",
              " <__main__.OPNode at 0x7f873d425190>,\n",
              " <__main__.OPNode at 0x7f873d425290>,\n",
              " <__main__.OPNode at 0x7f873d425310>,\n",
              " <__main__.OPNode at 0x7f873d425390>,\n",
              " <__main__.OPNode at 0x7f873d425510>,\n",
              " <__main__.OPNode at 0x7f873d425810>,\n",
              " <__main__.OPNode at 0x7f873d425410>,\n",
              " <__main__.OPNode at 0x7f873d43ecd0>,\n",
              " <__main__.OPNode at 0x7f873d437890>,\n",
              " <__main__.OPNode at 0x7f873d43ce90>,\n",
              " <__main__.OPNode at 0x7f873d440b90>,\n",
              " <__main__.OPNode at 0x7f873d42b290>,\n",
              " <__main__.OPNode at 0x7f873d43c210>,\n",
              " <__main__.OPNode at 0x7f873d443b50>,\n",
              " <__main__.OPNode at 0x7f873d429350>,\n",
              " <__main__.OPNode at 0x7f873d425910>,\n",
              " <__main__.OPNode at 0x7f873d43e550>,\n",
              " <__main__.OPNode at 0x7f873d439450>,\n",
              " <__main__.OPNode at 0x7f873d437210>,\n",
              " <__main__.OPNode at 0x7f873d425490>,\n",
              " <__main__.OPNode at 0x7f873d42d7d0>,\n",
              " <__main__.OPNode at 0x7f873d437510>,\n",
              " <__main__.OPNode at 0x7f873d43efd0>,\n",
              " <__main__.OPNode at 0x7f873d440790>,\n",
              " <__main__.OPNode at 0x7f873d4394d0>,\n",
              " <__main__.OPNode at 0x7f873d4393d0>,\n",
              " <__main__.OPNode at 0x7f873d42d050>,\n",
              " <__main__.OPNode at 0x7f873d443750>,\n",
              " <__main__.OPNode at 0x7f873d425a10>,\n",
              " <__main__.OPNode at 0x7f873d430090>,\n",
              " <__main__.OPNode at 0x7f873d4295d0>,\n",
              " <__main__.OPNode at 0x7f873d437990>,\n",
              " <__main__.OPNode at 0x7f873d43e250>,\n",
              " <__main__.OPNode at 0x7f873d437910>,\n",
              " <__main__.OPNode at 0x7f873d433350>,\n",
              " <__main__.OPNode at 0x7f873d42b210>,\n",
              " <__main__.OPNode at 0x7f873d4338d0>,\n",
              " <__main__.OPNode at 0x7f873d4436d0>,\n",
              " <__main__.OPNode at 0x7f873d429dd0>,\n",
              " <__main__.OPNode at 0x7f873d440490>,\n",
              " <__main__.OPNode at 0x7f873d425890>,\n",
              " <__main__.OPNode at 0x7f873d425a90>,\n",
              " <__main__.OPNode at 0x7f873d425690>,\n",
              " <__main__.OPNode at 0x7f873d425790>,\n",
              " <__main__.OPNode at 0x7f873d425b10>,\n",
              " <__main__.OPNode at 0x7f873d425b90>,\n",
              " <__main__.OPNode at 0x7f873d425c90>,\n",
              " <__main__.OPNode at 0x7f873d443f10>,\n",
              " <__main__.OPNode at 0x7f873d443f50>,\n",
              " <__main__.OPNode at 0x7f873d446110>,\n",
              " <__main__.OPNode at 0x7f873d440710>,\n",
              " <__main__.OPNode at 0x7f873d4390d0>,\n",
              " <__main__.OPNode at 0x7f873d443fd0>,\n",
              " <__main__.OPNode at 0x7f873d446090>,\n",
              " <__main__.OPNode at 0x7f873d425e90>,\n",
              " <__main__.OPNode at 0x7f873d446190>,\n",
              " <__main__.OPNode at 0x7f873d446210>,\n",
              " <__main__.OPNode at 0x7f873d446290>,\n",
              " <__main__.OPNode at 0x7f873d446310>,\n",
              " <__main__.OPNode at 0x7f873d446390>,\n",
              " <__main__.OPNode at 0x7f873d446410>,\n",
              " <__main__.OPNode at 0x7f873d446590>,\n",
              " <__main__.OPNode at 0x7f873d446610>,\n",
              " <__main__.OPNode at 0x7f873d446490>,\n",
              " <__main__.OPNode at 0x7f873d446510>,\n",
              " <__main__.OPNode at 0x7f873d446690>,\n",
              " <__main__.OPNode at 0x7f873d446710>,\n",
              " <__main__.OPNode at 0x7f873d439f50>,\n",
              " <__main__.OPNode at 0x7f873d42d4d0>,\n",
              " <__main__.OPNode at 0x7f873d429ad0>,\n",
              " <__main__.OPNode at 0x7f873d429e50>,\n",
              " <__main__.OPNode at 0x7f873d43ed50>,\n",
              " <__main__.OPNode at 0x7f873d443250>,\n",
              " <__main__.OPNode at 0x7f873d42d850>,\n",
              " <__main__.OPNode at 0x7f873d42bc90>,\n",
              " <__main__.OPNode at 0x7f873d446790>,\n",
              " <__main__.OPNode at 0x7f873d446910>,\n",
              " <__main__.OPNode at 0x7f873d446990>,\n",
              " <__main__.OPNode at 0x7f873d446810>,\n",
              " <__main__.OPNode at 0x7f873d446890>,\n",
              " <__main__.OPNode at 0x7f873d446a10>,\n",
              " <__main__.OPNode at 0x7f873d446a90>,\n",
              " <__main__.OPNode at 0x7f873d446b10>,\n",
              " <__main__.OPNode at 0x7f873d446b90>,\n",
              " <__main__.OPNode at 0x7f873d446bd0>,\n",
              " <__main__.OPNode at 0x7f873d446d50>,\n",
              " <__main__.OPNode at 0x7f873d446c50>,\n",
              " <__main__.OPNode at 0x7f873d446cd0>,\n",
              " <__main__.OPNode at 0x7f873d42dbd0>,\n",
              " <__main__.OPNode at 0x7f873d429b50>,\n",
              " <__main__.OPNode at 0x7f873d446dd0>,\n",
              " <__main__.OPNode at 0x7f873d446e50>,\n",
              " <__main__.OPNode at 0x7f873d446ed0>,\n",
              " <__main__.OPNode at 0x7f873d446f50>,\n",
              " <__main__.OPNode at 0x7f873d446fd0>,\n",
              " <__main__.OPNode at 0x7f873d3c8090>,\n",
              " <__main__.OPNode at 0x7f873d3c8410>,\n",
              " <__main__.OPNode at 0x7f873d3c8210>,\n",
              " <__main__.OPNode at 0x7f873d3c8290>,\n",
              " <__main__.OPNode at 0x7f873d3c8110>,\n",
              " <__main__.OPNode at 0x7f873d3c8190>,\n",
              " <__main__.OPNode at 0x7f873d3c8310>,\n",
              " <__main__.OPNode at 0x7f873d3c8390>,\n",
              " <__main__.OPNode at 0x7f873d3c8490>,\n",
              " <__main__.OPNode at 0x7f873d3c8610>,\n",
              " <__main__.OPNode at 0x7f873d3c8690>,\n",
              " <__main__.OPNode at 0x7f873d3c8510>,\n",
              " <__main__.OPNode at 0x7f873d3c8590>,\n",
              " <__main__.OPNode at 0x7f873d3c8710>,\n",
              " <__main__.OPNode at 0x7f873d3c8790>,\n",
              " <__main__.OPNode at 0x7f873d3c8810>,\n",
              " <__main__.OPNode at 0x7f873d3c8890>,\n",
              " <__main__.OPNode at 0x7f873d3c88d0>,\n",
              " <__main__.OPNode at 0x7f873d3c8a50>,\n",
              " <__main__.OPNode at 0x7f873d3c8ad0>,\n",
              " <__main__.OPNode at 0x7f873d3c8950>,\n",
              " <__main__.OPNode at 0x7f873d3c89d0>,\n",
              " <__main__.OPNode at 0x7f873d3c8b50>,\n",
              " <__main__.OPNode at 0x7f873d3c8bd0>,\n",
              " <__main__.OPNode at 0x7f873d3c8c50>,\n",
              " <__main__.OPNode at 0x7f873d3c8cd0>,\n",
              " <__main__.OPNode at 0x7f873d3c8d50>,\n",
              " <__main__.OPNode at 0x7f873d3c8ed0>,\n",
              " <__main__.OPNode at 0x7f873d3c8f50>,\n",
              " <__main__.OPNode at 0x7f873d3c8dd0>,\n",
              " <__main__.OPNode at 0x7f873d3c8e50>,\n",
              " <__main__.OPNode at 0x7f873d3c8fd0>,\n",
              " <__main__.OPNode at 0x7f873d3cd090>,\n",
              " <__main__.OPNode at 0x7f873d3cd210>,\n",
              " <__main__.OPNode at 0x7f873d3cd390>,\n",
              " <__main__.OPNode at 0x7f873d3cd290>,\n",
              " <__main__.OPNode at 0x7f873d3cd310>,\n",
              " <__main__.OPNode at 0x7f873d3cd110>,\n",
              " <__main__.OPNode at 0x7f873d3cd190>,\n",
              " <__main__.OPNode at 0x7f873d439fd0>,\n",
              " <__main__.OPNode at 0x7f873d437c10>,\n",
              " <__main__.OPNode at 0x7f873d437590>,\n",
              " <__main__.OPNode at 0x7f873d440f90>,\n",
              " <__main__.OPNode at 0x7f873d443350>,\n",
              " <__main__.OPNode at 0x7f873d430790>,\n",
              " <__main__.OPNode at 0x7f873d42d750>,\n",
              " <__main__.OPNode at 0x7f873d42bd10>,\n",
              " <__main__.OPNode at 0x7f873d3cd410>,\n",
              " <__main__.OPNode at 0x7f873d3cd490>,\n",
              " <__main__.OPNode at 0x7f873d3cd510>,\n",
              " <__main__.OPNode at 0x7f873d3cd590>,\n",
              " <__main__.OPNode at 0x7f873d3cd610>,\n",
              " <__main__.OPNode at 0x7f873d3cd650>,\n",
              " <__main__.OPNode at 0x7f873d3cd7d0>,\n",
              " <__main__.OPNode at 0x7f873d3cd6d0>,\n",
              " <__main__.OPNode at 0x7f873d3cd750>,\n",
              " <__main__.OPNode at 0x7f873d43c610>,\n",
              " <__main__.OPNode at 0x7f873d430290>,\n",
              " <__main__.OPNode at 0x7f873d42d1d0>,\n",
              " <__main__.OPNode at 0x7f873d3cd850>,\n",
              " <__main__.OPNode at 0x7f873d3cd8d0>,\n",
              " <__main__.OPNode at 0x7f873d3cd950>,\n",
              " <__main__.OPNode at 0x7f873d3cd9d0>,\n",
              " <__main__.OPNode at 0x7f873d3cda50>,\n",
              " <__main__.OPNode at 0x7f873d3cdad0>,\n",
              " <__main__.OPNode at 0x7f873d3cdc50>,\n",
              " <__main__.OPNode at 0x7f873d3cdcd0>,\n",
              " <__main__.OPNode at 0x7f873d3cdb50>,\n",
              " <__main__.OPNode at 0x7f873d3cdbd0>,\n",
              " <__main__.OPNode at 0x7f873d3cdd50>,\n",
              " <__main__.OPNode at 0x7f873d3cddd0>,\n",
              " <__main__.OPNode at 0x7f873d3cde50>,\n",
              " <__main__.OPNode at 0x7f873d3cdfd0>,\n",
              " <__main__.OPNode at 0x7f873d3cf090>,\n",
              " <__main__.OPNode at 0x7f873d3cded0>,\n",
              " <__main__.OPNode at 0x7f873d3cdf50>,\n",
              " <__main__.OPNode at 0x7f873d3cf110>,\n",
              " <__main__.OPNode at 0x7f873d3cf190>,\n",
              " <__main__.OPNode at 0x7f873d3cf210>,\n",
              " <__main__.OPNode at 0x7f873d3cf290>,\n",
              " <__main__.OPNode at 0x7f873d3cf2d0>,\n",
              " <__main__.OPNode at 0x7f873d3cf450>,\n",
              " <__main__.OPNode at 0x7f873d3cf4d0>,\n",
              " <__main__.OPNode at 0x7f873d3cf350>,\n",
              " <__main__.OPNode at 0x7f873d3cf3d0>,\n",
              " <__main__.OPNode at 0x7f873d3cf550>,\n",
              " <__main__.OPNode at 0x7f873d3cf5d0>,\n",
              " <__main__.OPNode at 0x7f873d3cf650>,\n",
              " <__main__.OPNode at 0x7f873d3cf6d0>,\n",
              " <__main__.OPNode at 0x7f873d3cf750>,\n",
              " <__main__.OPNode at 0x7f873d3cf8d0>,\n",
              " <__main__.OPNode at 0x7f873d3cf950>,\n",
              " <__main__.OPNode at 0x7f873d3cf7d0>,\n",
              " <__main__.OPNode at 0x7f873d3cf850>,\n",
              " <__main__.OPNode at 0x7f873d3cf9d0>,\n",
              " <__main__.OPNode at 0x7f873d3cfa50>,\n",
              " <__main__.OPNode at 0x7f873d3cfad0>,\n",
              " <__main__.OPNode at 0x7f873d3cfc50>,\n",
              " <__main__.OPNode at 0x7f873d3cfcd0>,\n",
              " <__main__.OPNode at 0x7f873d3cfb50>,\n",
              " <__main__.OPNode at 0x7f873d3cfbd0>,\n",
              " <__main__.OPNode at 0x7f873d3cfd50>,\n",
              " <__main__.OPNode at 0x7f873d3cfdd0>,\n",
              " <__main__.OPNode at 0x7f873d3cfe50>,\n",
              " <__main__.OPNode at 0x7f873d3cfed0>,\n",
              " <__main__.OPNode at 0x7f873d3cff10>,\n",
              " <__main__.OPNode at 0x7f873d3d10d0>,\n",
              " <__main__.OPNode at 0x7f873d3d1150>,\n",
              " <__main__.OPNode at 0x7f873d3cff90>,\n",
              " <__main__.OPNode at 0x7f873d3d1050>,\n",
              " <__main__.OPNode at 0x7f873d3d11d0>,\n",
              " <__main__.OPNode at 0x7f873d3d1250>,\n",
              " <__main__.OPNode at 0x7f873d3d12d0>,\n",
              " <__main__.OPNode at 0x7f873d3d1350>,\n",
              " <__main__.OPNode at 0x7f873d3d13d0>,\n",
              " <__main__.OPNode at 0x7f873d3d1750>,\n",
              " <__main__.OPNode at 0x7f873d3d1550>,\n",
              " <__main__.OPNode at 0x7f873d3d1450>,\n",
              " <__main__.OPNode at 0x7f873d3d14d0>,\n",
              " <__main__.OPNode at 0x7f873d430a10>,\n",
              " <__main__.OPNode at 0x7f873d3d15d0>,\n",
              " <__main__.OPNode at 0x7f873d3d1650>,\n",
              " <__main__.OPNode at 0x7f873d3d16d0>,\n",
              " <__main__.OPNode at 0x7f873d3d17d0>,\n",
              " <__main__.OPNode at 0x7f873d3d1950>,\n",
              " <__main__.OPNode at 0x7f873d3d19d0>,\n",
              " <__main__.OPNode at 0x7f873d3d1850>,\n",
              " <__main__.OPNode at 0x7f873d3d18d0>,\n",
              " <__main__.OPNode at 0x7f873d3d1a50>,\n",
              " <__main__.OPNode at 0x7f873d3d1ad0>,\n",
              " <__main__.OPNode at 0x7f873d3d1b50>,\n",
              " <__main__.OPNode at 0x7f873d3d1bd0>,\n",
              " <__main__.OPNode at 0x7f873d3d1c10>,\n",
              " <__main__.OPNode at 0x7f873d3d1d90>,\n",
              " <__main__.OPNode at 0x7f873d3d1e10>,\n",
              " <__main__.OPNode at 0x7f873d3d1c90>,\n",
              " <__main__.OPNode at 0x7f873d3d1d10>,\n",
              " <__main__.OPNode at 0x7f873d3d1e90>,\n",
              " <__main__.OPNode at 0x7f873d3d1f10>,\n",
              " <__main__.OPNode at 0x7f873d3d1f90>,\n",
              " <__main__.OPNode at 0x7f873d3d5050>,\n",
              " <__main__.OPNode at 0x7f873d3d50d0>,\n",
              " <__main__.OPNode at 0x7f873d3d5250>,\n",
              " <__main__.OPNode at 0x7f873d3d52d0>,\n",
              " <__main__.OPNode at 0x7f873d3d5150>,\n",
              " <__main__.OPNode at 0x7f873d3d51d0>,\n",
              " <__main__.OPNode at 0x7f873d3d5350>,\n",
              " <__main__.OPNode at 0x7f873d3d53d0>,\n",
              " <__main__.OPNode at 0x7f873d3d5550>,\n",
              " <__main__.OPNode at 0x7f873d3d56d0>,\n",
              " <__main__.OPNode at 0x7f873d3d5750>,\n",
              " <__main__.OPNode at 0x7f873d3d55d0>,\n",
              " <__main__.OPNode at 0x7f873d3d5650>,\n",
              " <__main__.OPNode at 0x7f873d3d57d0>,\n",
              " <__main__.OPNode at 0x7f873d3d5850>,\n",
              " <__main__.OPNode at 0x7f873d3d58d0>,\n",
              " <__main__.OPNode at 0x7f873d3d5950>,\n",
              " <__main__.OPNode at 0x7f873d3d5990>,\n",
              " <__main__.OPNode at 0x7f873d3d5b10>,\n",
              " <__main__.OPNode at 0x7f873d3d5b90>,\n",
              " <__main__.OPNode at 0x7f873d3d5a10>,\n",
              " <__main__.OPNode at 0x7f873d3d5a90>,\n",
              " <__main__.OPNode at 0x7f873d3d5c10>,\n",
              " <__main__.OPNode at 0x7f873d3d5c90>,\n",
              " <__main__.OPNode at 0x7f873d3d5d10>,\n",
              " <__main__.OPNode at 0x7f873d3d5d90>,\n",
              " <__main__.OPNode at 0x7f873d3d5450>,\n",
              " <__main__.OPNode at 0x7f873d3d54d0>,\n",
              " <__main__.OPNode at 0x7f873d3d5e10>,\n",
              " <__main__.OPNode at 0x7f873d3d5f90>,\n",
              " <__main__.OPNode at 0x7f873d3d7050>,\n",
              " <__main__.OPNode at 0x7f873d3d5e90>,\n",
              " <__main__.OPNode at 0x7f873d3d5f10>,\n",
              " <__main__.OPNode at 0x7f873d3d70d0>,\n",
              " <__main__.OPNode at 0x7f873d3d7150>,\n",
              " <__main__.OPNode at 0x7f873d3d71d0>,\n",
              " <__main__.OPNode at 0x7f873d3d7350>,\n",
              " <__main__.OPNode at 0x7f873d3d73d0>,\n",
              " <__main__.OPNode at 0x7f873d3d7250>,\n",
              " <__main__.OPNode at 0x7f873d3d72d0>,\n",
              " <__main__.OPNode at 0x7f873d3d7450>,\n",
              " <__main__.OPNode at 0x7f873d3d74d0>,\n",
              " <__main__.OPNode at 0x7f873d3d7550>,\n",
              " <__main__.OPNode at 0x7f873d3d75d0>,\n",
              " <__main__.OPNode at 0x7f873d3d7610>,\n",
              " <__main__.OPNode at 0x7f873d3d7790>,\n",
              " <__main__.OPNode at 0x7f873d3d7810>,\n",
              " <__main__.OPNode at 0x7f873d3d7690>,\n",
              " <__main__.OPNode at 0x7f873d3d7710>,\n",
              " <__main__.OPNode at 0x7f873d3d7890>,\n",
              " <__main__.OPNode at 0x7f873d3d7910>,\n",
              " <__main__.OPNode at 0x7f873d3d7990>,\n",
              " <__main__.OPNode at 0x7f873d3d7a10>,\n",
              " <__main__.OPNode at 0x7f873d3d7a90>,\n",
              " <__main__.OPNode at 0x7f873d3d7c10>,\n",
              " <__main__.OPNode at 0x7f873d3d7c90>,\n",
              " <__main__.OPNode at 0x7f873d3d7b10>,\n",
              " <__main__.OPNode at 0x7f873d3d7b90>,\n",
              " <__main__.OPNode at 0x7f873d3d7d10>,\n",
              " <__main__.OPNode at 0x7f873d3d7d90>,\n",
              " <__main__.OPNode at 0x7f873d3d7e10>,\n",
              " <__main__.OPNode at 0x7f873d3d7f90>,\n",
              " <__main__.OPNode at 0x7f873d3d7e90>,\n",
              " <__main__.OPNode at 0x7f873d3d7f10>,\n",
              " <__main__.OPNode at 0x7f873d437f10>,\n",
              " <__main__.OPNode at 0x7f873d3d9050>,\n",
              " <__main__.OPNode at 0x7f873d3d90d0>,\n",
              " <__main__.OPNode at 0x7f873d3d9150>,\n",
              " <__main__.OPNode at 0x7f873d3d91d0>,\n",
              " <__main__.OPNode at 0x7f873d3d9250>,\n",
              " <__main__.OPNode at 0x7f873d3d9290>,\n",
              " <__main__.OPNode at 0x7f873d3d9410>,\n",
              " <__main__.OPNode at 0x7f873d3d9490>,\n",
              " <__main__.OPNode at 0x7f873d3d9310>,\n",
              " <__main__.OPNode at 0x7f873d3d9390>,\n",
              " <__main__.OPNode at 0x7f873d3d9510>,\n",
              " <__main__.OPNode at 0x7f873d3d9590>,\n",
              " <__main__.OPNode at 0x7f873d3d9610>,\n",
              " <__main__.OPNode at 0x7f873d3d9690>,\n",
              " <__main__.OPNode at 0x7f873d3d9710>,\n",
              " <__main__.OPNode at 0x7f873d3d9890>,\n",
              " <__main__.OPNode at 0x7f873d3d9910>,\n",
              " <__main__.OPNode at 0x7f873d3d9790>,\n",
              " <__main__.OPNode at 0x7f873d3d9810>,\n",
              " <__main__.OPNode at 0x7f873d3d9990>,\n",
              " <__main__.OPNode at 0x7f873d3d9a10>,\n",
              " <__main__.OPNode at 0x7f873d3d9a90>,\n",
              " <__main__.OPNode at 0x7f873d3d9c10>,\n",
              " <__main__.OPNode at 0x7f873d3d9c90>,\n",
              " <__main__.OPNode at 0x7f873d3d9b10>,\n",
              " <__main__.OPNode at 0x7f873d3d9b90>,\n",
              " <__main__.OPNode at 0x7f873d3d9d10>,\n",
              " <__main__.OPNode at 0x7f873d3d9d90>,\n",
              " <__main__.OPNode at 0x7f873d3d9e10>,\n",
              " <__main__.OPNode at 0x7f873d3d9e90>,\n",
              " <__main__.OPNode at 0x7f873d3d9ed0>,\n",
              " <__main__.OPNode at 0x7f873d3dd090>,\n",
              " <__main__.OPNode at 0x7f873d3dd110>,\n",
              " <__main__.OPNode at 0x7f873d3d9f50>,\n",
              " <__main__.OPNode at 0x7f873d3d9fd0>,\n",
              " <__main__.OPNode at 0x7f873d3dd190>,\n",
              " <__main__.OPNode at 0x7f873d3dd210>,\n",
              " <__main__.OPNode at 0x7f873d3dd290>,\n",
              " <__main__.OPNode at 0x7f873d3dd310>,\n",
              " <__main__.OPNode at 0x7f873d3dd390>,\n",
              " <__main__.OPNode at 0x7f873d3dd510>,\n",
              " <__main__.OPNode at 0x7f873d3dd590>,\n",
              " <__main__.OPNode at 0x7f873d3dd410>,\n",
              " <__main__.OPNode at 0x7f873d3dd490>,\n",
              " <__main__.OPNode at 0x7f873d3dd610>,\n",
              " <__main__.OPNode at 0x7f873d3dd690>,\n",
              " <__main__.OPNode at 0x7f873d3dd710>,\n",
              " <__main__.OPNode at 0x7f873d3dd890>,\n",
              " <__main__.OPNode at 0x7f873d3dd910>,\n",
              " <__main__.OPNode at 0x7f873d3dd790>,\n",
              " <__main__.OPNode at 0x7f873d3dd810>,\n",
              " <__main__.OPNode at 0x7f873d3dd990>,\n",
              " <__main__.OPNode at 0x7f873d3dda10>,\n",
              " <__main__.OPNode at 0x7f873d3dda90>,\n",
              " <__main__.OPNode at 0x7f873d3ddb10>,\n",
              " <__main__.OPNode at 0x7f873d3ddb50>,\n",
              " <__main__.OPNode at 0x7f873d3ddcd0>,\n",
              " <__main__.OPNode at 0x7f873d3ddd50>,\n",
              " <__main__.OPNode at 0x7f873d3ddbd0>,\n",
              " <__main__.OPNode at 0x7f873d3ddc50>,\n",
              " <__main__.OPNode at 0x7f873d3dddd0>,\n",
              " <__main__.OPNode at 0x7f873d3dde50>,\n",
              " <__main__.OPNode at 0x7f873d3dded0>,\n",
              " <__main__.OPNode at 0x7f873d3ddf50>,\n",
              " <__main__.OPNode at 0x7f873d3ddfd0>,\n",
              " <__main__.OPNode at 0x7f873d3df390>,\n",
              " <__main__.OPNode at 0x7f873d3df190>,\n",
              " <__main__.OPNode at 0x7f873d3df210>,\n",
              " <__main__.OPNode at 0x7f873d3df090>,\n",
              " <__main__.OPNode at 0x7f873d3df110>,\n",
              " <__main__.OPNode at 0x7f873d3df290>,\n",
              " <__main__.OPNode at 0x7f873d3df310>,\n",
              " <__main__.OPNode at 0x7f873d3df410>,\n",
              " <__main__.OPNode at 0x7f873d3df590>,\n",
              " <__main__.OPNode at 0x7f873d3df610>,\n",
              " <__main__.OPNode at 0x7f873d3df490>,\n",
              " <__main__.OPNode at 0x7f873d3df510>,\n",
              " <__main__.OPNode at 0x7f873d3df690>,\n",
              " <__main__.OPNode at 0x7f873d3df710>,\n",
              " <__main__.OPNode at 0x7f873d3df790>,\n",
              " <__main__.OPNode at 0x7f873d3df810>,\n",
              " <__main__.OPNode at 0x7f873d3df850>,\n",
              " <__main__.OPNode at 0x7f873d3df9d0>,\n",
              " <__main__.OPNode at 0x7f873d3dfa50>,\n",
              " <__main__.OPNode at 0x7f873d3df8d0>,\n",
              " <__main__.OPNode at 0x7f873d3df950>,\n",
              " <__main__.OPNode at 0x7f873d3dfad0>,\n",
              " <__main__.OPNode at 0x7f873d3dfb50>,\n",
              " <__main__.OPNode at 0x7f873d3dfbd0>,\n",
              " <__main__.OPNode at 0x7f873d3dfc50>,\n",
              " <__main__.OPNode at 0x7f873d3dfcd0>,\n",
              " <__main__.OPNode at 0x7f873d3dfe50>,\n",
              " <__main__.OPNode at 0x7f873d3dfed0>,\n",
              " <__main__.OPNode at 0x7f873d3dfd50>,\n",
              " <__main__.OPNode at 0x7f873d3dfdd0>,\n",
              " <__main__.OPNode at 0x7f873d3dff50>,\n",
              " <__main__.OPNode at 0x7f873d3dffd0>,\n",
              " <__main__.OPNode at 0x7f873d3e2190>,\n",
              " <__main__.OPNode at 0x7f873d3e2310>,\n",
              " <__main__.OPNode at 0x7f873d3e2390>,\n",
              " <__main__.OPNode at 0x7f873d3e2210>,\n",
              " <__main__.OPNode at 0x7f873d3e2290>,\n",
              " <__main__.OPNode at 0x7f873d3e2410>,\n",
              " <__main__.OPNode at 0x7f873d3e2490>,\n",
              " <__main__.OPNode at 0x7f873d3e2510>,\n",
              " <__main__.OPNode at 0x7f873d3e2590>,\n",
              " <__main__.OPNode at 0x7f873d3e25d0>,\n",
              " <__main__.OPNode at 0x7f873d3e2750>,\n",
              " <__main__.OPNode at 0x7f873d3e27d0>,\n",
              " <__main__.OPNode at 0x7f873d3e2650>,\n",
              " <__main__.OPNode at 0x7f873d3e26d0>,\n",
              " <__main__.OPNode at 0x7f873d3e2850>,\n",
              " <__main__.OPNode at 0x7f873d3e28d0>,\n",
              " <__main__.OPNode at 0x7f873d3e2950>,\n",
              " <__main__.OPNode at 0x7f873d3e29d0>,\n",
              " <__main__.OPNode at 0x7f873d3e2090>,\n",
              " <__main__.OPNode at 0x7f873d3e2110>,\n",
              " <__main__.OPNode at 0x7f873d3e2a50>,\n",
              " <__main__.OPNode at 0x7f873d3e2bd0>,\n",
              " <__main__.OPNode at 0x7f873d3e2c50>,\n",
              " <__main__.OPNode at 0x7f873d3e2ad0>,\n",
              " <__main__.OPNode at 0x7f873d3e2b50>,\n",
              " <__main__.OPNode at 0x7f873d3e2cd0>,\n",
              " <__main__.OPNode at 0x7f873d3e2d50>,\n",
              " <__main__.OPNode at 0x7f873d3e2dd0>,\n",
              " <__main__.OPNode at 0x7f873d3e2f50>,\n",
              " <__main__.OPNode at 0x7f873d3e2fd0>,\n",
              " <__main__.OPNode at 0x7f873d3e2e50>,\n",
              " <__main__.OPNode at 0x7f873d3e2ed0>,\n",
              " <__main__.OPNode at 0x7f873d3e5090>,\n",
              " <__main__.OPNode at 0x7f873d3e5110>,\n",
              " <__main__.OPNode at 0x7f873d3e5190>,\n",
              " <__main__.OPNode at 0x7f873d3e5210>,\n",
              " <__main__.OPNode at 0x7f873d3e5250>,\n",
              " <__main__.OPNode at 0x7f873d3e53d0>,\n",
              " <__main__.OPNode at 0x7f873d3e5450>,\n",
              " <__main__.OPNode at 0x7f873d3e52d0>,\n",
              " <__main__.OPNode at 0x7f873d3e5350>,\n",
              " <__main__.OPNode at 0x7f873d3e54d0>,\n",
              " <__main__.OPNode at 0x7f873d3e5550>,\n",
              " <__main__.OPNode at 0x7f873d3e55d0>,\n",
              " <__main__.OPNode at 0x7f873d3e5650>,\n",
              " <__main__.OPNode at 0x7f873d3e56d0>,\n",
              " <__main__.OPNode at 0x7f873d3e5850>,\n",
              " <__main__.OPNode at 0x7f873d3e58d0>,\n",
              " <__main__.OPNode at 0x7f873d3e5750>,\n",
              " <__main__.OPNode at 0x7f873d3e57d0>,\n",
              " <__main__.OPNode at 0x7f873d3e5950>,\n",
              " <__main__.OPNode at 0x7f873d3e59d0>,\n",
              " <__main__.OPNode at 0x7f873d3e5a50>,\n",
              " <__main__.OPNode at 0x7f873d3e5bd0>,\n",
              " <__main__.OPNode at 0x7f873d3e5c50>,\n",
              " <__main__.OPNode at 0x7f873d3e5ad0>,\n",
              " <__main__.OPNode at 0x7f873d3e5b50>,\n",
              " <__main__.OPNode at 0x7f873d3e5cd0>,\n",
              " <__main__.OPNode at 0x7f873d3e5d50>,\n",
              " <__main__.OPNode at 0x7f873d3e5dd0>,\n",
              " <__main__.OPNode at 0x7f873d3e5e50>,\n",
              " <__main__.OPNode at 0x7f873d3e5e90>,\n",
              " <__main__.OPNode at 0x7f873d3e7050>,\n",
              " <__main__.OPNode at 0x7f873d3e70d0>,\n",
              " <__main__.OPNode at 0x7f873d3e5f10>,\n",
              " <__main__.OPNode at 0x7f873d3e5f90>,\n",
              " <__main__.OPNode at 0x7f873d3e7150>,\n",
              " <__main__.OPNode at 0x7f873d3e71d0>,\n",
              " <__main__.OPNode at 0x7f873d3e7250>,\n",
              " <__main__.OPNode at 0x7f873d3e72d0>,\n",
              " <__main__.OPNode at 0x7f873d3e7350>,\n",
              " <__main__.OPNode at 0x7f873d3e74d0>,\n",
              " <__main__.OPNode at 0x7f873d3e7550>,\n",
              " <__main__.OPNode at 0x7f873d3e73d0>,\n",
              " <__main__.OPNode at 0x7f873d3e7450>,\n",
              " <__main__.OPNode at 0x7f873d3e75d0>,\n",
              " <__main__.OPNode at 0x7f873d3e7650>,\n",
              " <__main__.OPNode at 0x7f873d3e7710>,\n",
              " <__main__.OPNode at 0x7f873d3e77d0>,\n",
              " <__main__.OPNode at 0x7f873d3e7850>,\n",
              " <__main__.OPNode at 0x7f873d3e7910>,\n",
              " <__main__.OPNode at 0x7f873d3e79d0>,\n",
              " <__main__.OPNode at 0x7f873d3e7a50>,\n",
              " <__main__.OPNode at 0x7f873d3e7ad0>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -> pytorch"
      ],
      "metadata": {
        "id": "f5U6xKShGC5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWWGzsYzG9Y3",
        "outputId": "96c41624-c12e-4de4-f2e5-aef23edfe5a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 45.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### albert model"
      ],
      "metadata": {
        "id": "1K93MIJeZuwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import torch\n",
        "from transformers import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, albert_config_file, pytorch_dump_path):\n",
        "    # 初始化 PyTorch 模型\n",
        "    config = AlbertConfig.from_json_file(albert_config_file)\n",
        "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
        "    model = AlbertForPreTraining(config)\n",
        "\n",
        "    # 从 tf 的 checkpoint 文件中加载模型权重\n",
        "    load_tf_weights_in_albert(model, config, tf_checkpoint_path)\n",
        "\n",
        "    # 保存 PyTorch 模型\n",
        "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
        "    torch.save(model.state_dict(), pytorch_dump_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gXxTXIAjGGpG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "\n",
        "# https://github.com/delldu/Albert\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "# 参数设置\n",
        "parser.add_argument(\"--tf_checkpoint_path\", default='./resources/albert_base_zh/model.ckpt-best', type=str,\n",
        "                    required=False, help=\"Path to the TensorFlow checkpoint path.\")\n",
        "parser.add_argument(\"--albert_config_file\", default='./resources/albert_base_zh/albert_config.json', type=str,\n",
        "                    required=False,\n",
        "                    help=\"The config json file corresponding to the pre-trained ALBERT model. \\n\"\"This specifies the model architecture.\", )\n",
        "parser.add_argument(\"--pytorch_dump_path\", default='./resources/albert_base_zh/pytorch_model.bin', type=str, required=False,\n",
        "                    help=\"Path to the output PyTorch model.\")\n",
        "\n",
        "args = parser.parse_args(args = [])\n",
        "args.tf_checkpoint_path = \"models/COVIDNet-CXR-2/model\"\n",
        "args.albert_config_file = \"albert_config.json\"\n",
        "args.pytorch_dump_path = \"pytorch_model.bin\"\n",
        "\n",
        "\n",
        "convert_tf_checkpoint_to_pytorch(args.tf_checkpoint_path, args.albert_config_file, args.pytorch_dump_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "wdbu1PicG8KM",
        "outputId": "c6a51820-f3f4-4669-9917-4122a857f000"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c2e6feac0abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mconvert_tf_checkpoint_to_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malbert_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch_dump_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-6d505404c7af>\u001b[0m in \u001b[0;36mconvert_tf_checkpoint_to_pytorch\u001b[0;34m(tf_checkpoint_path, albert_config_file, pytorch_dump_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_tf_checkpoint_to_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malbert_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_dump_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 初始化 PyTorch 模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building PyTorch model from configuration: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertForPreTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \"\"\"\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_dict_from_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'albert_config.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get graph"
      ],
      "metadata": {
        "id": "kl7nkyJUJLi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print( tf.__version__ )"
      ],
      "metadata": {
        "id": "CwOcYqqfJ67K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' //////////////////////////////////////////////////// '''\n",
        "# definitions\n",
        "\n",
        "# data.py\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def crop_top(img, percent=0.15):\n",
        "    offset = int(img.shape[0] * percent)\n",
        "    return img[offset:]\n",
        "\n",
        "def central_crop(img):\n",
        "    size = min(img.shape[0], img.shape[1])\n",
        "    offset_h = int((img.shape[0] - size) / 2)\n",
        "    offset_w = int((img.shape[1] - size) / 2)\n",
        "    return img[offset_h:offset_h + size, offset_w:offset_w + size]\n",
        "\n",
        "def process_image_file(filepath, size, top_percent=0.08, crop=True):\n",
        "    img = cv2.imread(filepath)\n",
        "    # print(\"filepath\", filepath)\n",
        "    img = crop_top(img, percent=top_percent)\n",
        "    if crop:\n",
        "        img = central_crop(img)\n",
        "    img = cv2.resize(img, (size, size))\n",
        "    return img\n",
        "\n",
        "def process_image_file_medusa(filepath, size):\n",
        "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, (size, size))\n",
        "    img = img.astype('float64')\n",
        "    img -= img.mean()\n",
        "    img /= img.std()\n",
        "    return np.expand_dims(img, -1)\n",
        "\n",
        "def random_ratio_resize(img, prob=0.3, delta=0.1):\n",
        "    if np.random.rand() >= prob:\n",
        "        return img\n",
        "    ratio = img.shape[0] / img.shape[1]\n",
        "    ratio = np.random.uniform(max(ratio - delta, 0.01), ratio + delta)\n",
        "\n",
        "    if ratio * img.shape[1] <= img.shape[1]:\n",
        "        size = (int(img.shape[1] * ratio), img.shape[1])\n",
        "    else:\n",
        "        size = (img.shape[0], int(img.shape[0] / ratio))\n",
        "\n",
        "    dh = img.shape[0] - size[1]\n",
        "    top, bot = dh // 2, dh - dh // 2\n",
        "    dw = img.shape[1] - size[0]\n",
        "    left, right = dw // 2, dw - dw // 2\n",
        "\n",
        "    if size[0] > 480 or size[1] > 480:\n",
        "        print(img.shape, size, ratio)\n",
        "\n",
        "    img = cv2.resize(img, size)\n",
        "    img = cv2.copyMakeBorder(img, top, bot, left, right, cv2.BORDER_CONSTANT,\n",
        "                             (0, 0, 0))\n",
        "\n",
        "    if img.shape[0] != 480 or img.shape[1] != 480:\n",
        "        raise ValueError(img.shape, size)\n",
        "    return img\n",
        "\n",
        "_augmentation_transform = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=(0.9, 1.1),\n",
        "    zoom_range=(0.85, 1.15),\n",
        "    fill_mode='constant',\n",
        "    cval=0.,\n",
        ")\n",
        "\n",
        "def apply_augmentation(img):\n",
        "    img = random_ratio_resize(img)\n",
        "    img = _augmentation_transform.random_transform(img)\n",
        "    return img\n",
        "\n",
        "def _process_csv_file(file):\n",
        "    with open(file, 'r') as fr:\n",
        "        files = fr.readlines()\n",
        "    return files\n",
        "\n",
        "\n",
        "class BalanceCovidDataset(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            data_dir,\n",
        "            csv_file,\n",
        "            is_training=True,\n",
        "            batch_size=8,\n",
        "            medusa_input_shape=(256, 256),\n",
        "            input_shape=(480, 480),\n",
        "            n_classes=2,\n",
        "            num_channels=3,\n",
        "            mapping={\n",
        "                'negative': 0,\n",
        "                'positive': 1,\n",
        "            },\n",
        "            shuffle=True,\n",
        "            augmentation=apply_augmentation,\n",
        "            covid_percent=0.5,\n",
        "            class_weights=[1., 1.],\n",
        "            top_percent=0.08,\n",
        "            is_severity_model=False,\n",
        "            is_medusa_backbone=False,\n",
        "    ):\n",
        "        'Initialization'\n",
        "        self.datadir = data_dir\n",
        "        self.dataset = _process_csv_file(csv_file)\n",
        "        self.is_training = is_training\n",
        "        self.batch_size = batch_size\n",
        "        self.N = len(self.dataset)\n",
        "        self.medusa_input_shape = medusa_input_shape\n",
        "        self.input_shape = input_shape\n",
        "        self.n_classes = n_classes\n",
        "        self.num_channels = num_channels\n",
        "        self.mapping = mapping\n",
        "        self.shuffle = shuffle\n",
        "        self.covid_percent = covid_percent\n",
        "        self.class_weights = class_weights\n",
        "        self.n = 0\n",
        "        self.augmentation = augmentation\n",
        "        self.top_percent = top_percent\n",
        "        self.is_severity_model = is_severity_model\n",
        "        self.is_medusa_backbone = is_medusa_backbone\n",
        "\n",
        "        # If using MEDUSA backbone load images without crop\n",
        "        if self.is_medusa_backbone:\n",
        "            self.load_image = partial(process_image_file, top_percent=0, crop=False)\n",
        "        else:\n",
        "            self.load_image = process_image_file\n",
        "\n",
        "        datasets = {}\n",
        "        for key in self.mapping.keys():\n",
        "            datasets[key] = []\n",
        "\n",
        "        for l in self.dataset:\n",
        "            datasets[l.split()[2]].append(l)\n",
        "        \n",
        "        if self.is_severity_model:\n",
        "            self.datasets = [\n",
        "                datasets['level2'], datasets['level1']\n",
        "            ]\n",
        "        elif self.n_classes == 2:\n",
        "            self.datasets = [\n",
        "                datasets['negative'], datasets['positive']\n",
        "            ]\n",
        "        elif self.n_classes == 3:\n",
        "            self.datasets = [\n",
        "                datasets['normal'] + datasets['pneumonia'],\n",
        "                datasets['COVID-19'],\n",
        "            ]\n",
        "        else:\n",
        "            raise Exception('Only binary or 3 class classification currently supported.')\n",
        "        print(len(self.datasets[0]), len(self.datasets[1]))\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __next__(self):\n",
        "        # Get one batch of data\n",
        "        model_inputs = self.__getitem__(self.n)\n",
        "        # Batch index\n",
        "        self.n += 1\n",
        "\n",
        "        # If we have processed the entire dataset then\n",
        "        if self.n >= self.__len__():\n",
        "            self.on_epoch_end()\n",
        "            self.n = 0\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.datasets[0]) / float(self.batch_size)))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        if self.shuffle == True:\n",
        "            for v in self.datasets:\n",
        "                np.random.shuffle(v)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = np.zeros((self.batch_size, *self.input_shape, self.num_channels))\n",
        "        batch_y = np.zeros(self.batch_size)\n",
        "\n",
        "        if self.is_medusa_backbone:\n",
        "            batch_sem_x = np.zeros((self.batch_size, *self.medusa_input_shape, 1))\n",
        "\n",
        "        batch_files = self.datasets[0][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        # upsample covid cases\n",
        "        covid_size = max(int(len(batch_files) * self.covid_percent), 1)\n",
        "        covid_inds = np.random.choice(np.arange(len(batch_files)),\n",
        "                                      size=covid_size,\n",
        "                                      replace=False)\n",
        "        covid_files = np.random.choice(self.datasets[1],\n",
        "                                       size=covid_size,\n",
        "                                       replace=False)\n",
        "        for i in range(covid_size):\n",
        "            batch_files[covid_inds[i]] = covid_files[i]\n",
        "\n",
        "        for i in range(len(batch_files)):\n",
        "            sample = batch_files[i].split()\n",
        "\n",
        "            if self.is_training:\n",
        "                folder = 'train'\n",
        "            else:\n",
        "                folder = 'test'\n",
        "\n",
        "            image_file = os.path.join(self.datadir, folder, sample[1])\n",
        "            x = self.load_image(\n",
        "                image_file,\n",
        "                self.input_shape[0],\n",
        "                top_percent=self.top_percent,\n",
        "            )\n",
        "\n",
        "            if self.is_training and hasattr(self, 'augmentation'):\n",
        "                x = self.augmentation(x)\n",
        "\n",
        "            x = x.astype('float32') / 255.0\n",
        "\n",
        "            if self.is_medusa_backbone:\n",
        "                sem_x = process_image_file_medusa(image_file, self.medusa_input_shape[0])\n",
        "                batch_sem_x[i] = sem_x\n",
        "            \n",
        "            y = self.mapping[sample[2]]\n",
        "\n",
        "            batch_x[i] = x\n",
        "            batch_y[i] = y\n",
        "\n",
        "        class_weights = self.class_weights\n",
        "        weights = np.take(class_weights, batch_y.astype('int64'))\n",
        "        batch_y = keras.utils.to_categorical(batch_y, num_classes=self.n_classes)\n",
        "\n",
        "        if self.is_medusa_backbone:\n",
        "            return batch_sem_x, batch_x, batch_y, weights, self.is_training\n",
        "        else:\n",
        "            return batch_x, batch_y, weights, self.is_training\n",
        "\n",
        "''' \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ '''        \n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os, argparse\n",
        "\n",
        "# To remove TF Warnings\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "    # args = []\n",
        "'''\n",
        "python inference.py \\\n",
        "--weightspath models/COVIDNet-CXR-2 \\\n",
        "--metaname model.meta \\\n",
        "--ckptname model \\\n",
        "--n_classes 2 \\\n",
        "--imagepath assets/ex-covid.jpeg \\\n",
        "--in_tensorname input_1:0 \\\n",
        "--out_tensorname norm_dense_2/Softmax:0\n",
        "\n",
        "python inference.py \\\n",
        "--weightspath models/COVIDNet-CXR-3 \\\n",
        "--metaname model.meta \\\n",
        "--ckptname model \\\n",
        "--n_classes 2 \\\n",
        "--imagepath assets/ex-covid.jpeg \\\n",
        "--out_tensorname softmax/Softmax:0 \\\n",
        "--is_medusa_backbone\n",
        "\n",
        "'''\n",
        "\n",
        "# USE COVIDNet CXR 2\n",
        "\n",
        "args_weightspath = 'models/COVIDNet-CXR-2' \n",
        "args_metaname = 'model.meta'\n",
        "args_ckptname = 'model'\n",
        "args_n_classes = 2\n",
        "\n",
        "args_testfolder = 'data/test'\n",
        "args_trainfile = 'labels/train_COVIDx9B.txt'\n",
        "args_testfile = 'labels/test_COVIDx9B.txt'\n",
        "\n",
        "args_out_tensorname = 'norm_dense_2/Softmax:0'\n",
        "args_logit_tensorname = 'norm_dense_2/MatMul:0'\n",
        "args_is_severity_model = False\n",
        "args_is_medusa_backbone = False\n",
        "\n",
        "args_in_tensorname = 'input_1:0'\n",
        "args_in_tensorname_medusa = 'input_1:0'\n",
        "args_input_size = 480\n",
        "args_input_size_medusa = 256\n",
        "args_top_percent = 0.08\n",
        "\n",
        "'''\n",
        "<<<<<<<<<<<<<<<<<<Here to change the test image!!! >>>>>>>>>>>>>>>>>>>>\n",
        "'''\n",
        "\n",
        "args_imagepath = 'assets/ex-covid.jpeg'\n",
        "# args_imagepath = \"data/test/0a8d486f-1aa6-4fcf-b7be-4bf04fc8628b.png\"\n",
        "# args_imagepath = \"drive/MyDrive/covid/ricord_images/MIDRC-RICORD-1C-SITE2-000293-40361-0.png\"\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "# tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "\n",
        "if args_is_severity_model:\n",
        "    # For COVIDNet CXR-S training with COVIDxSev level 1 and level 2 air space seveirty grading\n",
        "    mapping = {'level2': 0, 'level1': 1}\n",
        "    inv_mapping = {0: 'level2', 1: 'level1'}\n",
        "elif args_n_classes == 2:\n",
        "    # For COVID-19 positive/negative detection\n",
        "    mapping = {'negative': 0, 'positive': 1}\n",
        "    inv_mapping = {0: 'negative', 1: 'positive'}\n",
        "elif args_n_classes == 3:\n",
        "    # For detection of no pneumonia/non-COVID-19 pneumonia/COVID-19 pneumonia\n",
        "    mapping = {'normal': 0, 'pneumonia': 1, 'COVID-19': 2}\n",
        "    inv_mapping = {0: 'normal', 1: 'pneumonia', 2: 'COVID-19'}\n",
        "else:\n",
        "    raise Exception('''COVID-Net currently only supports 2 class COVID-19 positive/negative detection\n",
        "        or 3 class detection of no pneumonia/non-COVID-19 pneumonia/COVID-19 pneumonia''')\n",
        "\n",
        "print(\"mapping\", mapping)\n",
        "mapping_keys = list(mapping.keys())\n",
        "\n",
        "# tf.get_default_graph()\n",
        "# tf.compat.v1.get_default_graph()\n",
        "# saver = tf.compat.v1.train.import_meta_graph(os.path.join(args_weightspath, args_metaname))\n",
        "# saver.restore(sess, os.path.join(args_weightspath, args_ckptname))\n",
        "\n",
        "# graph = tf.get_default_graph()\n",
        "\n",
        "# image_tensor = graph.get_tensor_by_name(args_in_tensorname)\n",
        "# pred_tensor = graph.get_tensor_by_name(args_out_tensorname)\n",
        "\n",
        "sess = tf.Session()\n",
        "tf.get_default_graph()\n",
        "saver = tf.train.import_meta_graph(os.path.join(args_weightspath, args_metaname))\n",
        "saver.restore(sess, os.path.join(args_weightspath, args_ckptname))\n",
        "\n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "# image_tensor = graph.get_tensor_by_name(args_in_tensorname)\n",
        "# pred_tensor = graph.get_tensor_by_name(args_out_tensorname)\n",
        "\n",
        "# if args_is_medusa_backbone:\n",
        "#     x = process_image_file(args_imagepath, args_input_size, top_percent=0, crop=False)\n",
        "#     x = x.astype('float32') / 255.0\n",
        "#     medusa_image_tensor = graph.get_tensor_by_name(args_in_tensorname_medusa)\n",
        "#     medusa_x = process_image_file_medusa(args_imagepath, args_input_size_medusa)\n",
        "#     feed_dict = {\n",
        "#                 medusa_image_tensor: np.expand_dims(medusa_x, axis=0),\n",
        "#                 image_tensor: np.expand_dims(x, axis=0),\n",
        "#             } \n",
        "# else:\n",
        "#     x = process_image_file(args_imagepath, args_input_size, top_percent=args_top_percent)\n",
        "#     x = x.astype('float32') / 255.0\n",
        "#     feed_dict = {image_tensor: np.expand_dims(x, axis=0)}\n",
        "\n",
        "# print(\"feed_dict\", feed_dict)\n",
        "\n",
        "# pred = sess.run(pred_tensor, feed_dict=feed_dict)\n",
        "\n",
        "# # print(\"pred\", pred)\n",
        "\n",
        "# # print('Prediction: {}'.format(inv_mapping[pred.argmax(axis=1)[0]]))\n",
        "# # print('Confidence')\n",
        "# # print(' '.join('{}: {:.3f}'.format(cls.capitalize(), pred[0][i]) for cls, i in mapping.items()))\n",
        "# # print('**DISCLAIMER**')\n",
        "# # print('Do not use this prediction for self-diagnosis. You should check with your local authorities for the latest advice on seeking medical assistance.')\n",
        "\n"
      ],
      "metadata": {
        "id": "YcsxMFs-JKh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph)"
      ],
      "metadata": {
        "id": "uhaOZ8HqMIZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "VxvS-7DsJPOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prnet_full.py"
      ],
      "metadata": {
        "id": "oh9pdkBbLhfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prnet_full.py  - https://github.com/liguohao96/pytorch-prnet/blob/7475fa1c593977a9989fd80bec7ead686f075811/prnet_full.py#L123\n",
        "from torch import nn\n",
        "\n",
        "def padding_same_conv2d(input_size, in_c, out_c, kernel_size=4, stride=1):\n",
        "    output_size = input_size // stride\n",
        "    padding_num = stride * (output_size - 1) - input_size + kernel_size\n",
        "    if padding_num % 2 == 0:\n",
        "        return nn.Sequential(nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=stride, padding=padding_num // 2, bias=False))\n",
        "    else:\n",
        "        return nn.Sequential(\n",
        "            nn.ConstantPad2d((padding_num // 2, padding_num // 2 + 1, padding_num // 2, padding_num // 2 + 1), 0),\n",
        "            nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=stride, padding=0, bias=False)\n",
        "        )\n",
        "\n",
        "# import threading\n",
        "class resBlock(nn.Module):\n",
        "    instance_num = 0\n",
        "    # instance_num_lock = threading.Lock()\n",
        "    def __init__(self, in_c, out_c, kernel_size=4, stride=1, input_size=None):\n",
        "        super().__init__()\n",
        "        self.instance_idx = self.__class__.instance_num\n",
        "        self.__class__.instance_num += 1\n",
        "        assert kernel_size == 4\n",
        "        self.shortcut = lambda x: x\n",
        "        self.tf_map = {}\n",
        "        if in_c != out_c:\n",
        "            self.shortcut = nn.Conv2d(in_c, out_c, kernel_size=1, stride=stride, bias=False)\n",
        "            self.tf_map['{}/shortcut/weights'.format(self.instance_name())] = 'shortcut.weight'\n",
        "\n",
        "        main_layers = [\n",
        "            nn.Conv2d(in_c, out_c // 2, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_c // 2, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        self.tf_map['{}/Conv/weights'.format(self.instance_name())] = 'main.0.weight'\n",
        "        self.tf_map['{}/Conv/BatchNorm/gamma'.format(self.instance_name())] = 'main.1.weight'\n",
        "        self.tf_map['{}/Conv/BatchNorm/beta'.format(self.instance_name())] = 'main.1.bias'\n",
        "        self.tf_map['{}/Conv/BatchNorm/moving_mean'.format(self.instance_name())] = 'main.1.running_mean'\n",
        "        self.tf_map['{}/Conv/BatchNorm/moving_variance'.format(self.instance_name())] = 'main.1.running_var'\n",
        "\n",
        "        main_layers.extend([\n",
        "            *padding_same_conv2d(input_size, out_c // 2, out_c // 2, kernel_size=kernel_size, stride=stride),\n",
        "            nn.BatchNorm2d(out_c // 2, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True)])\n",
        "        conv_idx = len(main_layers) - 3\n",
        "        self.tf_map['{}/Conv_1/weights'.format(self.instance_name())] = 'main.{}.weight'.format(conv_idx)\n",
        "        self.tf_map['{}/Conv_1/BatchNorm/gamma'.format(self.instance_name())] = 'main.{}.weight'.format(conv_idx+1)\n",
        "        self.tf_map['{}/Conv_1/BatchNorm/beta'.format(self.instance_name())] = 'main.{}.bias'.format(conv_idx+1)\n",
        "        self.tf_map['{}/Conv_1/BatchNorm/moving_mean'.format(self.instance_name())] = 'main.{}.running_mean'.format(conv_idx+1)\n",
        "        self.tf_map['{}/Conv_1/BatchNorm/moving_variance'.format(self.instance_name())] = 'main.{}.running_var'.format(conv_idx+1)\n",
        "\n",
        "        main_layers.extend(\n",
        "            padding_same_conv2d(input_size, out_c // 2, out_c, kernel_size=1, stride=1)\n",
        "        )\n",
        "        conv_idx = len(main_layers) - 1\n",
        "        self.tf_map['{}/Conv_2/weights'.format(self.instance_name())] = 'main.{}.weight'.format(conv_idx)\n",
        "        self.main = nn.Sequential(*main_layers)\n",
        "        self.activate = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_c, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.tf_map['{}/BatchNorm/gamma'.format(self.instance_name())] = 'activate.0.weight'\n",
        "        self.tf_map['{}/BatchNorm/beta'.format(self.instance_name())]  = 'activate.0.bias'\n",
        "        self.tf_map['{}/BatchNorm/moving_mean'.format(self.instance_name())]      = 'activate.0.running_mean'\n",
        "        self.tf_map['{}/BatchNorm/moving_variance'.format(self.instance_name())]  = 'activate.0.running_var'\n",
        "\n",
        "    def instance_name(self):\n",
        "        return 'resBlock' if self.instance_idx == 0 else 'resBlock_{}'.format(self.instance_idx)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut_x = self.shortcut(x)\n",
        "        main_x = self.main(x)\n",
        "        x = self.activate(shortcut_x + main_x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class upBlock(nn.Module):\n",
        "    convtranspose_num = 0\n",
        "    def __init__(self, in_c, out_c, conv_num=2):\n",
        "        super().__init__()\n",
        "        self.tf_map = {}\n",
        "        additional_conv = []\n",
        "        layer_length = 4\n",
        "\n",
        "        self.convtrans_idx = self.__class__.convtranspose_num\n",
        "        self.__class__.convtranspose_num += 1\n",
        "        self.tf_map['{}/weights'.format(self.convtranspose_name())] = 'main.0.weight'\n",
        "        self.tf_map['{}/BatchNorm/gamma'.format(self.convtranspose_name())] = 'main.1.weight'\n",
        "        self.tf_map['{}/BatchNorm/beta'.format(self.convtranspose_name())] = 'main.1.bias'\n",
        "        self.tf_map['{}/BatchNorm/moving_mean'.format(self.convtranspose_name())] = 'main.1.running_mean'\n",
        "        self.tf_map['{}/BatchNorm/moving_variance'.format(self.convtranspose_name())] = 'main.1.running_var'\n",
        "        for i in range(1, conv_num+1):\n",
        "            self.convtrans_idx = self.__class__.convtranspose_num\n",
        "            self.__class__.convtranspose_num += 1\n",
        "            additional_conv += [\n",
        "                nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "                nn.ConvTranspose2d(out_c, out_c, kernel_size=4, stride=1, padding=3, bias=False),\n",
        "                nn.BatchNorm2d(out_c, eps=0.001, momentum=0.001),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            self.tf_map['{}/weights'.format(self.convtranspose_name())] = 'main.{}.weight'.format(i*layer_length + 0)\n",
        "            self.tf_map['{}/BatchNorm/gamma'.format(self.convtranspose_name())] = 'main.{}.weight'.format(i*layer_length + 1)\n",
        "            self.tf_map['{}/BatchNorm/beta'.format(self.convtranspose_name())] = 'main.{}.bias'.format(i*layer_length + 1)\n",
        "            self.tf_map['{}/BatchNorm/moving_mean'.format(self.convtranspose_name())] = 'main.{}.running_mean'.format(i*layer_length + 1)\n",
        "            self.tf_map['{}/BatchNorm/moving_variance'.format(self.convtranspose_name())] = 'main.{}.running_var'.format(i*layer_length + 1)\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # nn.ConstantPad2d((0, 1, 0, 1), 0),\n",
        "            nn.ConvTranspose2d(in_c, out_c, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_c, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True),\n",
        "            *additional_conv\n",
        "            )\n",
        "\n",
        "    def convtranspose_name(self):\n",
        "        return 'Conv2d_transpose' if self.convtrans_idx == 0 else 'Conv2d_transpose_{}'.format(self.convtrans_idx)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.main(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PRNet(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel=3):\n",
        "        super().__init__()\n",
        "        size = 16\n",
        "        self.input_conv = nn.Sequential( #*[\n",
        "            *padding_same_conv2d(256, in_channel, size, kernel_size=4, stride=1),  # 256x256x16\n",
        "            nn.BatchNorm2d(size, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True)\n",
        "            # ]\n",
        "        ) \n",
        "        self.tf_map = {}\n",
        "        conv_idx = len(self.input_conv) - 3\n",
        "        self.tf_map['resfcn256/Conv/weights'] = 'input_conv.{}.weight'.format(conv_idx)\n",
        "        self.tf_map['resfcn256/Conv/BatchNorm/gamma'] = 'input_conv.{}.weight'.format(conv_idx + 1)\n",
        "        self.tf_map['resfcn256/Conv/BatchNorm/beta'] = 'input_conv.{}.bias'.format(conv_idx + 1)\n",
        "        self.tf_map['resfcn256/Conv/BatchNorm/moving_mean'] = 'input_conv.{}.running_mean'.format(conv_idx + 1)\n",
        "        self.tf_map['resfcn256/Conv/BatchNorm/moving_variance'] = 'input_conv.{}.running_var'.format(conv_idx + 1)\n",
        "        self.down_conv_1 = resBlock(size, size * 2, kernel_size=4, stride=2, input_size=256)  # 128x128x32\n",
        "        self.down_conv_2 = resBlock(size * 2, size * 2, kernel_size=4, stride=1, input_size=128)  # 128x128x32\n",
        "        self.down_conv_3 = resBlock(size * 2, size * 4, kernel_size=4, stride=2, input_size=128)  # 64x64x64\n",
        "        self.down_conv_4 = resBlock(size * 4, size * 4, kernel_size=4, stride=1, input_size=64)  # 64x64x64\n",
        "        self.down_conv_5 = resBlock(size * 4, size * 8, kernel_size=4, stride=2, input_size=64)  # 32x32x128\n",
        "        self.down_conv_6 = resBlock(size * 8, size * 8, kernel_size=4, stride=1, input_size=32)  # 32x32x128\n",
        "        self.down_conv_7 = resBlock(size * 8, size * 16, kernel_size=4, stride=2, input_size=32)  # 16x16x256\n",
        "        self.down_conv_8 = resBlock(size * 16, size * 16, kernel_size=4, stride=1, input_size=16)  # 16x16x256\n",
        "        self.down_conv_9 = resBlock(size * 16, size * 32, kernel_size=4, stride=2, input_size=16)  # 8x8x512\n",
        "        self.down_conv_10 = resBlock(size * 32, size * 32, kernel_size=4, stride=1, input_size=8)  # 8x8x512\n",
        "\n",
        "        self.center_conv = nn.Sequential(\n",
        "            nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "            nn.ConvTranspose2d(size * 32, size * 32, kernel_size=4, stride=1, padding=3, bias=False),  # 8x8x512\n",
        "            nn.BatchNorm2d(size * 32, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/weights'] = 'center_conv.1.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/BatchNorm/gamma'] = 'center_conv.2.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/BatchNorm/beta'] = 'center_conv.2.bias'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/BatchNorm/moving_mean'] = 'center_conv.2.running_mean'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/BatchNorm/moving_variance'] = 'center_conv.2.running_var'\n",
        "        upBlock.convtranspose_num = 1\n",
        "\n",
        "        self.up_conv_5 = upBlock(size * 32, size * 16)  # 16x16x256\n",
        "        self.up_conv_4 = upBlock(size * 16, size * 8)  # 32x32x128\n",
        "        self.up_conv_3 = upBlock(size * 8, size * 4)  # 64x64x64\n",
        "\n",
        "        self.up_conv_2 = upBlock(size * 4, size * 2, 1)  # 128x128x32\n",
        "        self.up_conv_1 = upBlock(size * 2, size, 1)  # 256x256x16\n",
        "\n",
        "        convtranspose_idx = upBlock.convtranspose_num\n",
        "        self.output_conv = nn.Sequential(\n",
        "            nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "            nn.ConvTranspose2d(size, 3, kernel_size=4, stride=1, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(3, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "            nn.ConvTranspose2d(3, 3, kernel_size=4, stride=1, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(3, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "            nn.ConvTranspose2d(3, 3, kernel_size=4, stride=1, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(3, eps=0.001, momentum=0.001),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/weights'.format(convtranspose_idx)] = 'output_conv.1.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/gamma'.format(convtranspose_idx)] = 'output_conv.2.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/beta'.format(convtranspose_idx)] = 'output_conv.2.bias'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_mean'.format(convtranspose_idx)] = 'output_conv.2.running_mean'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_variance'.format(convtranspose_idx)] = 'output_conv.2.running_var'\n",
        "\n",
        "        convtranspose_idx += 1\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/weights'.format(convtranspose_idx)] = 'output_conv.5.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/gamma'.format(convtranspose_idx)] = 'output_conv.6.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/beta'.format(convtranspose_idx)] = 'output_conv.6.bias'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_mean'.format(convtranspose_idx)] = 'output_conv.6.running_mean'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_variance'.format(convtranspose_idx)] = 'output_conv.6.running_var'\n",
        "\n",
        "        convtranspose_idx += 1\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/weights'.format(convtranspose_idx)] = 'output_conv.9.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/gamma'.format(convtranspose_idx)] = 'output_conv.10.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/beta'.format(convtranspose_idx)] = 'output_conv.10.bias'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_mean'.format(convtranspose_idx)] = 'output_conv.10.running_mean'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_variance'.format(convtranspose_idx)] = 'output_conv.10.running_var'\n",
        "        self.collact_names()\n",
        "    \n",
        "    def collact_names(self):\n",
        "        for name, child in self.named_children():\n",
        "            if hasattr(child, 'tf_map'):\n",
        "                child_map = getattr(child, 'tf_map')\n",
        "                for k, v in child_map.items():\n",
        "                    self.tf_map['resfcn256/{}'.format(k)] = '{}.{}'.format(name, v)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_conv(x)\n",
        "        x = self.down_conv_1(x)\n",
        "        x = self.down_conv_2(x)\n",
        "        x = self.down_conv_3(x)\n",
        "        x = self.down_conv_4(x)\n",
        "        x = self.down_conv_5(x)\n",
        "        x = self.down_conv_6(x)\n",
        "        x = self.down_conv_7(x)\n",
        "        x = self.down_conv_8(x)\n",
        "        x = self.down_conv_9(x)\n",
        "        x = self.down_conv_10(x)\n",
        "\n",
        "        x = self.center_conv(x)\n",
        "\n",
        "        x = self.up_conv_5(x)\n",
        "        x = self.up_conv_4(x)\n",
        "        x = self.up_conv_3(x)\n",
        "        x = self.up_conv_2(x)\n",
        "        x = self.up_conv_1(x)\n",
        "        x = self.output_conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "twi4fXMULSRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tf2torch "
      ],
      "metadata": {
        "id": "L_Bc33spLlc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf2torch - https://github.com/liguohao96/pytorch-prnet/blob/master/tf2torch.py\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "torch_model = PRNet(3, 3)\n",
        "torch_dict = OrderedDict()\n",
        "\n",
        "for node in graph.as_graph_def().node:\n",
        "    # print(node)\n",
        "    if node.name in torch_model.tf_map:\n",
        "        torch_name = torch_model.tf_map[node.name]\n",
        "        data = graph.get_operation_by_name(node.name).outputs[0]\n",
        "        data_np = sess.run(data)\n",
        "        if len(data_np.shape) > 1:\n",
        "            # weight layouts  |   tensorflow   |     pytorch     |  transpose   |\n",
        "            # conv2d_transpose (H, W, out, in) -> (in, out, H, W)  (3, 2, 0, 1)\n",
        "            # conv2d           (H, W, in, out) -> (out, in, H, W)  (3, 2, 0, 1)\n",
        "            torch_dict[torch_name] = torch.tensor(np.transpose(data_np, (3, 2, 0, 1)).astype(np.float32))\n",
        "        else:\n",
        "            torch_dict[torch_name] = torch.tensor(data_np.astype(np.float32))\n",
        "    else:\n",
        "        if node.name.find('save') == -1:\n",
        "            pass\n",
        "            print('not in {}'.format(node.name))\n",
        "torch.save(torch_dict, 'from_tf.pth')\n"
      ],
      "metadata": {
        "id": "R0iLtjAhHvN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch_dict)"
      ],
      "metadata": {
        "id": "Di-M4dEqOJIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cam -tf"
      ],
      "metadata": {
        "id": "TI7CzoXQaAWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model\n"
      ],
      "metadata": {
        "id": "n8DlofxknEuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_r50 = ResNet50(weights='imagenet', include_top=False)\n",
        "# model_r50.summary()\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define a simple sequential model\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    keras.layers.Dense(512, activation=tf.keras.activations.relu, input_shape=(784,)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(10, activation=tf.keras.activations.softmax)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# Create a basic model instance\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TuKs_2n6aCqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver = tf.train.import_meta_graph('models/COVIDNet-CXR-2/model.meta')\n",
        "    saver.restore(sess, \"models/COVIDNet-CXR-2/model\")\n"
      ],
      "metadata": {
        "id": "UQfbaisAbvxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "saver = tf.train.import_meta_graph('models/COVIDNet-CXR-2/model.meta')\n",
        "sess = tf.Session()\n",
        "saver.restore(sess, \"models/COVIDNet-CXR-2/model\")\n",
        "# result = sess.run(\"v4:0\", feed_dict={\"v1:0\": 12.0, \"v2:0\": 3.3})\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "mdfggY2niXwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "checkpoint_path = \"models/COVIDNet-CXR-2/model.data-00000-of-00001.ckpt\"\n",
        "test_images = \"assets/ex-covid.jpeg\"\n",
        "test_labels = \"labels\"\n",
        "model.load_weights(checkpoint_path)\n",
        "loss,acc = model.evaluate(test_image s,  test_labels, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "id": "MKc-KWxggulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "aFxaWR7wnHac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os, argparse\n",
        "\n",
        " \n",
        " \n",
        "def grad_cam(prob_name, label, layer_name, sess, feed_dict, nb_classes):\n",
        "    \"\"\"\n",
        "    prob_name为softmax输出层节点名, label标签, layer_name最后一层卷积层的节点名, sess,                 \n",
        "    feed_dict, nb_classes分类数\n",
        "    \"\"\"\n",
        " \n",
        "    prob = sess.graph.get_tensor_by_name(prob_name + ':0')\n",
        "    print(\"prob\", prob)\n",
        "    print(\"tf.one_hot([label]\", tf.one_hot([label], nb_classes))\n",
        "    loss = tf.multiply(prob, tf.one_hot([label], nb_classes))\n",
        "    print(\"loss\", loss)\n",
        "    reduced_loss = tf.reduce_sum(loss[0])\n",
        "    conv_output = sess.graph.get_tensor_by_name(layer_name + ':0')\n",
        "    print(\"reduced_loss\", reduced_loss)\n",
        "    print(\"conv_output\", conv_output)\n",
        "    grads = tf.gradients(reduced_loss, conv_output)[0] # d loss / d conv\n",
        "    print(\"grads\", grads)\n",
        "    # output, grads_val = sess.run([conv_output, grads], feed_dict=feed_dict)\n",
        "    output = sess.run(conv_output, feed_dict=feed_dict)\n",
        "    grads_val = sess.run(grads, feed_dict=feed_dict)\n",
        "    print(output.shape)\n",
        "    weights = np.mean(grads_val, axis=(1, 2)) # average pooling\n",
        "    cams = np.sum(weights * output, axis=3)\n",
        "    return cams\n",
        " \n",
        " \n",
        "def save_cam(cam, image, save_path):\n",
        "    \"\"\"\n",
        "    save Grad-CAM images\n",
        "    \"\"\"\n",
        " \n",
        "    cam = cam[0] # the first GRAD-CAM for the first image in  batch\n",
        "    # image = np.uint8(image_batch[0][:, :, ::-1] * 255.0) # RGB -> BGR\n",
        "    cam = cv.resize(cam, (224, 224)) # enlarge heatmap\n",
        "    cam = np.maximum(cam, 0)\n",
        "    heatmap = cam / np.max(cam) # normalize\n",
        "    cam = cv.applyColorMap(np.uint8(255 * heatmap), cv.COLORMAP_JET) \n",
        "    # balck-and-white to color\n",
        "    cam = np.float32(cam) + np.float32(image) # everlay heatmap onto the image\n",
        "    cam = 255 * cam / np.max(cam)\n",
        "    cam = np.uint8(cam)\n",
        "    \n",
        "    cv.imwrite(save_path+\"cam.jpg\", cam)\n",
        "    cv.imwrite(save_path+\"heatmap.jpg\", (heatmap * 255.0).astype(np.uint8))\n",
        "    cv.imwrite(save_path+\"segmentation.jpg\", (heatmap[:, :, None].astype(float) * image).astype(np.uint8))\n",
        " \n",
        "    return  cam\n",
        "\n",
        "def crop_top(img, percent=0.15):\n",
        "    offset = int(img.shape[0] * percent)\n",
        "    return img[offset:]\n",
        "\n",
        "def central_crop(img):\n",
        "    size = min(img.shape[0], img.shape[1])\n",
        "    offset_h = int((img.shape[0] - size) / 2)\n",
        "    offset_w = int((img.shape[1] - size) / 2)\n",
        "    return img[offset_h:offset_h + size, offset_w:offset_w + size]\n",
        "\n",
        "\n",
        "def process_image_file(filepath, size, top_percent=0.08, crop=True):\n",
        "    img = cv.imread(filepath)\n",
        "    # print(\"filepath\", filepath)\n",
        "    img = crop_top(img, percent=top_percent)\n",
        "    if crop:\n",
        "        img = central_crop(img)\n",
        "    img = cv.resize(img, (size, size))\n",
        "    return img\n",
        " \n",
        " \n",
        "# def main():\n",
        "IMAGE_PATH =\"assets/ex-covid.jpeg\"\n",
        "output_node_names = \"norm_dense_2/Softmax\"\n",
        "final_conv_name=\"conv4_block3_1_bn/gamma/initial_value\"\n",
        "model_path = 'models/COVIDNet-CXR-2/model'\n",
        "\n",
        "\n",
        "# ckpt = tf.train.get_checkpoint_state(model_path)  # 通过检查点文件锁定最新的模型\n",
        "saver = tf.train.import_meta_graph(model_path + '.meta')  # 载入图结构，保存在.meta文件中\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, model_path)  # 载入参数，参数保存在两个文件中，不过restore会自己寻找\n",
        "\n",
        "\n",
        "    img_m = cv.imread(IMAGE_PATH)\n",
        "    img_m = cv.resize(img_m, (224, 224))\n",
        "\n",
        "\n",
        "    img_m = tf.cast(img_m, tf.float32)\n",
        "    img_m = tf.reshape(img_m, [224, 224, 3])\n",
        "    img_m_f= sess.run([img_m])\n",
        "\n",
        "\n",
        "    input_image_tensor_m = sess.graph.get_tensor_by_name(\"input_1:0\")\n",
        "\n",
        "    # input_is_training_tensor = sess.graph.get_tensor_by_name(\"input/is_training:0\")\n",
        "\n",
        "    x = process_image_file('assets/ex-covid.jpeg', 480, top_percent=0.08)\n",
        "    x = x.astype('float32') / 255.0\n",
        "    # feed_dict = {input_image_tensor_m: np.expand_dims(x, axis=0)}\n",
        "\n",
        "    cam=grad_cam(prob_name=output_node_names, label=0, \n",
        "                  layer_name=final_conv_name, sess=sess, \n",
        "                  feed_dict={input_image_tensor_m: np.expand_dims(x, axis=0)}, \n",
        "                  nb_classes=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    dst_m=save_cam(cam, img_m, 'm')\n",
        "    cv.imshow('dst_m_v',dst_m)\n",
        "    cv.waitKey(0)\n",
        "\n",
        "    # print(cam)\n",
        " \n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "wUlWPFb6nIqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o08IF8gjxd0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
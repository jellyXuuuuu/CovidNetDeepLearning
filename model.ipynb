{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1OC5IMqG59dZ9zl4TEVfI4UcmIZQIDOKl",
      "authorship_tag": "ABX9TyM3+Thl/DyR6VzSGr4Cdtvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jellyXuuuuu/CovidNetDeepLearning/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t_69Gk24AUc",
        "outputId": "1130f1f9-b506-42e9-8a01-f9b3fd56f0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 84 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.19.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.50.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.38.4)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (2.1.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.1.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print( tf.__version__ )"
      ],
      "metadata": {
        "id": "UPEQR19K4EMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# def"
      ],
      "metadata": {
        "id": "zLDhHe5f3ljR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQFVCh963c-t"
      },
      "outputs": [],
      "source": [
        "def read_graph_from_ckpt(ckpt_path,input_names,output_name ):   \n",
        "    saver = tf.train.import_meta_graph(ckpt_path+'.meta',clear_devices=True)\n",
        "    graph = tf.get_default_graph()\n",
        "    with tf.Session( graph=graph) as sess:\n",
        "        sess.run(tf.global_variables_initializer()) \n",
        "        saver.restore(sess,ckpt_path) \n",
        "        output_tf =graph.get_tensor_by_name(output_name) \n",
        "        pb_graph = tf.graph_util.convert_variables_to_constants( sess, graph.as_graph_def(), [output_tf.op.name]) \n",
        "     \n",
        "    with tf.Graph().as_default() as g:\n",
        "        tf.import_graph_def(pb_graph, name='')  \n",
        "    with tf.Session(graph=g) as sess:\n",
        "        OPS=get_ops_from_pb(g,input_names,output_name)\n",
        "    return OPS\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_graph_from_pb(tf_model_path ,input_names,output_name):  \n",
        "    with open(tf_model_path, 'rb') as f:\n",
        "        serialized = f.read() \n",
        "    tf.reset_default_graph()\n",
        "    gdef = tf.GraphDef()\n",
        "    gdef.ParseFromString(serialized) \n",
        "    with tf.Graph().as_default() as g:\n",
        "        tf.import_graph_def(gdef, name='') \n",
        "    \n",
        "    with tf.Session(graph=g) as sess: \n",
        "        OPS=get_ops_from_pb(g,input_names,output_name)\n",
        "    return OPS"
      ],
      "metadata": {
        "id": "c5s06XHm3og_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ops_from_pb(graph,input_names,output_name,save_ori_network=True):\n",
        "    if save_ori_network:\n",
        "        with open('ori_network.txt','w+') as w: \n",
        "            OPS=graph.get_operations()\n",
        "            for op in OPS:\n",
        "                txt = str([v.name for v in op.inputs])+'---->'+op.type+'--->'+str([v.name for v in op.outputs])\n",
        "                w.write(txt+'\\n') \n",
        "    inputs_tf = [graph.get_tensor_by_name(input_name) for input_name in input_names]\n",
        "    output_tf =graph.get_tensor_by_name(output_name) \n",
        "    OPS =get_ops_from_inputs_outputs(graph, inputs_tf,[output_tf] ) \n",
        "    with open('network.txt','w+') as w: \n",
        "        for op in OPS:\n",
        "            txt = str([v.name for v in op.inputs])+'---->'+op.type+'--->'+str([v.name for v in op.outputs])\n",
        "            w.write(txt+'\\n') \n",
        "    OPS = sort_ops(OPS)\n",
        "    OPS = merge_layers(OPS)\n",
        "    return OPS"
      ],
      "metadata": {
        "id": "cQcVXB2q35fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# from github"
      ],
      "metadata": {
        "id": "SmaQD5Dv4r3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# util.py\n",
        "\n",
        "def create_graph(ops):\n",
        "  \n",
        "  n = len(ops)\n",
        "  G = [[] for i in range(n)]\n",
        "  op_name_to_index = dict() \n",
        "  for i, op in enumerate(ops):\n",
        "    op_name_to_index[op.name] = i \n",
        "  for i, op in enumerate(ops):\n",
        "    for inp in op.inputs:\n",
        "      G[op_name_to_index[inp.op.name]].append(i)\n",
        "\n",
        "  return G \n",
        "def push_stack(stack, node, in_stack,ops):\n",
        "  stack.append(node)\n",
        "  if node in in_stack:\n",
        "    print('cycles---->',ops[node])\n",
        "    raise ValueError('Graph has cycles.')\n",
        "  else:\n",
        "    in_stack[node] = True\n",
        "\n",
        "def get_unvisited_child(G, node, not_visited):\n",
        "  for child in G[node]:\n",
        "    if child in not_visited:\n",
        "      return child\n",
        "  return -1\n",
        "\n",
        " \n",
        "#对计算节点排序，使得每个计算节点所依赖的计算节点在前面\n",
        "def  sort_ops(ops):  \n",
        "\n",
        "  G = create_graph(ops)\n",
        "  n = len(ops) \n",
        "  topological_label = [-1 for i in range(n)]\n",
        "  stack = []\n",
        "  in_stack = dict()\n",
        "  not_visited = dict.fromkeys([i for i in range(n)])\n",
        "  label_counter = n-1\n",
        "\n",
        "  while len(not_visited) > 0:\n",
        "    node = list(not_visited.keys())[0]\n",
        "    push_stack(stack, node, in_stack,ops)\n",
        "    while len(stack) > 0:\n",
        "      node = get_unvisited_child(G, stack[-1], not_visited)\n",
        "      if node != -1:\n",
        "        push_stack(stack, node, in_stack,ops)\n",
        "      else:\n",
        "        node = stack.pop()\n",
        "        in_stack.pop(node)\n",
        "        not_visited.pop(node)\n",
        "        topological_label[node] = label_counter\n",
        "        label_counter -= 1\n",
        "\n",
        "  return [x for _, x in sorted(zip(topological_label, ops))]\n",
        " \n",
        "def _get_ops_in_path(from_tensors,to_tensors,ops):\n",
        "    invalid_ops=[]\n",
        "    valid_ops=ops.copy()\n",
        "    removed=[]\n",
        "    find_invalid=True\n",
        "    in_tensors = [tensor.name for tensor in from_tensors] \n",
        "    out_tensors = [tensor.name for tensor in to_tensors]\n",
        "    while find_invalid:\n",
        "      find_invalid=False\n",
        "      for op in valid_ops:  \n",
        "          for input in op.inputs:\n",
        "              if input.name in out_tensors:\n",
        "                out_tensors=out_tensors+[out.name for out in op.outputs]\n",
        "                find_invalid=True\n",
        "                break\n",
        "          for output in op.outputs:\n",
        "              if output.name in in_tensors:\n",
        "                in_tensors=in_tensors+[input.name for input in op.inputs]\n",
        "                find_invalid=True\n",
        "                break\n",
        "          if find_invalid:\n",
        "             invalid_ops.append(op)\n",
        "    \n",
        "      valid_ops=[op for op in valid_ops if not op in invalid_ops]       \n",
        "    print('inputs===========================')\n",
        "    print([op.type for op in invalid_ops if 'Con' in op.name])\n",
        "    print('end inputs========================')\n",
        "    print(out_tensors)\n",
        "    return valid_ops"
      ],
      "metadata": {
        "id": "wOyp10pZ4lV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nodeObj.py\n",
        "\n",
        "class OPNode:\n",
        "    def __init__(self,node):\n",
        "        if not node is None:\n",
        "            self.node=node \n",
        "            self.type=node.type\n",
        "            self.inputs=node.inputs\n",
        "            self.outputs=node.outputs\n",
        "            self.name = node.name \n",
        "    @classmethod\n",
        "    def my_init(cls,type,inputs,outputs,name):\n",
        "        thiz = cls(None)\n",
        "        thiz.node=None\n",
        "        thiz.type= type\n",
        "        thiz.inputs= inputs\n",
        "        thiz.outputs= outputs\n",
        "        thiz.name = name \n",
        "        return thiz\n",
        "    def __eq__(self,obj):\n",
        "        if obj is None:\n",
        "            return False\n",
        "        if obj.name:\n",
        "            return self.name== obj.name\n",
        "        return False\n",
        "    def __hash__(self):\n",
        "        return self.node.__hash__()\n",
        "class TSNode:\n",
        "    def __init__(self,node,op_node):\n",
        "        self.node=node  \n",
        "        self.op=op_node \n",
        "        self.dtype=node.dtype\n",
        "        self.name = node.name \n",
        "        self.shape=node.shape \n",
        "        self.get_shape=node.get_shape\n",
        "        self.next_ops=node.consumers()\n",
        "   \n",
        "    def consumers(self):\n",
        "        return self.next_ops      \n",
        "    def eval(self):\n",
        "        return self.node.eval()\n",
        "    def __eq__(self,obj):\n",
        "        if obj is None:\n",
        "            return False\n",
        "        if obj.name:\n",
        "            return self.name== obj.name\n",
        "        return False\n",
        "    def __hash__(self):\n",
        "        return self.node.__hash__()"
      ],
      "metadata": {
        "id": "ePeGv9r343YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph_builder.py\n",
        "\n",
        "# from NodeObj import OPNode,TSNode\n",
        "class GNode:\n",
        "    def __init__(self,id,node):\n",
        "        self.node=node\n",
        "        self.name=node.name\n",
        "        self.id=id\n",
        "        self.pre_ids=None\n",
        "        self.next_ids=None\n",
        "        if isinstance(node,OPNode):\n",
        "            self.type=node.type\n",
        "            self.pre=node.inputs\n",
        "            self.next=node.outputs\n",
        "        else:\n",
        "            self.pre=[node.op]\n",
        "            self.next=node.consumers()\n",
        "            self.shape=node.shape\n",
        "    \n",
        "    def __str__(self):\n",
        "        if isinstance(self.node,OPNode):\n",
        "            return 'op☯'+str(self.id)+'☯'+self.type+'☯'+self.name+'☯'\\\n",
        "                   +' '.join([str(id) for id in self.pre_ids])+'☯'\\\n",
        "                   +' '.join([str(id) for id in self.next_ids])\n",
        "        else:\n",
        "             \n",
        "            return 'ts☯'+str(self.id)+'☯'+str(self.shape)+'☯'+self.name+'☯'\\\n",
        "                   +' '.join([str(id) for id in self.pre_ids])+'☯'\\\n",
        "                   +' '.join([str(id) for id in self.next_ids])\n",
        "class GraphBuilder:\n",
        "    def __init__(self,file_dst):\n",
        "        self.file_dst=file_dst  \n",
        "        self.layers=[]\n",
        "        self.nodes_in_layers=set()\n",
        "        self.node_map=dict()\n",
        "        self.id=0\n",
        "    def __next_id(self):\n",
        "        self.id=self.id+1\n",
        "        return self.id \n",
        "    def __add_node_at(self,g_node,layer_idx):\n",
        "       \n",
        "        max_layer=len(self.layers)\n",
        "        if layer_idx>=max_layer:\n",
        "            self.layers.append([g_node])\n",
        "        else:\n",
        "            nodes=self.layers[layer_idx]\n",
        "            nodes.append(g_node) \n",
        "        self.nodes_in_layers.add(g_node.name)\n",
        "    def __to_gnode(self,nodes):\n",
        "        gnodes=[]\n",
        "        for node in nodes:\n",
        "            g_node=self.node_map.get(node.name,None)\n",
        "            if g_node is None:\n",
        "                id=self.__next_id()\n",
        "                g_node=GNode(id,node)\n",
        "                self.node_map[node.name]=g_node\n",
        "            gnodes.append(g_node)\n",
        "        return gnodes\n",
        "    def __max_ts_idx(self,ts_list):\n",
        "        max_idx=-1\n",
        "        for ts in ts_list:\n",
        "            for idx,nodes in enumerate(self.layers):\n",
        "                if ts in nodes and (idx>max_idx):\n",
        "                    max_idx=idx\n",
        "        return max_idx\n",
        "    def __mv_node_to_layer(self,gnode,layer_idx):\n",
        "        for idx,nodes in enumerate(self.layers):\n",
        "            if gnode in nodes:\n",
        "                if not idx==layer_idx:\n",
        "                   nodes.remove(gnode)\n",
        "                   self.layers[layer_idx].append(gnode)\n",
        "                break\n",
        "    def __print_layers(self):\n",
        "        for idx,layer in enumerate(self.layers):\n",
        "            dnames=[]\n",
        "            for node in layer:\n",
        "                dnames.append(node.name.split('/')[-1])\n",
        "            print('layer-->',idx,','.join(dnames))          \n",
        "    def add_op(self,op_node):\n",
        "         \n",
        "        inputs = self.__to_gnode(op_node.inputs)\n",
        "        op_gnode = self.__to_gnode([op_node])[0]\n",
        "        outputs = self.__to_gnode(op_node.outputs)\n",
        "        max_idx=self.__max_ts_idx(inputs)\n",
        "        # print('max_idx-->',max_idx)\n",
        "        \n",
        "        if max_idx<0:\n",
        "            for input in inputs:\n",
        "                self.__add_node_at(input,0)\n",
        "            self.__add_node_at(op_gnode,1)\n",
        "            op_idx=1\n",
        "        else:\n",
        "            op_idx=max_idx+1\n",
        "            self.__add_node_at(op_gnode,max_idx+1)\n",
        "            for input in inputs: \n",
        "                if input.name in self.nodes_in_layers:#已经添加过\n",
        "                    continue\n",
        "                else:\n",
        "                    self.__add_node_at(input,max_idx)\n",
        "                     \n",
        "        if len(outputs)>0:\n",
        "            output=outputs[0]\n",
        "            if not output.name in self.nodes_in_layers:#未添加 \n",
        "                self.__add_node_at(output,op_idx+1)\n",
        "                \n",
        "        # print('==============================',len(outputs))\n",
        "        # self.__print_layers()\n",
        "    def __layers_to_str(self):\n",
        "        str_layers=[]\n",
        "        for layer in self.layers: \n",
        "            layer = [str(l) for l in layer]\n",
        "            str_layers.append('&'.join(layer))\n",
        "        return '卍'.join(str_layers)\n",
        "    def __set_ids(self):\n",
        "        for layer in self.layers:\n",
        "            for node in layer:\n",
        "                node.pre_ids=[]\n",
        "                node.next_ids=[]\n",
        "                for pre in node.pre:\n",
        "                    gnode = self.node_map.get(pre.name,None)\n",
        "                    if not gnode is None:\n",
        "                        node.pre_ids.append(gnode.id)\n",
        "                for next in node.next:\n",
        "                    gnode = self.node_map.get(next.name,None)\n",
        "                    if not gnode is None:\n",
        "                        node.next_ids.append(gnode.id)\n",
        "    def build(self):  \n",
        "        # self.__print_layers()\n",
        "        with open('html/show_graph.html','r',encoding='utf-8') as r:\n",
        "            tmp_html=r.read()\n",
        "        with open('html/NodeObj.js','r',encoding='utf-8') as r:\n",
        "            js1=r.read()\n",
        "        with open('html/ShowGraph.js','r',encoding='utf-8') as r:\n",
        "            js2=r.read()\n",
        "        self.__set_ids()\n",
        "        data=self.__layers_to_str()\n",
        "        data = '<script type=\"text/javascript\">data=\"'+data+'\";</script>'\n",
        "        with open(self.file_dst,'w+',encoding='utf-8') as w:\n",
        "            w.write(data+'\\n'+tmp_html)\n",
        "            w.write( '\\n<script type=\"text/javascript\">'+js1+'</script>')\n",
        "            w.write( '\\n<script type=\"text/javascript\">'+js2+'</script>') "
      ],
      "metadata": {
        "id": "CZnabHKw5Dqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge_layers.py\n",
        "\n",
        "# from NodeObj import OPNode \n",
        "def get_dropout_op_from(ops,start_dropout_op): \n",
        "    op_queue=[start_dropout_op]\n",
        "    do_ops=set() \n",
        "    while len(op_queue)>0:\n",
        "        op = op_queue.pop(0) \n",
        "        for inp in op.inputs:\n",
        "            if (inp.op in ops) and ('dropout' in inp.op.name) and (not inp.op in do_ops):\n",
        "                op_queue.append(inp.op) \n",
        "        for output in op.outputs:\n",
        "            for next_op in output.consumers(): \n",
        "                if (next_op in ops) and ('dropout' in next_op.name) and (not next_op in do_ops): \n",
        "                    op_queue.append(next_op)\n",
        "        do_ops.add(op) \n",
        "    return   do_ops  \n",
        "\n",
        "def merge_dropout_ops(sorted_ops,dropout_ops):\n",
        "    dropout_idx=0\n",
        "    merged_dropout=[]\n",
        "    dp_input_ops=[] \n",
        "    for dropout in dropout_ops:\n",
        "        dropout_input_tensor=set()\n",
        "        dropout_output_tensor=set()\n",
        "        input_op  = set()\n",
        "        output_op= set()\n",
        "        keep_prob=None \n",
        "        for dp in dropout:\n",
        "            if 'keep_prob' in dp.name:\n",
        "                keep_prob=dp\n",
        "            for input in  dp.inputs:\n",
        "                if input.op in sorted_ops and (not input.op in dropout):\n",
        "                    dropout_input_tensor.add(input)\n",
        "                    input_op.add(dp)\n",
        "            for output in dp.outputs:\n",
        "                for op in output.consumers():\n",
        "                    if op in sorted_ops and (not op in dropout):\n",
        "                        dropout_output_tensor.add(output)  \n",
        "                        output_op.add(dp)\n",
        "        name = 'Merged_dropout:'+str(dropout_idx)+'_'+str(keep_prob.outputs[0].eval())\n",
        "        dp_op = OPNode.my_init('Dropout',list(dropout_input_tensor),list(dropout_output_tensor) , name)  \n",
        "        for inp in dropout_input_tensor:\n",
        "            consumer=set()\n",
        "            for c_op in inp.consumers():\n",
        "                if c_op.name in [op.name for op in input_op]:\n",
        "                   consumer.add(dp_op)\n",
        "                else:\n",
        "                    consumer.add(c_op)\n",
        "        for output in dropout_output_tensor:\n",
        "            output.op=dp_op\n",
        "        dropout_idx=dropout_idx+1  \n",
        "        merged_dropout.append(dp_op)\n",
        "        dp_input_ops.append(input_op) \n",
        "    return merged_dropout,dp_input_ops\n",
        "def get_dropout_op_index(dp_op,dp_input_ops):\n",
        "    for idx,dp_set in enumerate(dp_input_ops):\n",
        "        if dp_op in dp_set:\n",
        "            return idx\n",
        "    return -1\n",
        "def merge_dropout(sorted_ops):\n",
        "    visited_op_name=set()\n",
        "    dropout_ops = []\n",
        "    for op in sorted_ops:\n",
        "        if op.name in visited_op_name: \n",
        "            continue\n",
        "        if 'dropout' in op.name:\n",
        "            do_ops = get_dropout_op_from(sorted_ops,op)\n",
        "            for v_op in do_ops:\n",
        "                visited_op_name.add(v_op.name)\n",
        "            dropout_ops.append( do_ops )\n",
        "    merged_dp_ops,dp_input_ops = merge_dropout_ops(sorted_ops,dropout_ops)\n",
        "    new_sorted_ops=[]\n",
        "    visited_dropout_idx=set()\n",
        "    for op in sorted_ops:\n",
        "        if 'dropout' in op.name:\n",
        "            idx = get_dropout_op_index(op, dp_input_ops)\n",
        "            if idx>=0 and (not idx in visited_dropout_idx):\n",
        "                dp_op = merged_dp_ops[idx]\n",
        "                new_sorted_ops.append(dp_op)\n",
        "                visited_dropout_idx.add(idx)\n",
        "        else:\n",
        "            new_sorted_ops.append(op)\n",
        "    return new_sorted_ops\n",
        "def merge_identity_const(sorted_ops):\n",
        "    new_sorted_ops=[]\n",
        "    for op in sorted_ops:\n",
        "        if op.type=='Const':#忽略Const\n",
        "            continue\n",
        "        if op.type=='Identity':#去掉Identity  \n",
        "            op.outputs[0].name=op.inputs[0].name \n",
        "            continue \n",
        "        new_sorted_ops.append(op)\n",
        "    return new_sorted_ops\n",
        "def get_squeeze_end_op(sorted_ops,start_squeeze_op):\n",
        "    ops_in_path=set()\n",
        "    end_op=None \n",
        "    flag=False\n",
        "    for op in sorted_ops:\n",
        "        if op == start_squeeze_op:\n",
        "            flag=True\n",
        "        if flag:\n",
        "            ops_in_path.add(op)\n",
        "            if op.type=='Reshape':\n",
        "                end_op=op\n",
        "                break \n",
        "    visited_ops=set() \n",
        "    if not end_op is None:\n",
        "        ops_in_path.remove(start_squeeze_op)\n",
        "        op_queue=list(ops_in_path)\n",
        "       \n",
        "        while len(op_queue)>0:\n",
        "            op =op_queue.pop(0) \n",
        "            for inp in op.inputs:\n",
        "                if inp.op !=start_squeeze_op and (not inp.op in visited_ops):\n",
        "                    op_queue.append(inp.op)\n",
        "            visited_ops.add(op)\n",
        "    print(len(ops_in_path))\n",
        "    return end_op,visited_ops\n",
        "def merge_squeeze(sorted_ops):\n",
        "    new_sorted_ops=[]\n",
        "    need_remove_ops=set()\n",
        "    for op in sorted_ops:\n",
        "        if op.type=='Squeeze':\n",
        "           end_op,need_removed = get_squeeze_end_op(sorted_ops,op)\n",
        "           if not end_op is None: \n",
        "               ts = end_op.outputs[0]\n",
        "               ts.op=op\n",
        "               op.outputs=[ts]\n",
        "               need_remove_ops|=need_removed\n",
        "        new_sorted_ops.append(op) \n",
        "    new_sorted_ops=[op for op in new_sorted_ops if not op in need_remove_ops]\n",
        "    return new_sorted_ops\n",
        "def merge_layers(sorted_ops):\n",
        "    sorted_ops = merge_dropout(sorted_ops)\n",
        "    sorted_ops = merge_identity_const(sorted_ops)\n",
        "    sorted_ops = merge_squeeze(sorted_ops)\n",
        "    return sorted_ops"
      ],
      "metadata": {
        "id": "vWz8TAE85KfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read_graph.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "# from utils import sort_ops ,_get_ops_in_path\n",
        "# from GraphBuilder import GraphBuilder\n",
        "# from MergeLayers import merge_layers\n",
        "# from NodeObj import OPNode,TSNode\n",
        "def remove_identity_const(sorted_ops):\n",
        "    ops=[]\n",
        "    for op in sorted_ops:\n",
        "        if op.type=='Const':\n",
        "            continue\n",
        "        elif op.type=='Identity':\n",
        "            output = op.outputs[0]\n",
        "            output.identity_from=op.inputs[0]\n",
        "            sorted_ops.remove(op)\n",
        "            # print('--->',output.identity_from)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return sorted_ops\n",
        "def read_graph_from_pb(tf_model_path ,input_names,output_name):  \n",
        "    with open(tf_model_path, 'rb') as f:\n",
        "        serialized = f.read() \n",
        "    tf.reset_default_graph()\n",
        "    gdef = tf.GraphDef()\n",
        "    gdef.ParseFromString(serialized) \n",
        "    with tf.Graph().as_default() as g:\n",
        "        tf.import_graph_def(gdef, name='') \n",
        "    \n",
        "    with tf.Session(graph=g) as sess: \n",
        "        OPS=get_ops_from_pb(g,input_names,output_name)\n",
        "    return OPS\n",
        " \n",
        "def remove_ops_before_inputs(inputs,ops):\n",
        "    tensor_queue=inputs.copy()  \n",
        "    visited_ts=set()\n",
        "    invalid_ops=set()\n",
        "    while len(tensor_queue)>0:\n",
        "        ts = tensor_queue.pop(0) \n",
        "        if not ts.op in invalid_ops: \n",
        "            invalid_ops.add(ts.op)\n",
        "            tensor_queue=tensor_queue+[inp for inp in  ts.op.inputs if not inp in visited_ts]  \n",
        "        visited_ts.add(ts) \n",
        "    ops = [op for op in ops if not op in invalid_ops]\n",
        "    ops = get_connected_ops(ops,inputs) \n",
        "    return ops\n",
        "def get_connected_ops(ops_set,start_tensors):\n",
        "    visited_ts = set()\n",
        "    visited_ops=set()\n",
        "    ts_queue=start_tensors \n",
        "    while len(ts_queue)>0:\n",
        "        ts = ts_queue.pop(0) \n",
        "        if ts.op in ops_set:\n",
        "            visited_ops.add(ts.op)\n",
        "            ts_queue=ts_queue+[input for input in ts.op.inputs if not input in visited_ts]\n",
        "        for op in ts.consumers():\n",
        "            if op in ops_set:\n",
        "                visited_ops.add(op) \n",
        "                ts_queue=ts_queue+[output for output in op.outputs if not output in visited_ts]\n",
        "        visited_ts.add(ts)\n",
        "    ops = [op for op in ops_set if op in visited_ops] \n",
        "    return ops\n",
        " \n",
        "def ops_to_OPNodes(ops,inputs):\n",
        "\n",
        "    ops_map = dict()\n",
        "    ts_set=set()\n",
        "    ts_map=dict()\n",
        "   \n",
        "    for op in ops:\n",
        "        op_node = OPNode(op)\n",
        "        ops_map[op]= op_node\n",
        "        ts_set |=set([ts for ts in op.inputs])\n",
        "        ts_set |=set([ts for ts in op.outputs])\n",
        "    for ts in ts_set:\n",
        "        ts_map[ts]=TSNode(ts,None) \n",
        "    for op,op_node in ops_map.items(): \n",
        "        inps=[]\n",
        "        for inp in op.inputs:#修改节点输入\n",
        "            inp = ts_map[inp]\n",
        "            inps.append(inp)\n",
        "        op_node.inputs=inps\n",
        "        outputs=[]\n",
        "        for output in op.outputs:#修改节点输出\n",
        "            output = ts_map[output]\n",
        "            outputs.append(output)\n",
        "        op_node.outputs=outputs\n",
        "    for ts,ts_node in ts_map.items():\n",
        "        consumers=[] \n",
        "        for op in ts.consumers():\n",
        "            if op in ops:\n",
        "                consumers.append(ops_map[op]) \n",
        "        ts_node.next_ops=consumers \n",
        "        ts_node.op = ops_map.get(ts.op,None)\n",
        "        if ts_node.op==None:\n",
        "            print('---->',ts_node.name)\n",
        "    print(inputs)\n",
        "    #将inputs用placeholder替换\n",
        "    replace_input=dict() \n",
        "    for input in inputs:#将input映射placeholder \n",
        "        # if not input.op.type=='Placeholder':\n",
        "        input_shape = input.get_shape()\n",
        "        if input_shape==None:\n",
        "            input_shape=[None,None,None,None]\n",
        "        ph = tf.placeholder(input.dtype,input_shape)\n",
        "        print(ph.get_shape())\n",
        "        replace_input[input.name] = ph\n",
        "        ph_node = OPNode(ph.op) \n",
        "        ops_map[ph.op]=ph_node\n",
        "   \n",
        "    for op,op_node in ops_map.items():\n",
        "        new_inputs=[] \n",
        "        for input in op_node.inputs:\n",
        "            input = replace_input.get(input.name,input) #placeholder output\n",
        "            new_inputs.append(input) \n",
        "        op_node.inputs=new_inputs \n",
        "        \n",
        "    return ops_map.values() \n",
        "\n",
        "def get_ops_from_inputs_outputs(graph, inputs,outputs):\n",
        "    ops = graph.get_operations() \n",
        "    ops=remove_ops_before_inputs(inputs.copy(),ops)\n",
        "    ops = ops_to_OPNodes(ops,inputs)\n",
        "    return ops\n",
        "def get_ops_from_pb(graph,input_names,output_name,save_ori_network=True):\n",
        "    if save_ori_network:\n",
        "        with open('ori_network.txt','w+') as w: \n",
        "            OPS=graph.get_operations()\n",
        "            for op in OPS:\n",
        "                txt = str([v.name for v in op.inputs])+'---->'+op.type+'--->'+str([v.name for v in op.outputs])\n",
        "                w.write(txt+'\\n') \n",
        "    inputs_tf = [graph.get_tensor_by_name(input_name) for input_name in input_names]\n",
        "    output_tf =graph.get_tensor_by_name(output_name) \n",
        "    OPS =get_ops_from_inputs_outputs(graph, inputs_tf,[output_tf] ) \n",
        "    with open('network.txt','w+') as w: \n",
        "        for op in OPS:\n",
        "            txt = str([v.name for v in op.inputs])+'---->'+op.type+'--->'+str([v.name for v in op.outputs])\n",
        "            w.write(txt+'\\n') \n",
        "    OPS = sort_ops(OPS)\n",
        "    OPS = merge_layers(OPS)\n",
        "    return OPS\n",
        "def read_graph_from_ckpt(ckpt_path,input_names,output_name ):   \n",
        "    saver = tf.train.import_meta_graph(ckpt_path+'.meta',clear_devices=True)\n",
        "    graph = tf.get_default_graph()\n",
        "    with tf.Session( graph=graph) as sess:\n",
        "        sess.run(tf.global_variables_initializer()) \n",
        "        saver.restore(sess,ckpt_path) \n",
        "        output_tf =graph.get_tensor_by_name(output_name) \n",
        "        pb_graph = tf.graph_util.convert_variables_to_constants( sess, graph.as_graph_def(), [output_tf.op.name]) \n",
        "     \n",
        "    with tf.Graph().as_default() as g:\n",
        "        tf.import_graph_def(pb_graph, name='')  \n",
        "    with tf.Session(graph=g) as sess:\n",
        "        OPS=get_ops_from_pb(g,input_names,output_name)\n",
        "    return OPS\n",
        "\n",
        "def gen_graph(ops,html_dst):\n",
        "    gb = GraphBuilder(html_dst) \n",
        "    for op in ops:\n",
        "        if not len(op.outputs)>0:\n",
        "            continue  \n",
        "        if(op.type=='Placeholder'):\n",
        "            continue\n",
        "        gb.add_op(op )\n",
        "    gb.build()\n",
        "def print_graph(ops):\n",
        "    \n",
        "    for op in ops:\n",
        "        output = op.outputs[0] \n",
        "        print(op.inputs,output)\n",
        "def read_graph(model_path,input_names,output_name,html_dst):\n",
        "    dir_path = os.path.dirname(html_dst)\n",
        "    if len(dir_path)>0 and not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "    if model_path.endswith('pb'):\n",
        "        ops = read_graph_from_pb( model_path ,input_names,output_name)\t\n",
        "    else:\n",
        "        ops = read_graph_from_ckpt(model_path ,input_names,output_name)\n",
        "    \n",
        "    print(dir_path)\n",
        "    gen_graph(ops,html_dst)\n",
        "# if __name__=='__main__':\n",
        "#     model_path = sys.argv[1]\n",
        "#     input_names = sys.argv[2]\n",
        "#     output_name = sys.argv[3]\n",
        "#     html_dst = sys.argv[4]\n",
        "#     input_names=input_names.split(',')\n",
        "#     read_graph(model_path,input_names,output_name,html_dst)\n",
        "  \n",
        "# read_graph('../../mobilenet_v1_1.0_192.ckpt',['batch:0'],'MobilenetV1/Predictions/Reshape_1:0','output/html_dst3.html')\n",
        "# read_graph( '../../mobilenet_v1_1.0_192_frozen.pb' ,['input:0'],'MobilenetV1/Predictions/Reshape_1:0','output/html_dst1.html')\n"
      ],
      "metadata": {
        "id": "K7KcRcCJ356o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c737897-aa22-4f9a-8bf2-27bb509356e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Io40SlE65XzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load files"
      ],
      "metadata": {
        "id": "RTxCsowRceoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r 'drive/MyDrive/covid/models/' ."
      ],
      "metadata": {
        "id": "fNkPbQqm5oFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r 'drive/MyDrive/covid/labels/' ."
      ],
      "metadata": {
        "id": "p0FvcwGdcb2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r 'drive/MyDrive/covid/assets/' ."
      ],
      "metadata": {
        "id": "zu-vmP-YczuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# operation"
      ],
      "metadata": {
        "id": "EjkcrAWS5Yqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_names = ['input_1:0']"
      ],
      "metadata": {
        "id": "PMphIzc-cc59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ops = read_graph_from_ckpt(\"models/COVIDNet-CXR-2/model\", input_names, 'norm_dense_2/Softmax:0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePLMhlTj5b3W",
        "outputId": "b11d57c2-9e9d-42d4-cf45-6357d1c1a4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "WARNING:tensorflow:From <ipython-input-9-b105af89481f>:152: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.compat.v1.graph_util.extract_sub_graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----> input_1:0\n",
            "[<tf.Tensor 'input_1:0' shape=(?, 480, 480, 3) dtype=float32>]\n",
            "(?, 480, 480, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ops\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnvM8IaqdiTw",
        "outputId": "c7ca2760-33df-447e-ffba-e2d9863ad76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.OPNode at 0x7f205590f290>,\n",
              " <__main__.OPNode at 0x7f2055c15850>,\n",
              " <__main__.OPNode at 0x7f2055c152d0>,\n",
              " <__main__.OPNode at 0x7f2055c25690>,\n",
              " <__main__.OPNode at 0x7f2055c1f510>,\n",
              " <__main__.OPNode at 0x7f2055c1da50>,\n",
              " <__main__.OPNode at 0x7f2055c2c7d0>,\n",
              " <__main__.OPNode at 0x7f2055c1ded0>,\n",
              " <__main__.OPNode at 0x7f2055c2bf10>,\n",
              " <__main__.OPNode at 0x7f2055c2bc90>,\n",
              " <__main__.OPNode at 0x7f2055c2c750>,\n",
              " <__main__.OPNode at 0x7f2055c19a90>,\n",
              " <__main__.OPNode at 0x7f2055c21550>,\n",
              " <__main__.OPNode at 0x7f2055c25410>,\n",
              " <__main__.OPNode at 0x7f2055c210d0>,\n",
              " <__main__.OPNode at 0x7f2055c1fd10>,\n",
              " <__main__.OPNode at 0x7f2055c2fa10>,\n",
              " <__main__.OPNode at 0x7f2055c19090>,\n",
              " <__main__.OPNode at 0x7f2055c1fd90>,\n",
              " <__main__.OPNode at 0x7f2055c1dad0>,\n",
              " <__main__.OPNode at 0x7f2055c19f10>,\n",
              " <__main__.OPNode at 0x7f2055c2f310>,\n",
              " <__main__.OPNode at 0x7f2055c1d5d0>,\n",
              " <__main__.OPNode at 0x7f2055c32c50>,\n",
              " <__main__.OPNode at 0x7f2055c25390>,\n",
              " <__main__.OPNode at 0x7f2055c218d0>,\n",
              " <__main__.OPNode at 0x7f2055c2cf50>,\n",
              " <__main__.OPNode at 0x7f2055c21850>,\n",
              " <__main__.OPNode at 0x7f2055c2c4d0>,\n",
              " <__main__.OPNode at 0x7f2055c289d0>,\n",
              " <__main__.OPNode at 0x7f2055c17950>,\n",
              " <__main__.OPNode at 0x7f2055c25810>,\n",
              " <__main__.OPNode at 0x7f2055c1d050>,\n",
              " <__main__.OPNode at 0x7f2055c2ca50>,\n",
              " <__main__.OPNode at 0x7f2055c1d750>,\n",
              " <__main__.OPNode at 0x7f2055c21f50>,\n",
              " <__main__.OPNode at 0x7f2055c2f390>,\n",
              " <__main__.OPNode at 0x7f2055c17c50>,\n",
              " <__main__.OPNode at 0x7f2055c281d0>,\n",
              " <__main__.OPNode at 0x7f2055c2c0d0>,\n",
              " <__main__.OPNode at 0x7f2055c28cd0>,\n",
              " <__main__.OPNode at 0x7f2055c286d0>,\n",
              " <__main__.OPNode at 0x7f2055c328d0>,\n",
              " <__main__.OPNode at 0x7f2055c2f590>,\n",
              " <__main__.OPNode at 0x7f2055c1d250>,\n",
              " <__main__.OPNode at 0x7f2055c2b110>,\n",
              " <__main__.OPNode at 0x7f2055c15a10>,\n",
              " <__main__.OPNode at 0x7f2055c25a90>,\n",
              " <__main__.OPNode at 0x7f2055c15a90>,\n",
              " <__main__.OPNode at 0x7f2055c17bd0>,\n",
              " <__main__.OPNode at 0x7f2055c2b410>,\n",
              " <__main__.OPNode at 0x7f2055c2b090>,\n",
              " <__main__.OPNode at 0x7f2055c32d50>,\n",
              " <__main__.OPNode at 0x7f2055c34190>,\n",
              " <__main__.OPNode at 0x7f2055c1ddd0>,\n",
              " <__main__.OPNode at 0x7f2055c21750>,\n",
              " <__main__.OPNode at 0x7f2055c17d50>,\n",
              " <__main__.OPNode at 0x7f2055c19410>,\n",
              " <__main__.OPNode at 0x7f2055c2cad0>,\n",
              " <__main__.OPNode at 0x7f2055c179d0>,\n",
              " <__main__.OPNode at 0x7f2055c25e10>,\n",
              " <__main__.OPNode at 0x7f2055c19e90>,\n",
              " <__main__.OPNode at 0x7f2055c21450>,\n",
              " <__main__.OPNode at 0x7f2055c1f910>,\n",
              " <__main__.OPNode at 0x7f2055c19890>,\n",
              " <__main__.OPNode at 0x7f2055c19110>,\n",
              " <__main__.OPNode at 0x7f2055c32650>,\n",
              " <__main__.OPNode at 0x7f2055c2fa90>,\n",
              " <__main__.OPNode at 0x7f2055c25710>,\n",
              " <__main__.OPNode at 0x7f2055c2fd90>,\n",
              " <__main__.OPNode at 0x7f2055c1d3d0>,\n",
              " <__main__.OPNode at 0x7f2055c321d0>,\n",
              " <__main__.OPNode at 0x7f2055c2b590>,\n",
              " <__main__.OPNode at 0x7f2055c32fd0>,\n",
              " <__main__.OPNode at 0x7f2055c324d0>,\n",
              " <__main__.OPNode at 0x7f2055c15b90>,\n",
              " <__main__.OPNode at 0x7f2055c2cb50>,\n",
              " <__main__.OPNode at 0x7f2055c19f90>,\n",
              " <__main__.OPNode at 0x7f2055c2b210>,\n",
              " <__main__.OPNode at 0x7f2055c2bc10>,\n",
              " <__main__.OPNode at 0x7f2055c1f890>,\n",
              " <__main__.OPNode at 0x7f2055c34410>,\n",
              " <__main__.OPNode at 0x7f2055c2c350>,\n",
              " <__main__.OPNode at 0x7f2055c1fc90>,\n",
              " <__main__.OPNode at 0x7f2055c19710>,\n",
              " <__main__.OPNode at 0x7f2055c2fb10>,\n",
              " <__main__.OPNode at 0x7f2055c1d350>,\n",
              " <__main__.OPNode at 0x7f2055c25090>,\n",
              " <__main__.OPNode at 0x7f2055c2c050>,\n",
              " <__main__.OPNode at 0x7f2055c28550>,\n",
              " <__main__.OPNode at 0x7f2055c25a10>,\n",
              " <__main__.OPNode at 0x7f2055c2f610>,\n",
              " <__main__.OPNode at 0x7f2055c2c450>,\n",
              " <__main__.OPNode at 0x7f2055c178d0>,\n",
              " <__main__.OPNode at 0x7f2055c19b10>,\n",
              " <__main__.OPNode at 0x7f2055c214d0>,\n",
              " <__main__.OPNode at 0x7f2055c34490>,\n",
              " <__main__.OPNode at 0x7f2055c2b190>,\n",
              " <__main__.OPNode at 0x7f2055c2f290>,\n",
              " <__main__.OPNode at 0x7f2055c2b810>,\n",
              " <__main__.OPNode at 0x7f2055c25d90>,\n",
              " <__main__.OPNode at 0x7f2055c2f210>,\n",
              " <__main__.OPNode at 0x7f2055c21b50>,\n",
              " <__main__.OPNode at 0x7f2055c21bd0>,\n",
              " <__main__.OPNode at 0x7f2055c1f610>,\n",
              " <__main__.OPNode at 0x7f2055c21ed0>,\n",
              " <__main__.OPNode at 0x7f2055c1f690>,\n",
              " <__main__.OPNode at 0x7f2055c2cfd0>,\n",
              " <__main__.OPNode at 0x7f2055c2fe90>,\n",
              " <__main__.OPNode at 0x7f2055c25790>,\n",
              " <__main__.OPNode at 0x7f2055c1f990>,\n",
              " <__main__.OPNode at 0x7f2055c2b490>,\n",
              " <__main__.OPNode at 0x7f2055c2b890>,\n",
              " <__main__.OPNode at 0x7f2055c32250>,\n",
              " <__main__.OPNode at 0x7f2055c1d2d0>,\n",
              " <__main__.OPNode at 0x7f2055c1dd50>,\n",
              " <__main__.OPNode at 0x7f2055c21fd0>,\n",
              " <__main__.OPNode at 0x7f2055c329d0>,\n",
              " <__main__.OPNode at 0x7f2055c21050>,\n",
              " <__main__.OPNode at 0x7f2055c2fd10>,\n",
              " <__main__.OPNode at 0x7f2055c32850>,\n",
              " <__main__.OPNode at 0x7f2055c25b10>,\n",
              " <__main__.OPNode at 0x7f2055c2bd10>,\n",
              " <__main__.OPNode at 0x7f2055c21c50>,\n",
              " <__main__.OPNode at 0x7f2055c217d0>,\n",
              " <__main__.OPNode at 0x7f2055c1f290>,\n",
              " <__main__.OPNode at 0x7f2055c19390>,\n",
              " <__main__.OPNode at 0x7f2055c34590>,\n",
              " <__main__.OPNode at 0x7f2055c17350>,\n",
              " <__main__.OPNode at 0x7f2055c28950>,\n",
              " <__main__.OPNode at 0x7f2055c325d0>,\n",
              " <__main__.OPNode at 0x7f2055c28650>,\n",
              " <__main__.OPNode at 0x7f2055c211d0>,\n",
              " <__main__.OPNode at 0x7f2055c2b510>,\n",
              " <__main__.OPNode at 0x7f2055c15390>,\n",
              " <__main__.OPNode at 0x7f2055c15410>,\n",
              " <__main__.OPNode at 0x7f2055c15510>,\n",
              " <__main__.OPNode at 0x7f2055c155d0>,\n",
              " <__main__.OPNode at 0x7f2055c15990>,\n",
              " <__main__.OPNode at 0x7f2055c15750>,\n",
              " <__main__.OPNode at 0x7f2055c15910>,\n",
              " <__main__.OPNode at 0x7f2055c2c3d0>,\n",
              " <__main__.OPNode at 0x7f2055c2c6d0>,\n",
              " <__main__.OPNode at 0x7f2055c2ce50>,\n",
              " <__main__.OPNode at 0x7f2055c25310>,\n",
              " <__main__.OPNode at 0x7f2055c17fd0>,\n",
              " <__main__.OPNode at 0x7f2055c15b10>,\n",
              " <__main__.OPNode at 0x7f2055c15c10>,\n",
              " <__main__.OPNode at 0x7f2055c15c90>,\n",
              " <__main__.OPNode at 0x7f2055c15d10>,\n",
              " <__main__.OPNode at 0x7f2055c15e90>,\n",
              " <__main__.OPNode at 0x7f2055c171d0>,\n",
              " <__main__.OPNode at 0x7f2055c15d90>,\n",
              " <__main__.OPNode at 0x7f2055c2f690>,\n",
              " <__main__.OPNode at 0x7f2055c28250>,\n",
              " <__main__.OPNode at 0x7f2055c2c850>,\n",
              " <__main__.OPNode at 0x7f2055c32550>,\n",
              " <__main__.OPNode at 0x7f2055c19c10>,\n",
              " <__main__.OPNode at 0x7f2055c2bb90>,\n",
              " <__main__.OPNode at 0x7f2055c34510>,\n",
              " <__main__.OPNode at 0x7f2055c17cd0>,\n",
              " <__main__.OPNode at 0x7f2055c172d0>,\n",
              " <__main__.OPNode at 0x7f2055c2ced0>,\n",
              " <__main__.OPNode at 0x7f2055c28dd0>,\n",
              " <__main__.OPNode at 0x7f2055c25b90>,\n",
              " <__main__.OPNode at 0x7f2055c15e10>,\n",
              " <__main__.OPNode at 0x7f2055c1f190>,\n",
              " <__main__.OPNode at 0x7f2055c25e90>,\n",
              " <__main__.OPNode at 0x7f2055c2f990>,\n",
              " <__main__.OPNode at 0x7f2055c32150>,\n",
              " <__main__.OPNode at 0x7f2055c28e50>,\n",
              " <__main__.OPNode at 0x7f2055c28d50>,\n",
              " <__main__.OPNode at 0x7f2055c1d9d0>,\n",
              " <__main__.OPNode at 0x7f2055c34110>,\n",
              " <__main__.OPNode at 0x7f2055c173d0>,\n",
              " <__main__.OPNode at 0x7f2055c1fa10>,\n",
              " <__main__.OPNode at 0x7f2055c17f50>,\n",
              " <__main__.OPNode at 0x7f2055c28350>,\n",
              " <__main__.OPNode at 0x7f2055c2cbd0>,\n",
              " <__main__.OPNode at 0x7f2055c282d0>,\n",
              " <__main__.OPNode at 0x7f2055c21cd0>,\n",
              " <__main__.OPNode at 0x7f2055c19b90>,\n",
              " <__main__.OPNode at 0x7f2055c25290>,\n",
              " <__main__.OPNode at 0x7f2055c34090>,\n",
              " <__main__.OPNode at 0x7f2055c19790>,\n",
              " <__main__.OPNode at 0x7f2055c2fe10>,\n",
              " <__main__.OPNode at 0x7f2055c17250>,\n",
              " <__main__.OPNode at 0x7f2055c17450>,\n",
              " <__main__.OPNode at 0x7f2055c17050>,\n",
              " <__main__.OPNode at 0x7f2055c17150>,\n",
              " <__main__.OPNode at 0x7f2055c174d0>,\n",
              " <__main__.OPNode at 0x7f2055c17550>,\n",
              " <__main__.OPNode at 0x7f2055c17650>,\n",
              " <__main__.OPNode at 0x7f2055c348d0>,\n",
              " <__main__.OPNode at 0x7f2055c34910>,\n",
              " <__main__.OPNode at 0x7f2055c34a90>,\n",
              " <__main__.OPNode at 0x7f2055c320d0>,\n",
              " <__main__.OPNode at 0x7f2055c28a50>,\n",
              " <__main__.OPNode at 0x7f2055c34990>,\n",
              " <__main__.OPNode at 0x7f2055c34a10>,\n",
              " <__main__.OPNode at 0x7f2055c17850>,\n",
              " <__main__.OPNode at 0x7f2055c34b10>,\n",
              " <__main__.OPNode at 0x7f2055c34b90>,\n",
              " <__main__.OPNode at 0x7f2055c34c10>,\n",
              " <__main__.OPNode at 0x7f2055c34c90>,\n",
              " <__main__.OPNode at 0x7f2055c34d10>,\n",
              " <__main__.OPNode at 0x7f2055c34d90>,\n",
              " <__main__.OPNode at 0x7f2055c34f10>,\n",
              " <__main__.OPNode at 0x7f2055c34f90>,\n",
              " <__main__.OPNode at 0x7f2055c34e10>,\n",
              " <__main__.OPNode at 0x7f2055c34e90>,\n",
              " <__main__.OPNode at 0x7f2055c37050>,\n",
              " <__main__.OPNode at 0x7f2055c370d0>,\n",
              " <__main__.OPNode at 0x7f2055c2b910>,\n",
              " <__main__.OPNode at 0x7f2055c1de50>,\n",
              " <__main__.OPNode at 0x7f2055c19490>,\n",
              " <__main__.OPNode at 0x7f2055c19810>,\n",
              " <__main__.OPNode at 0x7f2055c2f710>,\n",
              " <__main__.OPNode at 0x7f2055c32bd0>,\n",
              " <__main__.OPNode at 0x7f2055c1f210>,\n",
              " <__main__.OPNode at 0x7f2055c1d650>,\n",
              " <__main__.OPNode at 0x7f2055c37150>,\n",
              " <__main__.OPNode at 0x7f2055c372d0>,\n",
              " <__main__.OPNode at 0x7f2055c37350>,\n",
              " <__main__.OPNode at 0x7f2055c371d0>,\n",
              " <__main__.OPNode at 0x7f2055c37250>,\n",
              " <__main__.OPNode at 0x7f2055c373d0>,\n",
              " <__main__.OPNode at 0x7f2055c37450>,\n",
              " <__main__.OPNode at 0x7f2055c374d0>,\n",
              " <__main__.OPNode at 0x7f2055c37550>,\n",
              " <__main__.OPNode at 0x7f2055c37590>,\n",
              " <__main__.OPNode at 0x7f2055c37710>,\n",
              " <__main__.OPNode at 0x7f2055c37610>,\n",
              " <__main__.OPNode at 0x7f2055c37690>,\n",
              " <__main__.OPNode at 0x7f2055c1f590>,\n",
              " <__main__.OPNode at 0x7f2055c19510>,\n",
              " <__main__.OPNode at 0x7f2055c37790>,\n",
              " <__main__.OPNode at 0x7f2055c37810>,\n",
              " <__main__.OPNode at 0x7f2055c37890>,\n",
              " <__main__.OPNode at 0x7f2055c37910>,\n",
              " <__main__.OPNode at 0x7f2055c37990>,\n",
              " <__main__.OPNode at 0x7f2055c37a10>,\n",
              " <__main__.OPNode at 0x7f2055c37d90>,\n",
              " <__main__.OPNode at 0x7f2055c37b90>,\n",
              " <__main__.OPNode at 0x7f2055c37c10>,\n",
              " <__main__.OPNode at 0x7f2055c37a90>,\n",
              " <__main__.OPNode at 0x7f2055c37b10>,\n",
              " <__main__.OPNode at 0x7f2055c37c90>,\n",
              " <__main__.OPNode at 0x7f2055c37d10>,\n",
              " <__main__.OPNode at 0x7f2055c37e10>,\n",
              " <__main__.OPNode at 0x7f2055c37f90>,\n",
              " <__main__.OPNode at 0x7f2055c3b050>,\n",
              " <__main__.OPNode at 0x7f2055c37e90>,\n",
              " <__main__.OPNode at 0x7f2055c37f10>,\n",
              " <__main__.OPNode at 0x7f2055c3b0d0>,\n",
              " <__main__.OPNode at 0x7f2055c3b150>,\n",
              " <__main__.OPNode at 0x7f2055c3b1d0>,\n",
              " <__main__.OPNode at 0x7f2055c3b250>,\n",
              " <__main__.OPNode at 0x7f2055c3b290>,\n",
              " <__main__.OPNode at 0x7f2055c3b410>,\n",
              " <__main__.OPNode at 0x7f2055c3b490>,\n",
              " <__main__.OPNode at 0x7f2055c3b310>,\n",
              " <__main__.OPNode at 0x7f2055c3b390>,\n",
              " <__main__.OPNode at 0x7f2055c3b510>,\n",
              " <__main__.OPNode at 0x7f2055c3b590>,\n",
              " <__main__.OPNode at 0x7f2055c3b610>,\n",
              " <__main__.OPNode at 0x7f2055c3b690>,\n",
              " <__main__.OPNode at 0x7f2055c3b710>,\n",
              " <__main__.OPNode at 0x7f2055c3b890>,\n",
              " <__main__.OPNode at 0x7f2055c3b910>,\n",
              " <__main__.OPNode at 0x7f2055c3b790>,\n",
              " <__main__.OPNode at 0x7f2055c3b810>,\n",
              " <__main__.OPNode at 0x7f2055c3b990>,\n",
              " <__main__.OPNode at 0x7f2055c3ba10>,\n",
              " <__main__.OPNode at 0x7f2055c3bb90>,\n",
              " <__main__.OPNode at 0x7f2055c3bd10>,\n",
              " <__main__.OPNode at 0x7f2055c3bc10>,\n",
              " <__main__.OPNode at 0x7f2055c3bc90>,\n",
              " <__main__.OPNode at 0x7f2055c3ba90>,\n",
              " <__main__.OPNode at 0x7f2055c3bb10>,\n",
              " <__main__.OPNode at 0x7f2055c2b990>,\n",
              " <__main__.OPNode at 0x7f2055c285d0>,\n",
              " <__main__.OPNode at 0x7f2055c25f10>,\n",
              " <__main__.OPNode at 0x7f2055c32950>,\n",
              " <__main__.OPNode at 0x7f2055c32cd0>,\n",
              " <__main__.OPNode at 0x7f2055c21150>,\n",
              " <__main__.OPNode at 0x7f2055c1f110>,\n",
              " <__main__.OPNode at 0x7f2055c1d6d0>,\n",
              " <__main__.OPNode at 0x7f2055c3bd90>,\n",
              " <__main__.OPNode at 0x7f2055c3be10>,\n",
              " <__main__.OPNode at 0x7f2055c3be90>,\n",
              " <__main__.OPNode at 0x7f2055c3bf10>,\n",
              " <__main__.OPNode at 0x7f2055c3bf90>,\n",
              " <__main__.OPNode at 0x7f2055c3bfd0>,\n",
              " <__main__.OPNode at 0x7f2055c3d190>,\n",
              " <__main__.OPNode at 0x7f2055c3d090>,\n",
              " <__main__.OPNode at 0x7f2055c3d110>,\n",
              " <__main__.OPNode at 0x7f2055c2bf90>,\n",
              " <__main__.OPNode at 0x7f2055c1fc10>,\n",
              " <__main__.OPNode at 0x7f2055c1db50>,\n",
              " <__main__.OPNode at 0x7f2055c3d210>,\n",
              " <__main__.OPNode at 0x7f2055c3d290>,\n",
              " <__main__.OPNode at 0x7f2055c3d310>,\n",
              " <__main__.OPNode at 0x7f2055c3d390>,\n",
              " <__main__.OPNode at 0x7f2055c3d410>,\n",
              " <__main__.OPNode at 0x7f2055c3d490>,\n",
              " <__main__.OPNode at 0x7f2055c3d610>,\n",
              " <__main__.OPNode at 0x7f2055c3d690>,\n",
              " <__main__.OPNode at 0x7f2055c3d510>,\n",
              " <__main__.OPNode at 0x7f2055c3d590>,\n",
              " <__main__.OPNode at 0x7f2055c3d710>,\n",
              " <__main__.OPNode at 0x7f2055c3d790>,\n",
              " <__main__.OPNode at 0x7f2055c3d810>,\n",
              " <__main__.OPNode at 0x7f2055c3d990>,\n",
              " <__main__.OPNode at 0x7f2055c3da10>,\n",
              " <__main__.OPNode at 0x7f2055c3d890>,\n",
              " <__main__.OPNode at 0x7f2055c3d910>,\n",
              " <__main__.OPNode at 0x7f2055c3da90>,\n",
              " <__main__.OPNode at 0x7f2055c3db10>,\n",
              " <__main__.OPNode at 0x7f2055c3db90>,\n",
              " <__main__.OPNode at 0x7f2055c3dc10>,\n",
              " <__main__.OPNode at 0x7f2055c3dc50>,\n",
              " <__main__.OPNode at 0x7f2055c3ddd0>,\n",
              " <__main__.OPNode at 0x7f2055c3de50>,\n",
              " <__main__.OPNode at 0x7f2055c3dcd0>,\n",
              " <__main__.OPNode at 0x7f2055c3dd50>,\n",
              " <__main__.OPNode at 0x7f2055c3ded0>,\n",
              " <__main__.OPNode at 0x7f2055c3df50>,\n",
              " <__main__.OPNode at 0x7f2055c3dfd0>,\n",
              " <__main__.OPNode at 0x7f2055c41090>,\n",
              " <__main__.OPNode at 0x7f2055c41110>,\n",
              " <__main__.OPNode at 0x7f2055c41290>,\n",
              " <__main__.OPNode at 0x7f2055c41310>,\n",
              " <__main__.OPNode at 0x7f2055c41190>,\n",
              " <__main__.OPNode at 0x7f2055c41210>,\n",
              " <__main__.OPNode at 0x7f2055c41390>,\n",
              " <__main__.OPNode at 0x7f2055c41410>,\n",
              " <__main__.OPNode at 0x7f2055c41490>,\n",
              " <__main__.OPNode at 0x7f2055c41610>,\n",
              " <__main__.OPNode at 0x7f2055c41690>,\n",
              " <__main__.OPNode at 0x7f2055c41510>,\n",
              " <__main__.OPNode at 0x7f2055c41590>,\n",
              " <__main__.OPNode at 0x7f2055c41710>,\n",
              " <__main__.OPNode at 0x7f2055c41790>,\n",
              " <__main__.OPNode at 0x7f2055c41810>,\n",
              " <__main__.OPNode at 0x7f2055c41890>,\n",
              " <__main__.OPNode at 0x7f2055c418d0>,\n",
              " <__main__.OPNode at 0x7f2055c41a50>,\n",
              " <__main__.OPNode at 0x7f2055c41ad0>,\n",
              " <__main__.OPNode at 0x7f2055c41950>,\n",
              " <__main__.OPNode at 0x7f2055c419d0>,\n",
              " <__main__.OPNode at 0x7f2055c41b50>,\n",
              " <__main__.OPNode at 0x7f2055c41bd0>,\n",
              " <__main__.OPNode at 0x7f2055c41c50>,\n",
              " <__main__.OPNode at 0x7f2055c41cd0>,\n",
              " <__main__.OPNode at 0x7f2055c41d50>,\n",
              " <__main__.OPNode at 0x7f2055c43110>,\n",
              " <__main__.OPNode at 0x7f2055c41ed0>,\n",
              " <__main__.OPNode at 0x7f2055c41dd0>,\n",
              " <__main__.OPNode at 0x7f2055c41e50>,\n",
              " <__main__.OPNode at 0x7f2055c213d0>,\n",
              " <__main__.OPNode at 0x7f2055c41f50>,\n",
              " <__main__.OPNode at 0x7f2055c41fd0>,\n",
              " <__main__.OPNode at 0x7f2055c43090>,\n",
              " <__main__.OPNode at 0x7f2055c43190>,\n",
              " <__main__.OPNode at 0x7f2055c43310>,\n",
              " <__main__.OPNode at 0x7f2055c43390>,\n",
              " <__main__.OPNode at 0x7f2055c43210>,\n",
              " <__main__.OPNode at 0x7f2055c43290>,\n",
              " <__main__.OPNode at 0x7f2055c43410>,\n",
              " <__main__.OPNode at 0x7f2055c43490>,\n",
              " <__main__.OPNode at 0x7f2055c43510>,\n",
              " <__main__.OPNode at 0x7f2055c43590>,\n",
              " <__main__.OPNode at 0x7f2055c435d0>,\n",
              " <__main__.OPNode at 0x7f2055c43750>,\n",
              " <__main__.OPNode at 0x7f2055c437d0>,\n",
              " <__main__.OPNode at 0x7f2055c43650>,\n",
              " <__main__.OPNode at 0x7f2055c436d0>,\n",
              " <__main__.OPNode at 0x7f2055c43850>,\n",
              " <__main__.OPNode at 0x7f2055c438d0>,\n",
              " <__main__.OPNode at 0x7f2055c43950>,\n",
              " <__main__.OPNode at 0x7f2055c439d0>,\n",
              " <__main__.OPNode at 0x7f2055c43a50>,\n",
              " <__main__.OPNode at 0x7f2055c43bd0>,\n",
              " <__main__.OPNode at 0x7f2055c43c50>,\n",
              " <__main__.OPNode at 0x7f2055c43ad0>,\n",
              " <__main__.OPNode at 0x7f2055c43b50>,\n",
              " <__main__.OPNode at 0x7f2055c43cd0>,\n",
              " <__main__.OPNode at 0x7f2055c43d50>,\n",
              " <__main__.OPNode at 0x7f2055c43ed0>,\n",
              " <__main__.OPNode at 0x7f2055c45090>,\n",
              " <__main__.OPNode at 0x7f2055c45110>,\n",
              " <__main__.OPNode at 0x7f2055c43f50>,\n",
              " <__main__.OPNode at 0x7f2055c43fd0>,\n",
              " <__main__.OPNode at 0x7f2055c45190>,\n",
              " <__main__.OPNode at 0x7f2055c45210>,\n",
              " <__main__.OPNode at 0x7f2055c45290>,\n",
              " <__main__.OPNode at 0x7f2055c45310>,\n",
              " <__main__.OPNode at 0x7f2055c45350>,\n",
              " <__main__.OPNode at 0x7f2055c454d0>,\n",
              " <__main__.OPNode at 0x7f2055c45550>,\n",
              " <__main__.OPNode at 0x7f2055c453d0>,\n",
              " <__main__.OPNode at 0x7f2055c45450>,\n",
              " <__main__.OPNode at 0x7f2055c455d0>,\n",
              " <__main__.OPNode at 0x7f2055c45650>,\n",
              " <__main__.OPNode at 0x7f2055c456d0>,\n",
              " <__main__.OPNode at 0x7f2055c45750>,\n",
              " <__main__.OPNode at 0x7f2055c43dd0>,\n",
              " <__main__.OPNode at 0x7f2055c43e50>,\n",
              " <__main__.OPNode at 0x7f2055c457d0>,\n",
              " <__main__.OPNode at 0x7f2055c45950>,\n",
              " <__main__.OPNode at 0x7f2055c459d0>,\n",
              " <__main__.OPNode at 0x7f2055c45850>,\n",
              " <__main__.OPNode at 0x7f2055c458d0>,\n",
              " <__main__.OPNode at 0x7f2055c45a50>,\n",
              " <__main__.OPNode at 0x7f2055c45ad0>,\n",
              " <__main__.OPNode at 0x7f2055c45b50>,\n",
              " <__main__.OPNode at 0x7f2055c45cd0>,\n",
              " <__main__.OPNode at 0x7f2055c45d50>,\n",
              " <__main__.OPNode at 0x7f2055c45bd0>,\n",
              " <__main__.OPNode at 0x7f2055c45c50>,\n",
              " <__main__.OPNode at 0x7f2055c45dd0>,\n",
              " <__main__.OPNode at 0x7f2055c45e50>,\n",
              " <__main__.OPNode at 0x7f2055c45ed0>,\n",
              " <__main__.OPNode at 0x7f2055c45f50>,\n",
              " <__main__.OPNode at 0x7f2055c45f90>,\n",
              " <__main__.OPNode at 0x7f2055bc9150>,\n",
              " <__main__.OPNode at 0x7f2055bc91d0>,\n",
              " <__main__.OPNode at 0x7f2055bc9050>,\n",
              " <__main__.OPNode at 0x7f2055bc90d0>,\n",
              " <__main__.OPNode at 0x7f2055bc9250>,\n",
              " <__main__.OPNode at 0x7f2055bc92d0>,\n",
              " <__main__.OPNode at 0x7f2055bc9350>,\n",
              " <__main__.OPNode at 0x7f2055bc93d0>,\n",
              " <__main__.OPNode at 0x7f2055bc9450>,\n",
              " <__main__.OPNode at 0x7f2055bc95d0>,\n",
              " <__main__.OPNode at 0x7f2055bc9650>,\n",
              " <__main__.OPNode at 0x7f2055bc94d0>,\n",
              " <__main__.OPNode at 0x7f2055bc9550>,\n",
              " <__main__.OPNode at 0x7f2055bc96d0>,\n",
              " <__main__.OPNode at 0x7f2055bc9750>,\n",
              " <__main__.OPNode at 0x7f2055bc97d0>,\n",
              " <__main__.OPNode at 0x7f2055bc9950>,\n",
              " <__main__.OPNode at 0x7f2055bc9850>,\n",
              " <__main__.OPNode at 0x7f2055bc98d0>,\n",
              " <__main__.OPNode at 0x7f2055c288d0>,\n",
              " <__main__.OPNode at 0x7f2055bc99d0>,\n",
              " <__main__.OPNode at 0x7f2055bc9a50>,\n",
              " <__main__.OPNode at 0x7f2055bc9ad0>,\n",
              " <__main__.OPNode at 0x7f2055bc9b50>,\n",
              " <__main__.OPNode at 0x7f2055bc9bd0>,\n",
              " <__main__.OPNode at 0x7f2055bc9c10>,\n",
              " <__main__.OPNode at 0x7f2055bc9d90>,\n",
              " <__main__.OPNode at 0x7f2055bc9e10>,\n",
              " <__main__.OPNode at 0x7f2055bc9c90>,\n",
              " <__main__.OPNode at 0x7f2055bc9d10>,\n",
              " <__main__.OPNode at 0x7f2055bc9e90>,\n",
              " <__main__.OPNode at 0x7f2055bc9f10>,\n",
              " <__main__.OPNode at 0x7f2055bc9f90>,\n",
              " <__main__.OPNode at 0x7f2055bcb050>,\n",
              " <__main__.OPNode at 0x7f2055bcb0d0>,\n",
              " <__main__.OPNode at 0x7f2055bcb250>,\n",
              " <__main__.OPNode at 0x7f2055bcb2d0>,\n",
              " <__main__.OPNode at 0x7f2055bcb150>,\n",
              " <__main__.OPNode at 0x7f2055bcb1d0>,\n",
              " <__main__.OPNode at 0x7f2055bcb350>,\n",
              " <__main__.OPNode at 0x7f2055bcb3d0>,\n",
              " <__main__.OPNode at 0x7f2055bcb450>,\n",
              " <__main__.OPNode at 0x7f2055bcb5d0>,\n",
              " <__main__.OPNode at 0x7f2055bcb650>,\n",
              " <__main__.OPNode at 0x7f2055bcb4d0>,\n",
              " <__main__.OPNode at 0x7f2055bcb550>,\n",
              " <__main__.OPNode at 0x7f2055bcb6d0>,\n",
              " <__main__.OPNode at 0x7f2055bcb750>,\n",
              " <__main__.OPNode at 0x7f2055bcb7d0>,\n",
              " <__main__.OPNode at 0x7f2055bcb850>,\n",
              " <__main__.OPNode at 0x7f2055bcb890>,\n",
              " <__main__.OPNode at 0x7f2055bcba10>,\n",
              " <__main__.OPNode at 0x7f2055bcba90>,\n",
              " <__main__.OPNode at 0x7f2055bcb910>,\n",
              " <__main__.OPNode at 0x7f2055bcb990>,\n",
              " <__main__.OPNode at 0x7f2055bcbb10>,\n",
              " <__main__.OPNode at 0x7f2055bcbb90>,\n",
              " <__main__.OPNode at 0x7f2055bcbc10>,\n",
              " <__main__.OPNode at 0x7f2055bcbc90>,\n",
              " <__main__.OPNode at 0x7f2055bcbd10>,\n",
              " <__main__.OPNode at 0x7f2055bcbe90>,\n",
              " <__main__.OPNode at 0x7f2055bcbf10>,\n",
              " <__main__.OPNode at 0x7f2055bcbd90>,\n",
              " <__main__.OPNode at 0x7f2055bcbe10>,\n",
              " <__main__.OPNode at 0x7f2055bcbf90>,\n",
              " <__main__.OPNode at 0x7f2055bcd050>,\n",
              " <__main__.OPNode at 0x7f2055bcd0d0>,\n",
              " <__main__.OPNode at 0x7f2055bcd250>,\n",
              " <__main__.OPNode at 0x7f2055bcd2d0>,\n",
              " <__main__.OPNode at 0x7f2055bcd150>,\n",
              " <__main__.OPNode at 0x7f2055bcd1d0>,\n",
              " <__main__.OPNode at 0x7f2055bcd350>,\n",
              " <__main__.OPNode at 0x7f2055bcd3d0>,\n",
              " <__main__.OPNode at 0x7f2055bcd450>,\n",
              " <__main__.OPNode at 0x7f2055bcd4d0>,\n",
              " <__main__.OPNode at 0x7f2055bcd510>,\n",
              " <__main__.OPNode at 0x7f2055bcd690>,\n",
              " <__main__.OPNode at 0x7f2055bcd710>,\n",
              " <__main__.OPNode at 0x7f2055bcd590>,\n",
              " <__main__.OPNode at 0x7f2055bcd610>,\n",
              " <__main__.OPNode at 0x7f2055bcd790>,\n",
              " <__main__.OPNode at 0x7f2055bcd810>,\n",
              " <__main__.OPNode at 0x7f2055bcd890>,\n",
              " <__main__.OPNode at 0x7f2055bcd910>,\n",
              " <__main__.OPNode at 0x7f2055bcd990>,\n",
              " <__main__.OPNode at 0x7f2055bcdd10>,\n",
              " <__main__.OPNode at 0x7f2055bcdb10>,\n",
              " <__main__.OPNode at 0x7f2055bcdb90>,\n",
              " <__main__.OPNode at 0x7f2055bcda10>,\n",
              " <__main__.OPNode at 0x7f2055bcda90>,\n",
              " <__main__.OPNode at 0x7f2055bcdc10>,\n",
              " <__main__.OPNode at 0x7f2055bcdc90>,\n",
              " <__main__.OPNode at 0x7f2055bcdd90>,\n",
              " <__main__.OPNode at 0x7f2055bcdf10>,\n",
              " <__main__.OPNode at 0x7f2055bcdf90>,\n",
              " <__main__.OPNode at 0x7f2055bcde10>,\n",
              " <__main__.OPNode at 0x7f2055bcde90>,\n",
              " <__main__.OPNode at 0x7f2055bd1050>,\n",
              " <__main__.OPNode at 0x7f2055bd10d0>,\n",
              " <__main__.OPNode at 0x7f2055bd1150>,\n",
              " <__main__.OPNode at 0x7f2055bd11d0>,\n",
              " <__main__.OPNode at 0x7f2055bd1210>,\n",
              " <__main__.OPNode at 0x7f2055bd1390>,\n",
              " <__main__.OPNode at 0x7f2055bd1410>,\n",
              " <__main__.OPNode at 0x7f2055bd1290>,\n",
              " <__main__.OPNode at 0x7f2055bd1310>,\n",
              " <__main__.OPNode at 0x7f2055bd1490>,\n",
              " <__main__.OPNode at 0x7f2055bd1510>,\n",
              " <__main__.OPNode at 0x7f2055bd1590>,\n",
              " <__main__.OPNode at 0x7f2055bd1610>,\n",
              " <__main__.OPNode at 0x7f2055bd1690>,\n",
              " <__main__.OPNode at 0x7f2055bd1810>,\n",
              " <__main__.OPNode at 0x7f2055bd1890>,\n",
              " <__main__.OPNode at 0x7f2055bd1710>,\n",
              " <__main__.OPNode at 0x7f2055bd1790>,\n",
              " <__main__.OPNode at 0x7f2055bd1910>,\n",
              " <__main__.OPNode at 0x7f2055bd1990>,\n",
              " <__main__.OPNode at 0x7f2055bd1b10>,\n",
              " <__main__.OPNode at 0x7f2055bd1c90>,\n",
              " <__main__.OPNode at 0x7f2055bd1d10>,\n",
              " <__main__.OPNode at 0x7f2055bd1b90>,\n",
              " <__main__.OPNode at 0x7f2055bd1c10>,\n",
              " <__main__.OPNode at 0x7f2055bd1d90>,\n",
              " <__main__.OPNode at 0x7f2055bd1e10>,\n",
              " <__main__.OPNode at 0x7f2055bd1e90>,\n",
              " <__main__.OPNode at 0x7f2055bd1f10>,\n",
              " <__main__.OPNode at 0x7f2055bd1f50>,\n",
              " <__main__.OPNode at 0x7f2055bd3110>,\n",
              " <__main__.OPNode at 0x7f2055bd3190>,\n",
              " <__main__.OPNode at 0x7f2055bd1fd0>,\n",
              " <__main__.OPNode at 0x7f2055bd3090>,\n",
              " <__main__.OPNode at 0x7f2055bd3210>,\n",
              " <__main__.OPNode at 0x7f2055bd3290>,\n",
              " <__main__.OPNode at 0x7f2055bd3310>,\n",
              " <__main__.OPNode at 0x7f2055bd3390>,\n",
              " <__main__.OPNode at 0x7f2055bd1a10>,\n",
              " <__main__.OPNode at 0x7f2055bd1a90>,\n",
              " <__main__.OPNode at 0x7f2055bd3410>,\n",
              " <__main__.OPNode at 0x7f2055bd3590>,\n",
              " <__main__.OPNode at 0x7f2055bd3610>,\n",
              " <__main__.OPNode at 0x7f2055bd3490>,\n",
              " <__main__.OPNode at 0x7f2055bd3510>,\n",
              " <__main__.OPNode at 0x7f2055bd3690>,\n",
              " <__main__.OPNode at 0x7f2055bd3710>,\n",
              " <__main__.OPNode at 0x7f2055bd3790>,\n",
              " <__main__.OPNode at 0x7f2055bd3910>,\n",
              " <__main__.OPNode at 0x7f2055bd3990>,\n",
              " <__main__.OPNode at 0x7f2055bd3810>,\n",
              " <__main__.OPNode at 0x7f2055bd3890>,\n",
              " <__main__.OPNode at 0x7f2055bd3a10>,\n",
              " <__main__.OPNode at 0x7f2055bd3a90>,\n",
              " <__main__.OPNode at 0x7f2055bd3b10>,\n",
              " <__main__.OPNode at 0x7f2055bd3b90>,\n",
              " <__main__.OPNode at 0x7f2055bd3bd0>,\n",
              " <__main__.OPNode at 0x7f2055bd3d50>,\n",
              " <__main__.OPNode at 0x7f2055bd3dd0>,\n",
              " <__main__.OPNode at 0x7f2055bd3c50>,\n",
              " <__main__.OPNode at 0x7f2055bd3cd0>,\n",
              " <__main__.OPNode at 0x7f2055bd3e50>,\n",
              " <__main__.OPNode at 0x7f2055bd3ed0>,\n",
              " <__main__.OPNode at 0x7f2055bd3f50>,\n",
              " <__main__.OPNode at 0x7f2055bd3fd0>,\n",
              " <__main__.OPNode at 0x7f2055bd6090>,\n",
              " <__main__.OPNode at 0x7f2055bd6210>,\n",
              " <__main__.OPNode at 0x7f2055bd6290>,\n",
              " <__main__.OPNode at 0x7f2055bd6110>,\n",
              " <__main__.OPNode at 0x7f2055bd6190>,\n",
              " <__main__.OPNode at 0x7f2055bd6310>,\n",
              " <__main__.OPNode at 0x7f2055bd6390>,\n",
              " <__main__.OPNode at 0x7f2055bd6410>,\n",
              " <__main__.OPNode at 0x7f2055bd6590>,\n",
              " <__main__.OPNode at 0x7f2055bd6610>,\n",
              " <__main__.OPNode at 0x7f2055bd6490>,\n",
              " <__main__.OPNode at 0x7f2055bd6510>,\n",
              " <__main__.OPNode at 0x7f2055bd6690>,\n",
              " <__main__.OPNode at 0x7f2055bd6710>,\n",
              " <__main__.OPNode at 0x7f2055bd6790>,\n",
              " <__main__.OPNode at 0x7f2055bd6810>,\n",
              " <__main__.OPNode at 0x7f2055bd6850>,\n",
              " <__main__.OPNode at 0x7f2055bd69d0>,\n",
              " <__main__.OPNode at 0x7f2055bd6a50>,\n",
              " <__main__.OPNode at 0x7f2055bd68d0>,\n",
              " <__main__.OPNode at 0x7f2055bd6950>,\n",
              " <__main__.OPNode at 0x7f2055bd6ad0>,\n",
              " <__main__.OPNode at 0x7f2055bd6b50>,\n",
              " <__main__.OPNode at 0x7f2055bd6bd0>,\n",
              " <__main__.OPNode at 0x7f2055bd6c50>,\n",
              " <__main__.OPNode at 0x7f2055bd6cd0>,\n",
              " <__main__.OPNode at 0x7f2055bd6e50>,\n",
              " <__main__.OPNode at 0x7f2055bd6ed0>,\n",
              " <__main__.OPNode at 0x7f2055bd6d50>,\n",
              " <__main__.OPNode at 0x7f2055bd6dd0>,\n",
              " <__main__.OPNode at 0x7f2055bd6f50>,\n",
              " <__main__.OPNode at 0x7f2055bd6fd0>,\n",
              " <__main__.OPNode at 0x7f2055bd90d0>,\n",
              " <__main__.OPNode at 0x7f2055bd9190>,\n",
              " <__main__.OPNode at 0x7f2055bd9210>,\n",
              " <__main__.OPNode at 0x7f2055bd92d0>,\n",
              " <__main__.OPNode at 0x7f2055bd9390>,\n",
              " <__main__.OPNode at 0x7f2055bd9410>,\n",
              " <__main__.OPNode at 0x7f2055bd9490>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -> pytorch"
      ],
      "metadata": {
        "id": "f5U6xKShGC5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWWGzsYzG9Y3",
        "outputId": "96c41624-c12e-4de4-f2e5-aef23edfe5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 45.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### albert model"
      ],
      "metadata": {
        "id": "1K93MIJeZuwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import torch\n",
        "from transformers import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, albert_config_file, pytorch_dump_path):\n",
        "    # 初始化 PyTorch 模型\n",
        "    config = AlbertConfig.from_json_file(albert_config_file)\n",
        "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
        "    model = AlbertForPreTraining(config)\n",
        "\n",
        "    # 从 tf 的 checkpoint 文件中加载模型权重\n",
        "    load_tf_weights_in_albert(model, config, tf_checkpoint_path)\n",
        "\n",
        "    # 保存 PyTorch 模型\n",
        "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
        "    torch.save(model.state_dict(), pytorch_dump_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gXxTXIAjGGpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "\n",
        "# https://github.com/delldu/Albert\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "# 参数设置\n",
        "parser.add_argument(\"--tf_checkpoint_path\", default='./resources/albert_base_zh/model.ckpt-best', type=str,\n",
        "                    required=False, help=\"Path to the TensorFlow checkpoint path.\")\n",
        "parser.add_argument(\"--albert_config_file\", default='./resources/albert_base_zh/albert_config.json', type=str,\n",
        "                    required=False,\n",
        "                    help=\"The config json file corresponding to the pre-trained ALBERT model. \\n\"\"This specifies the model architecture.\", )\n",
        "parser.add_argument(\"--pytorch_dump_path\", default='./resources/albert_base_zh/pytorch_model.bin', type=str, required=False,\n",
        "                    help=\"Path to the output PyTorch model.\")\n",
        "\n",
        "args = parser.parse_args(args = [])\n",
        "args.tf_checkpoint_path = \"models/COVIDNet-CXR-2/model\"\n",
        "args.albert_config_file = \"albert_config.json\"\n",
        "args.pytorch_dump_path = \"pytorch_model.bin\"\n",
        "\n",
        "\n",
        "convert_tf_checkpoint_to_pytorch(args.tf_checkpoint_path, args.albert_config_file, args.pytorch_dump_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "wdbu1PicG8KM",
        "outputId": "c6a51820-f3f4-4669-9917-4122a857f000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c2e6feac0abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mconvert_tf_checkpoint_to_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malbert_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch_dump_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-6d505404c7af>\u001b[0m in \u001b[0;36mconvert_tf_checkpoint_to_pytorch\u001b[0;34m(tf_checkpoint_path, albert_config_file, pytorch_dump_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_tf_checkpoint_to_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malbert_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_dump_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 初始化 PyTorch 模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malbert_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building PyTorch model from configuration: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertForPreTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \"\"\"\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_dict_from_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'albert_config.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get graph"
      ],
      "metadata": {
        "id": "kl7nkyJUJLi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print( tf.__version__ )"
      ],
      "metadata": {
        "id": "CwOcYqqfJ67K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' //////////////////////////////////////////////////// '''\n",
        "# definitions\n",
        "\n",
        "# data.py\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def crop_top(img, percent=0.15):\n",
        "    offset = int(img.shape[0] * percent)\n",
        "    return img[offset:]\n",
        "\n",
        "def central_crop(img):\n",
        "    size = min(img.shape[0], img.shape[1])\n",
        "    offset_h = int((img.shape[0] - size) / 2)\n",
        "    offset_w = int((img.shape[1] - size) / 2)\n",
        "    return img[offset_h:offset_h + size, offset_w:offset_w + size]\n",
        "\n",
        "def process_image_file(filepath, size, top_percent=0.08, crop=True):\n",
        "    img = cv2.imread(filepath)\n",
        "    # print(\"filepath\", filepath)\n",
        "    img = crop_top(img, percent=top_percent)\n",
        "    if crop:\n",
        "        img = central_crop(img)\n",
        "    img = cv2.resize(img, (size, size))\n",
        "    return img\n",
        "\n",
        "def process_image_file_medusa(filepath, size):\n",
        "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, (size, size))\n",
        "    img = img.astype('float64')\n",
        "    img -= img.mean()\n",
        "    img /= img.std()\n",
        "    return np.expand_dims(img, -1)\n",
        "\n",
        "def random_ratio_resize(img, prob=0.3, delta=0.1):\n",
        "    if np.random.rand() >= prob:\n",
        "        return img\n",
        "    ratio = img.shape[0] / img.shape[1]\n",
        "    ratio = np.random.uniform(max(ratio - delta, 0.01), ratio + delta)\n",
        "\n",
        "    if ratio * img.shape[1] <= img.shape[1]:\n",
        "        size = (int(img.shape[1] * ratio), img.shape[1])\n",
        "    else:\n",
        "        size = (img.shape[0], int(img.shape[0] / ratio))\n",
        "\n",
        "    dh = img.shape[0] - size[1]\n",
        "    top, bot = dh // 2, dh - dh // 2\n",
        "    dw = img.shape[1] - size[0]\n",
        "    left, right = dw // 2, dw - dw // 2\n",
        "\n",
        "    if size[0] > 480 or size[1] > 480:\n",
        "        print(img.shape, size, ratio)\n",
        "\n",
        "    img = cv2.resize(img, size)\n",
        "    img = cv2.copyMakeBorder(img, top, bot, left, right, cv2.BORDER_CONSTANT,\n",
        "                             (0, 0, 0))\n",
        "\n",
        "    if img.shape[0] != 480 or img.shape[1] != 480:\n",
        "        raise ValueError(img.shape, size)\n",
        "    return img\n",
        "\n",
        "_augmentation_transform = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=(0.9, 1.1),\n",
        "    zoom_range=(0.85, 1.15),\n",
        "    fill_mode='constant',\n",
        "    cval=0.,\n",
        ")\n",
        "\n",
        "def apply_augmentation(img):\n",
        "    img = random_ratio_resize(img)\n",
        "    img = _augmentation_transform.random_transform(img)\n",
        "    return img\n",
        "\n",
        "def _process_csv_file(file):\n",
        "    with open(file, 'r') as fr:\n",
        "        files = fr.readlines()\n",
        "    return files\n",
        "\n",
        "\n",
        "class BalanceCovidDataset(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            data_dir,\n",
        "            csv_file,\n",
        "            is_training=True,\n",
        "            batch_size=8,\n",
        "            medusa_input_shape=(256, 256),\n",
        "            input_shape=(480, 480),\n",
        "            n_classes=2,\n",
        "            num_channels=3,\n",
        "            mapping={\n",
        "                'negative': 0,\n",
        "                'positive': 1,\n",
        "            },\n",
        "            shuffle=True,\n",
        "            augmentation=apply_augmentation,\n",
        "            covid_percent=0.5,\n",
        "            class_weights=[1., 1.],\n",
        "            top_percent=0.08,\n",
        "            is_severity_model=False,\n",
        "            is_medusa_backbone=False,\n",
        "    ):\n",
        "        'Initialization'\n",
        "        self.datadir = data_dir\n",
        "        self.dataset = _process_csv_file(csv_file)\n",
        "        self.is_training = is_training\n",
        "        self.batch_size = batch_size\n",
        "        self.N = len(self.dataset)\n",
        "        self.medusa_input_shape = medusa_input_shape\n",
        "        self.input_shape = input_shape\n",
        "        self.n_classes = n_classes\n",
        "        self.num_channels = num_channels\n",
        "        self.mapping = mapping\n",
        "        self.shuffle = shuffle\n",
        "        self.covid_percent = covid_percent\n",
        "        self.class_weights = class_weights\n",
        "        self.n = 0\n",
        "        self.augmentation = augmentation\n",
        "        self.top_percent = top_percent\n",
        "        self.is_severity_model = is_severity_model\n",
        "        self.is_medusa_backbone = is_medusa_backbone\n",
        "\n",
        "        # If using MEDUSA backbone load images without crop\n",
        "        if self.is_medusa_backbone:\n",
        "            self.load_image = partial(process_image_file, top_percent=0, crop=False)\n",
        "        else:\n",
        "            self.load_image = process_image_file\n",
        "\n",
        "        datasets = {}\n",
        "        for key in self.mapping.keys():\n",
        "            datasets[key] = []\n",
        "\n",
        "        for l in self.dataset:\n",
        "            datasets[l.split()[2]].append(l)\n",
        "        \n",
        "        if self.is_severity_model:\n",
        "            self.datasets = [\n",
        "                datasets['level2'], datasets['level1']\n",
        "            ]\n",
        "        elif self.n_classes == 2:\n",
        "            self.datasets = [\n",
        "                datasets['negative'], datasets['positive']\n",
        "            ]\n",
        "        elif self.n_classes == 3:\n",
        "            self.datasets = [\n",
        "                datasets['normal'] + datasets['pneumonia'],\n",
        "                datasets['COVID-19'],\n",
        "            ]\n",
        "        else:\n",
        "            raise Exception('Only binary or 3 class classification currently supported.')\n",
        "        print(len(self.datasets[0]), len(self.datasets[1]))\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __next__(self):\n",
        "        # Get one batch of data\n",
        "        model_inputs = self.__getitem__(self.n)\n",
        "        # Batch index\n",
        "        self.n += 1\n",
        "\n",
        "        # If we have processed the entire dataset then\n",
        "        if self.n >= self.__len__():\n",
        "            self.on_epoch_end()\n",
        "            self.n = 0\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.datasets[0]) / float(self.batch_size)))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        if self.shuffle == True:\n",
        "            for v in self.datasets:\n",
        "                np.random.shuffle(v)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = np.zeros((self.batch_size, *self.input_shape, self.num_channels))\n",
        "        batch_y = np.zeros(self.batch_size)\n",
        "\n",
        "        if self.is_medusa_backbone:\n",
        "            batch_sem_x = np.zeros((self.batch_size, *self.medusa_input_shape, 1))\n",
        "\n",
        "        batch_files = self.datasets[0][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        # upsample covid cases\n",
        "        covid_size = max(int(len(batch_files) * self.covid_percent), 1)\n",
        "        covid_inds = np.random.choice(np.arange(len(batch_files)),\n",
        "                                      size=covid_size,\n",
        "                                      replace=False)\n",
        "        covid_files = np.random.choice(self.datasets[1],\n",
        "                                       size=covid_size,\n",
        "                                       replace=False)\n",
        "        for i in range(covid_size):\n",
        "            batch_files[covid_inds[i]] = covid_files[i]\n",
        "\n",
        "        for i in range(len(batch_files)):\n",
        "            sample = batch_files[i].split()\n",
        "\n",
        "            if self.is_training:\n",
        "                folder = 'train'\n",
        "            else:\n",
        "                folder = 'test'\n",
        "\n",
        "            image_file = os.path.join(self.datadir, folder, sample[1])\n",
        "            x = self.load_image(\n",
        "                image_file,\n",
        "                self.input_shape[0],\n",
        "                top_percent=self.top_percent,\n",
        "            )\n",
        "\n",
        "            if self.is_training and hasattr(self, 'augmentation'):\n",
        "                x = self.augmentation(x)\n",
        "\n",
        "            x = x.astype('float32') / 255.0\n",
        "\n",
        "            if self.is_medusa_backbone:\n",
        "                sem_x = process_image_file_medusa(image_file, self.medusa_input_shape[0])\n",
        "                batch_sem_x[i] = sem_x\n",
        "            \n",
        "            y = self.mapping[sample[2]]\n",
        "\n",
        "            batch_x[i] = x\n",
        "            batch_y[i] = y\n",
        "\n",
        "        class_weights = self.class_weights\n",
        "        weights = np.take(class_weights, batch_y.astype('int64'))\n",
        "        batch_y = keras.utils.to_categorical(batch_y, num_classes=self.n_classes)\n",
        "\n",
        "        if self.is_medusa_backbone:\n",
        "            return batch_sem_x, batch_x, batch_y, weights, self.is_training\n",
        "        else:\n",
        "            return batch_x, batch_y, weights, self.is_training\n",
        "\n",
        "''' \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ '''        \n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os, argparse\n",
        "\n",
        "# To remove TF Warnings\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "    # args = []\n",
        "'''\n",
        "python inference.py \\\n",
        "--weightspath models/COVIDNet-CXR-2 \\\n",
        "--metaname model.meta \\\n",
        "--ckptname model \\\n",
        "--n_classes 2 \\\n",
        "--imagepath assets/ex-covid.jpeg \\\n",
        "--in_tensorname input_1:0 \\\n",
        "--out_tensorname norm_dense_2/Softmax:0\n",
        "\n",
        "python inference.py \\\n",
        "--weightspath models/COVIDNet-CXR-3 \\\n",
        "--metaname model.meta \\\n",
        "--ckptname model \\\n",
        "--n_classes 2 \\\n",
        "--imagepath assets/ex-covid.jpeg \\\n",
        "--out_tensorname softmax/Softmax:0 \\\n",
        "--is_medusa_backbone\n",
        "\n",
        "'''\n",
        "\n",
        "# USE COVIDNet CXR 2\n",
        "\n",
        "args_weightspath = 'models/COVIDNet-CXR-2' \n",
        "args_metaname = 'model.meta'\n",
        "args_ckptname = 'model'\n",
        "args_n_classes = 2\n",
        "\n",
        "args_testfolder = 'data/test'\n",
        "args_trainfile = 'labels/train_COVIDx9B.txt'\n",
        "args_testfile = 'labels/test_COVIDx9B.txt'\n",
        "\n",
        "args_out_tensorname = 'norm_dense_2/Softmax:0'\n",
        "args_logit_tensorname = 'norm_dense_2/MatMul:0'\n",
        "args_is_severity_model = False\n",
        "args_is_medusa_backbone = False\n",
        "\n",
        "args_in_tensorname = 'input_1:0'\n",
        "args_in_tensorname_medusa = 'input_1:0'\n",
        "args_input_size = 480\n",
        "args_input_size_medusa = 256\n",
        "args_top_percent = 0.08\n",
        "\n",
        "'''\n",
        "<<<<<<<<<<<<<<<<<<Here to change the test image!!! >>>>>>>>>>>>>>>>>>>>\n",
        "'''\n",
        "\n",
        "args_imagepath = 'assets/ex-covid.jpeg'\n",
        "# args_imagepath = \"data/test/0a8d486f-1aa6-4fcf-b7be-4bf04fc8628b.png\"\n",
        "# args_imagepath = \"drive/MyDrive/covid/ricord_images/MIDRC-RICORD-1C-SITE2-000293-40361-0.png\"\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "# tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "\n",
        "\n",
        "if args_is_severity_model:\n",
        "    # For COVIDNet CXR-S training with COVIDxSev level 1 and level 2 air space seveirty grading\n",
        "    mapping = {'level2': 0, 'level1': 1}\n",
        "    inv_mapping = {0: 'level2', 1: 'level1'}\n",
        "elif args_n_classes == 2:\n",
        "    # For COVID-19 positive/negative detection\n",
        "    mapping = {'negative': 0, 'positive': 1}\n",
        "    inv_mapping = {0: 'negative', 1: 'positive'}\n",
        "elif args_n_classes == 3:\n",
        "    # For detection of no pneumonia/non-COVID-19 pneumonia/COVID-19 pneumonia\n",
        "    mapping = {'normal': 0, 'pneumonia': 1, 'COVID-19': 2}\n",
        "    inv_mapping = {0: 'normal', 1: 'pneumonia', 2: 'COVID-19'}\n",
        "else:\n",
        "    raise Exception('''COVID-Net currently only supports 2 class COVID-19 positive/negative detection\n",
        "        or 3 class detection of no pneumonia/non-COVID-19 pneumonia/COVID-19 pneumonia''')\n",
        "\n",
        "print(\"mapping\", mapping)\n",
        "mapping_keys = list(mapping.keys())\n",
        "\n",
        "# tf.get_default_graph()\n",
        "# tf.compat.v1.get_default_graph()\n",
        "# saver = tf.compat.v1.train.import_meta_graph(os.path.join(args_weightspath, args_metaname))\n",
        "# saver.restore(sess, os.path.join(args_weightspath, args_ckptname))\n",
        "\n",
        "# graph = tf.get_default_graph()\n",
        "\n",
        "# image_tensor = graph.get_tensor_by_name(args_in_tensorname)\n",
        "# pred_tensor = graph.get_tensor_by_name(args_out_tensorname)\n",
        "\n",
        "sess = tf.Session()\n",
        "tf.get_default_graph()\n",
        "saver = tf.train.import_meta_graph(os.path.join(args_weightspath, args_metaname))\n",
        "saver.restore(sess, os.path.join(args_weightspath, args_ckptname))\n",
        "\n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "# image_tensor = graph.get_tensor_by_name(args_in_tensorname)\n",
        "# pred_tensor = graph.get_tensor_by_name(args_out_tensorname)\n",
        "\n",
        "# if args_is_medusa_backbone:\n",
        "#     x = process_image_file(args_imagepath, args_input_size, top_percent=0, crop=False)\n",
        "#     x = x.astype('float32') / 255.0\n",
        "#     medusa_image_tensor = graph.get_tensor_by_name(args_in_tensorname_medusa)\n",
        "#     medusa_x = process_image_file_medusa(args_imagepath, args_input_size_medusa)\n",
        "#     feed_dict = {\n",
        "#                 medusa_image_tensor: np.expand_dims(medusa_x, axis=0),\n",
        "#                 image_tensor: np.expand_dims(x, axis=0),\n",
        "#             } \n",
        "# else:\n",
        "#     x = process_image_file(args_imagepath, args_input_size, top_percent=args_top_percent)\n",
        "#     x = x.astype('float32') / 255.0\n",
        "#     feed_dict = {image_tensor: np.expand_dims(x, axis=0)}\n",
        "\n",
        "# print(\"feed_dict\", feed_dict)\n",
        "\n",
        "# pred = sess.run(pred_tensor, feed_dict=feed_dict)\n",
        "\n",
        "# # print(\"pred\", pred)\n",
        "\n",
        "# # print('Prediction: {}'.format(inv_mapping[pred.argmax(axis=1)[0]]))\n",
        "# # print('Confidence')\n",
        "# # print(' '.join('{}: {:.3f}'.format(cls.capitalize(), pred[0][i]) for cls, i in mapping.items()))\n",
        "# # print('**DISCLAIMER**')\n",
        "# # print('Do not use this prediction for self-diagnosis. You should check with your local authorities for the latest advice on seeking medical assistance.')\n",
        "\n"
      ],
      "metadata": {
        "id": "YcsxMFs-JKh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph)"
      ],
      "metadata": {
        "id": "uhaOZ8HqMIZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "VxvS-7DsJPOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prnet_full.py"
      ],
      "metadata": {
        "id": "oh9pdkBbLhfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prnet_full.py  - https://github.com/liguohao96/pytorch-prnet/blob/7475fa1c593977a9989fd80bec7ead686f075811/prnet_full.py#L123\n",
        "from torch import nn\n",
        "\n",
        "def padding_same_conv2d(input_size, in_c, out_c, kernel_size=4, stride=1):\n",
        "    output_size = input_size // stride\n",
        "    padding_num = stride * (output_size - 1) - input_size + kernel_size\n",
        "    if padding_num % 2 == 0:\n",
        "        return nn.Sequential(nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=stride, padding=padding_num // 2, bias=False))\n",
        "    else:\n",
        "        return nn.Sequential(\n",
        "            nn.ConstantPad2d((padding_num // 2, padding_num // 2 + 1, padding_num // 2, padding_num // 2 + 1), 0),\n",
        "            nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=stride, padding=0, bias=False)\n",
        "        )\n",
        "\n",
        "# import threading\n",
        "class resBlock(nn.Module):\n",
        "    instance_num = 0\n",
        "    # instance_num_lock = threading.Lock()\n",
        "    def __init__(self, in_c, out_c, kernel_size=4, stride=1, input_size=None):\n",
        "        super().__init__()\n",
        "        self.instance_idx = self.__class__.instance_num\n",
        "        self.__class__.instance_num += 1\n",
        "        assert kernel_size == 4\n",
        "        self.shortcut = lambda x: x\n",
        "        self.tf_map = {}\n",
        "        if in_c != out_c:\n",
        "            self.shortcut = nn.Conv2d(in_c, out_c, kernel_size=1, stride=stride, bias=False)\n",
        "            self.tf_map['{}/shortcut/weights'.format(self.instance_name())] = 'shortcut.weight'\n",
        "\n",
        "        main_layers = [\n",
        "            nn.Conv2d(in_c, out_c // 2, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_c // 2, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        self.tf_map['{}/Conv/weights'.format(self.instance_name())] = 'main.0.weight'\n",
        "        self.tf_map['{}/Conv/BatchNorm/gamma'.format(self.instance_name())] = 'main.1.weight'\n",
        "        self.tf_map['{}/Conv/BatchNorm/beta'.format(self.instance_name())] = 'main.1.bias'\n",
        "        self.tf_map['{}/Conv/BatchNorm/moving_mean'.format(self.instance_name())] = 'main.1.running_mean'\n",
        "        self.tf_map['{}/Conv/BatchNorm/moving_variance'.format(self.instance_name())] = 'main.1.running_var'\n",
        "\n",
        "        main_layers.extend([\n",
        "            *padding_same_conv2d(input_size, out_c // 2, out_c // 2, kernel_size=kernel_size, stride=stride),\n",
        "            nn.BatchNorm2d(out_c // 2, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True)])\n",
        "        conv_idx = len(main_layers) - 3\n",
        "        self.tf_map['{}/Conv_1/weights'.format(self.instance_name())] = 'main.{}.weight'.format(conv_idx)\n",
        "        self.tf_map['{}/Conv_1/BatchNorm/gamma'.format(self.instance_name())] = 'main.{}.weight'.format(conv_idx+1)\n",
        "        self.tf_map['{}/Conv_1/BatchNorm/beta'.format(self.instance_name())] = 'main.{}.bias'.format(conv_idx+1)\n",
        "        self.tf_map['{}/Conv_1/BatchNorm/moving_mean'.format(self.instance_name())] = 'main.{}.running_mean'.format(conv_idx+1)\n",
        "        self.tf_map['{}/Conv_1/BatchNorm/moving_variance'.format(self.instance_name())] = 'main.{}.running_var'.format(conv_idx+1)\n",
        "\n",
        "        main_layers.extend(\n",
        "            padding_same_conv2d(input_size, out_c // 2, out_c, kernel_size=1, stride=1)\n",
        "        )\n",
        "        conv_idx = len(main_layers) - 1\n",
        "        self.tf_map['{}/Conv_2/weights'.format(self.instance_name())] = 'main.{}.weight'.format(conv_idx)\n",
        "        self.main = nn.Sequential(*main_layers)\n",
        "        self.activate = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_c, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.tf_map['{}/BatchNorm/gamma'.format(self.instance_name())] = 'activate.0.weight'\n",
        "        self.tf_map['{}/BatchNorm/beta'.format(self.instance_name())]  = 'activate.0.bias'\n",
        "        self.tf_map['{}/BatchNorm/moving_mean'.format(self.instance_name())]      = 'activate.0.running_mean'\n",
        "        self.tf_map['{}/BatchNorm/moving_variance'.format(self.instance_name())]  = 'activate.0.running_var'\n",
        "\n",
        "    def instance_name(self):\n",
        "        return 'resBlock' if self.instance_idx == 0 else 'resBlock_{}'.format(self.instance_idx)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut_x = self.shortcut(x)\n",
        "        main_x = self.main(x)\n",
        "        x = self.activate(shortcut_x + main_x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class upBlock(nn.Module):\n",
        "    convtranspose_num = 0\n",
        "    def __init__(self, in_c, out_c, conv_num=2):\n",
        "        super().__init__()\n",
        "        self.tf_map = {}\n",
        "        additional_conv = []\n",
        "        layer_length = 4\n",
        "\n",
        "        self.convtrans_idx = self.__class__.convtranspose_num\n",
        "        self.__class__.convtranspose_num += 1\n",
        "        self.tf_map['{}/weights'.format(self.convtranspose_name())] = 'main.0.weight'\n",
        "        self.tf_map['{}/BatchNorm/gamma'.format(self.convtranspose_name())] = 'main.1.weight'\n",
        "        self.tf_map['{}/BatchNorm/beta'.format(self.convtranspose_name())] = 'main.1.bias'\n",
        "        self.tf_map['{}/BatchNorm/moving_mean'.format(self.convtranspose_name())] = 'main.1.running_mean'\n",
        "        self.tf_map['{}/BatchNorm/moving_variance'.format(self.convtranspose_name())] = 'main.1.running_var'\n",
        "        for i in range(1, conv_num+1):\n",
        "            self.convtrans_idx = self.__class__.convtranspose_num\n",
        "            self.__class__.convtranspose_num += 1\n",
        "            additional_conv += [\n",
        "                nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "                nn.ConvTranspose2d(out_c, out_c, kernel_size=4, stride=1, padding=3, bias=False),\n",
        "                nn.BatchNorm2d(out_c, eps=0.001, momentum=0.001),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            self.tf_map['{}/weights'.format(self.convtranspose_name())] = 'main.{}.weight'.format(i*layer_length + 0)\n",
        "            self.tf_map['{}/BatchNorm/gamma'.format(self.convtranspose_name())] = 'main.{}.weight'.format(i*layer_length + 1)\n",
        "            self.tf_map['{}/BatchNorm/beta'.format(self.convtranspose_name())] = 'main.{}.bias'.format(i*layer_length + 1)\n",
        "            self.tf_map['{}/BatchNorm/moving_mean'.format(self.convtranspose_name())] = 'main.{}.running_mean'.format(i*layer_length + 1)\n",
        "            self.tf_map['{}/BatchNorm/moving_variance'.format(self.convtranspose_name())] = 'main.{}.running_var'.format(i*layer_length + 1)\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # nn.ConstantPad2d((0, 1, 0, 1), 0),\n",
        "            nn.ConvTranspose2d(in_c, out_c, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_c, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True),\n",
        "            *additional_conv\n",
        "            )\n",
        "\n",
        "    def convtranspose_name(self):\n",
        "        return 'Conv2d_transpose' if self.convtrans_idx == 0 else 'Conv2d_transpose_{}'.format(self.convtrans_idx)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.main(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PRNet(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel=3):\n",
        "        super().__init__()\n",
        "        size = 16\n",
        "        self.input_conv = nn.Sequential( #*[\n",
        "            *padding_same_conv2d(256, in_channel, size, kernel_size=4, stride=1),  # 256x256x16\n",
        "            nn.BatchNorm2d(size, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True)\n",
        "            # ]\n",
        "        ) \n",
        "        self.tf_map = {}\n",
        "        conv_idx = len(self.input_conv) - 3\n",
        "        self.tf_map['resfcn256/Conv/weights'] = 'input_conv.{}.weight'.format(conv_idx)\n",
        "        self.tf_map['resfcn256/Conv/BatchNorm/gamma'] = 'input_conv.{}.weight'.format(conv_idx + 1)\n",
        "        self.tf_map['resfcn256/Conv/BatchNorm/beta'] = 'input_conv.{}.bias'.format(conv_idx + 1)\n",
        "        self.tf_map['resfcn256/Conv/BatchNorm/moving_mean'] = 'input_conv.{}.running_mean'.format(conv_idx + 1)\n",
        "        self.tf_map['resfcn256/Conv/BatchNorm/moving_variance'] = 'input_conv.{}.running_var'.format(conv_idx + 1)\n",
        "        self.down_conv_1 = resBlock(size, size * 2, kernel_size=4, stride=2, input_size=256)  # 128x128x32\n",
        "        self.down_conv_2 = resBlock(size * 2, size * 2, kernel_size=4, stride=1, input_size=128)  # 128x128x32\n",
        "        self.down_conv_3 = resBlock(size * 2, size * 4, kernel_size=4, stride=2, input_size=128)  # 64x64x64\n",
        "        self.down_conv_4 = resBlock(size * 4, size * 4, kernel_size=4, stride=1, input_size=64)  # 64x64x64\n",
        "        self.down_conv_5 = resBlock(size * 4, size * 8, kernel_size=4, stride=2, input_size=64)  # 32x32x128\n",
        "        self.down_conv_6 = resBlock(size * 8, size * 8, kernel_size=4, stride=1, input_size=32)  # 32x32x128\n",
        "        self.down_conv_7 = resBlock(size * 8, size * 16, kernel_size=4, stride=2, input_size=32)  # 16x16x256\n",
        "        self.down_conv_8 = resBlock(size * 16, size * 16, kernel_size=4, stride=1, input_size=16)  # 16x16x256\n",
        "        self.down_conv_9 = resBlock(size * 16, size * 32, kernel_size=4, stride=2, input_size=16)  # 8x8x512\n",
        "        self.down_conv_10 = resBlock(size * 32, size * 32, kernel_size=4, stride=1, input_size=8)  # 8x8x512\n",
        "\n",
        "        self.center_conv = nn.Sequential(\n",
        "            nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "            nn.ConvTranspose2d(size * 32, size * 32, kernel_size=4, stride=1, padding=3, bias=False),  # 8x8x512\n",
        "            nn.BatchNorm2d(size * 32, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/weights'] = 'center_conv.1.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/BatchNorm/gamma'] = 'center_conv.2.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/BatchNorm/beta'] = 'center_conv.2.bias'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/BatchNorm/moving_mean'] = 'center_conv.2.running_mean'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose/BatchNorm/moving_variance'] = 'center_conv.2.running_var'\n",
        "        upBlock.convtranspose_num = 1\n",
        "\n",
        "        self.up_conv_5 = upBlock(size * 32, size * 16)  # 16x16x256\n",
        "        self.up_conv_4 = upBlock(size * 16, size * 8)  # 32x32x128\n",
        "        self.up_conv_3 = upBlock(size * 8, size * 4)  # 64x64x64\n",
        "\n",
        "        self.up_conv_2 = upBlock(size * 4, size * 2, 1)  # 128x128x32\n",
        "        self.up_conv_1 = upBlock(size * 2, size, 1)  # 256x256x16\n",
        "\n",
        "        convtranspose_idx = upBlock.convtranspose_num\n",
        "        self.output_conv = nn.Sequential(\n",
        "            nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "            nn.ConvTranspose2d(size, 3, kernel_size=4, stride=1, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(3, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "            nn.ConvTranspose2d(3, 3, kernel_size=4, stride=1, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(3, eps=0.001, momentum=0.001),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConstantPad2d((2, 1, 2, 1), 0),\n",
        "            nn.ConvTranspose2d(3, 3, kernel_size=4, stride=1, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(3, eps=0.001, momentum=0.001),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/weights'.format(convtranspose_idx)] = 'output_conv.1.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/gamma'.format(convtranspose_idx)] = 'output_conv.2.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/beta'.format(convtranspose_idx)] = 'output_conv.2.bias'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_mean'.format(convtranspose_idx)] = 'output_conv.2.running_mean'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_variance'.format(convtranspose_idx)] = 'output_conv.2.running_var'\n",
        "\n",
        "        convtranspose_idx += 1\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/weights'.format(convtranspose_idx)] = 'output_conv.5.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/gamma'.format(convtranspose_idx)] = 'output_conv.6.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/beta'.format(convtranspose_idx)] = 'output_conv.6.bias'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_mean'.format(convtranspose_idx)] = 'output_conv.6.running_mean'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_variance'.format(convtranspose_idx)] = 'output_conv.6.running_var'\n",
        "\n",
        "        convtranspose_idx += 1\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/weights'.format(convtranspose_idx)] = 'output_conv.9.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/gamma'.format(convtranspose_idx)] = 'output_conv.10.weight'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/beta'.format(convtranspose_idx)] = 'output_conv.10.bias'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_mean'.format(convtranspose_idx)] = 'output_conv.10.running_mean'\n",
        "        self.tf_map['resfcn256/Conv2d_transpose_{}/BatchNorm/moving_variance'.format(convtranspose_idx)] = 'output_conv.10.running_var'\n",
        "        self.collact_names()\n",
        "    \n",
        "    def collact_names(self):\n",
        "        for name, child in self.named_children():\n",
        "            if hasattr(child, 'tf_map'):\n",
        "                child_map = getattr(child, 'tf_map')\n",
        "                for k, v in child_map.items():\n",
        "                    self.tf_map['resfcn256/{}'.format(k)] = '{}.{}'.format(name, v)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_conv(x)\n",
        "        x = self.down_conv_1(x)\n",
        "        x = self.down_conv_2(x)\n",
        "        x = self.down_conv_3(x)\n",
        "        x = self.down_conv_4(x)\n",
        "        x = self.down_conv_5(x)\n",
        "        x = self.down_conv_6(x)\n",
        "        x = self.down_conv_7(x)\n",
        "        x = self.down_conv_8(x)\n",
        "        x = self.down_conv_9(x)\n",
        "        x = self.down_conv_10(x)\n",
        "\n",
        "        x = self.center_conv(x)\n",
        "\n",
        "        x = self.up_conv_5(x)\n",
        "        x = self.up_conv_4(x)\n",
        "        x = self.up_conv_3(x)\n",
        "        x = self.up_conv_2(x)\n",
        "        x = self.up_conv_1(x)\n",
        "        x = self.output_conv(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "twi4fXMULSRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tf2torch "
      ],
      "metadata": {
        "id": "L_Bc33spLlc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf2torch - https://github.com/liguohao96/pytorch-prnet/blob/master/tf2torch.py\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "torch_model = PRNet(3, 3)\n",
        "torch_dict = OrderedDict()\n",
        "\n",
        "for node in graph.as_graph_def().node:\n",
        "    # print(node)\n",
        "    if node.name in torch_model.tf_map:\n",
        "        torch_name = torch_model.tf_map[node.name]\n",
        "        data = graph.get_operation_by_name(node.name).outputs[0]\n",
        "        data_np = sess.run(data)\n",
        "        if len(data_np.shape) > 1:\n",
        "            # weight layouts  |   tensorflow   |     pytorch     |  transpose   |\n",
        "            # conv2d_transpose (H, W, out, in) -> (in, out, H, W)  (3, 2, 0, 1)\n",
        "            # conv2d           (H, W, in, out) -> (out, in, H, W)  (3, 2, 0, 1)\n",
        "            torch_dict[torch_name] = torch.tensor(np.transpose(data_np, (3, 2, 0, 1)).astype(np.float32))\n",
        "        else:\n",
        "            torch_dict[torch_name] = torch.tensor(data_np.astype(np.float32))\n",
        "    else:\n",
        "        if node.name.find('save') == -1:\n",
        "            pass\n",
        "            print('not in {}'.format(node.name))\n",
        "torch.save(torch_dict, 'from_tf.pth')\n"
      ],
      "metadata": {
        "id": "R0iLtjAhHvN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch_dict)"
      ],
      "metadata": {
        "id": "Di-M4dEqOJIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cam -tf"
      ],
      "metadata": {
        "id": "TI7CzoXQaAWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model\n"
      ],
      "metadata": {
        "id": "n8DlofxknEuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_r50 = ResNet50(weights='imagenet', include_top=False)\n",
        "# model_r50.summary()\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define a simple sequential model\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    keras.layers.Dense(512, activation=tf.keras.activations.relu, input_shape=(784,)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(10, activation=tf.keras.activations.softmax)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# Create a basic model instance\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TuKs_2n6aCqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver = tf.train.import_meta_graph('models/COVIDNet-CXR-2/model.meta')\n",
        "    saver.restore(sess, \"models/COVIDNet-CXR-2/model\")\n"
      ],
      "metadata": {
        "id": "UQfbaisAbvxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "saver = tf.train.import_meta_graph('models/COVIDNet-CXR-2/model.meta')\n",
        "sess = tf.Session()\n",
        "saver.restore(sess, \"models/COVIDNet-CXR-2/model\")\n",
        "# result = sess.run(\"v4:0\", feed_dict={\"v1:0\": 12.0, \"v2:0\": 3.3})\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "mdfggY2niXwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "checkpoint_path = \"models/COVIDNet-CXR-2/model.data-00000-of-00001.ckpt\"\n",
        "test_images = \"assets/ex-covid.jpeg\"\n",
        "test_labels = \"labels\"\n",
        "model.load_weights(checkpoint_path)\n",
        "loss,acc = model.evaluate(test_image s,  test_labels, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "metadata": {
        "id": "MKc-KWxggulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "aFxaWR7wnHac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os, argparse\n",
        "\n",
        " \n",
        " \n",
        "def grad_cam(prob_name, label, layer_name, sess, feed_dict, nb_classes):\n",
        "    \"\"\"\n",
        "    prob_name为softmax输出层节点名, label标签, layer_name最后一层卷积层的节点名, sess,                 \n",
        "    feed_dict, nb_classes分类数\n",
        "    \"\"\"\n",
        " \n",
        "    prob = sess.graph.get_tensor_by_name(prob_name + ':0')\n",
        "    print(\"prob\", prob)\n",
        "    print(\"tf.one_hot([label]\", tf.one_hot([label], nb_classes))\n",
        "    loss = tf.multiply(prob, tf.one_hot([label], nb_classes))\n",
        "    print(\"loss\", loss)\n",
        "    \n",
        "    reduced_loss = tf.reduce_sum(loss[0]) # reduced_loss = tf.reduce_sum(loss, axis=1) \n",
        "    conv_output = sess.graph.get_tensor_by_name(layer_name + ':0')\n",
        "    images = tf.placeholder(\"float\", [None, 224, 224, 3])\n",
        "    print(\"reduced_loss\", reduced_loss)\n",
        "    print(\"conv_output\", conv_output)\n",
        "    print(\"images\", images)\n",
        "    grads = tf.gradients(reduced_loss, conv_output)[0] # grads = tf.gradients(reduced_loss, conv_output)[0] # d loss / d conv\n",
        "    print(\"grads\", grads)\n",
        "    # output, grads_val = sess.run([conv_output, grads], feed_dict=feed_dict)\n",
        "    output, grads_val = sess.run([conv_output, grads], feed_dict=feed_dict)\n",
        "    # output = sess.run(conv_output, feed_dict=feed_dict)\n",
        "    # grads_val = sess.run(grads, feed_dict=feed_dict)\n",
        "    print(\"output.shape\", output.shape)\n",
        "    print(\"grads_val\", grads_val)\n",
        "    weights = np.mean(grads_val, axis=(1,2))   # weights = np.mean(grads_val, axis=(1, 2)) # average pooling\n",
        "    cams = np.sum(weights * output, axis=3)  # axis=3\n",
        "    print(\"cams\", cams)\n",
        "    return cams\n",
        " \n",
        " \n",
        "def save_cam(cam, image, save_path):\n",
        "    \"\"\"\n",
        "    save Grad-CAM images\n",
        "    \"\"\"\n",
        "    print(\"cam\", cam)\n",
        "    cam = cam[0]  # cam = cam[0] # the first GRAD-CAM for the first image in  batch\n",
        "    # image = np.uint8(image_batch[0][:, :, ::-1] * 255.0) # RGB -> BGR\n",
        "    cam = cv.resize(cam, (224, 224)) # enlarge heatmap\n",
        "    cam = np.maximum(cam, 0)\n",
        "    heatmap = cam / np.max(cam) # normalize\n",
        "    cam = cv.applyColorMap(np.uint8(255 * heatmap), cv.COLORMAP_JET) \n",
        "    print(\"cam - cv.applyColorMap\", cam)\n",
        "    print(\"heatmap\", heatmap)\n",
        "    # balck-and-white to color\n",
        "    # test for type error :\n",
        "    # print(\"np.float32(cam)\", np.float32(cam))\n",
        "    print(\"image\", image)\n",
        "    print(\"np.float32(image)\",  np.float32(image))\n",
        "    cam = np.float32(cam) + np.float32(image) # everlay heatmap onto the image\n",
        "    cam = 255 * cam / np.max(cam)\n",
        "    cam = np.uint8(cam)\n",
        "    \n",
        "    cv.imwrite(save_path+\"cam.jpg\", cam)\n",
        "    cv.imwrite(save_path+\"heatmap.jpg\", (heatmap * 255.0).astype(np.uint8))\n",
        "    cv.imwrite(save_path+\"segmentation.jpg\", (heatmap[:, :, None].astype(float) * image).astype(np.uint8))\n",
        " \n",
        "    return  cam\n",
        "\n",
        "def crop_top(img, percent=0.15):\n",
        "    offset = int(img.shape[0] * percent)\n",
        "    return img[offset:]\n",
        "\n",
        "def central_crop(img):\n",
        "    size = min(img.shape[0], img.shape[1])\n",
        "    offset_h = int((img.shape[0] - size) / 2)\n",
        "    offset_w = int((img.shape[1] - size) / 2)\n",
        "    return img[offset_h:offset_h + size, offset_w:offset_w + size]\n",
        "\n",
        "\n",
        "def process_image_file(filepath, size, top_percent=0.08, crop=True):\n",
        "    img = cv.imread(filepath)\n",
        "    # print(\"filepath\", filepath)\n",
        "    img = crop_top(img, percent=top_percent)\n",
        "    if crop:\n",
        "        img = central_crop(img)\n",
        "    img = cv.resize(img, (size, size))\n",
        "    return img\n",
        " \n",
        " \n",
        "# def main():\n",
        "IMAGE_PATH =\"assets/ex-covid.jpeg\"\n",
        "output_node_names = \"norm_dense_2/Softmax\"\n",
        "final_conv_name=\"conv5_block3_1_conv/convolution\"  #conv4_block3_1_bn/gamma/initial_value,conv4_block3_1_bn/gamma \n",
        "model_path = 'models/COVIDNet-CXR-2/model'\n",
        "\n",
        "\n",
        "# ckpt = tf.train.get_checkpoint_state(model_path)  # 通过检查点文件锁定最新的模型\n",
        "saver = tf.train.import_meta_graph(model_path + '.meta')  # 载入图结构，保存在.meta文件中\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, model_path)  # 载入参数，参数保存在两个文件中，不过restore会自己寻找\n",
        "\n",
        "\n",
        "    img_m_0 = cv.imread(IMAGE_PATH)\n",
        "    print(\"img_m1\", img_m_0)\n",
        "    img_m = cv.resize(img_m_0, (224, 224))\n",
        "\n",
        "\n",
        "    img_m = tf.cast(img_m, tf.float32)\n",
        "    print(\"img_m2\", img_m)\n",
        "    img_m = tf.reshape(img_m, [224, 224, 3])\n",
        "    print(\"img_m3\", img_m)\n",
        "    img_m_f= sess.run([img_m])\n",
        "    print(\"img_m4\", img_m)  # shape 480,480,3\n",
        "    # print(\"img_m_f\", img_m_f)  # shape 1,1,480,480,3\n",
        "    \n",
        "\n",
        "    input_image_tensor_m = sess.graph.get_tensor_by_name(\"input_1:0\")\n",
        "\n",
        "    # input_is_training_tensor = sess.graph.get_tensor_by_name(\"input/is_training:0\")\n",
        "\n",
        "    x = process_image_file('assets/ex-covid.jpeg', 480, top_percent=0.08)\n",
        "    x = x.astype('float32') / 255.0\n",
        "    # feed_dict = {input_image_tensor_m: np.expand_dims(x, axis=0)}\n",
        "    print(\"np.expand_dims(x, axis=0)\", np.expand_dims(x, axis=0))\n",
        "\n",
        "    cam=grad_cam(prob_name=output_node_names, label=0, \n",
        "                  layer_name=final_conv_name, sess=sess, \n",
        "                  feed_dict={input_image_tensor_m: np.expand_dims(x, axis=0)}, \n",
        "                  nb_classes=2)\n",
        "\n",
        "    # feed_dict={input_image_tensor_m: [img_m_f], input_is_training_tensor: False}\n",
        "    image_batch = img_m_0[None, :, :, :3]\n",
        "    image = np.uint8(image_batch[0][:, :, ::-1] * 255.0)\n",
        "\n",
        "    dst_m=save_cam(cam, image, 'm')  # cam, img_m, 'm'\n",
        "    cv.imshow('dst_m_v',dst_m)\n",
        "    cv.waitKey(0)\n",
        "\n",
        "    # print(cam)\n",
        " \n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wUlWPFb6nIqk",
        "outputId": "ca714eb9-c4a7-4bb3-d47c-f38509fbe098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_m1 [[[184 184 184]\n",
            "  [183 183 183]\n",
            "  [183 183 183]\n",
            "  ...\n",
            "  [169 169 169]\n",
            "  [169 169 169]\n",
            "  [169 169 169]]\n",
            "\n",
            " [[187 187 187]\n",
            "  [184 184 184]\n",
            "  [181 181 181]\n",
            "  ...\n",
            "  [169 169 169]\n",
            "  [169 169 169]\n",
            "  [169 169 169]]\n",
            "\n",
            " [[190 190 190]\n",
            "  [186 186 186]\n",
            "  [183 183 183]\n",
            "  ...\n",
            "  [107 107 107]\n",
            "  [107 107 107]\n",
            "  [107 107 107]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  1   1   1]\n",
            "  [  1   1   1]\n",
            "  [  1   1   1]\n",
            "  ...\n",
            "  [  1   1   1]\n",
            "  [  1   1   1]\n",
            "  [  1   1   1]]\n",
            "\n",
            " [[  1   1   1]\n",
            "  [  1   1   1]\n",
            "  [  1   1   1]\n",
            "  ...\n",
            "  [  1   1   1]\n",
            "  [  1   1   1]\n",
            "  [  1   1   1]]\n",
            "\n",
            " [[  1   1   1]\n",
            "  [  1   1   1]\n",
            "  [  1   1   1]\n",
            "  ...\n",
            "  [  1   1   1]\n",
            "  [  1   1   1]\n",
            "  [  1   1   1]]]\n",
            "img_m2 Tensor(\"Cast:0\", shape=(224, 224, 3), dtype=float32)\n",
            "img_m3 Tensor(\"Reshape:0\", shape=(224, 224, 3), dtype=float32)\n",
            "img_m4 Tensor(\"Reshape:0\", shape=(224, 224, 3), dtype=float32)\n",
            "np.expand_dims(x, axis=0) [[[[0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   ...\n",
            "   [0.3764706  0.3764706  0.3764706 ]\n",
            "   [0.34509805 0.34509805 0.34509805]\n",
            "   [0.33333334 0.33333334 0.33333334]]\n",
            "\n",
            "  [[0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   ...\n",
            "   [0.35686275 0.35686275 0.35686275]\n",
            "   [0.3529412  0.3529412  0.3529412 ]\n",
            "   [0.3529412  0.3529412  0.3529412 ]]\n",
            "\n",
            "  [[0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.         0.         0.        ]\n",
            "   ...\n",
            "   [0.3529412  0.3529412  0.3529412 ]\n",
            "   [0.34901962 0.34901962 0.34901962]\n",
            "   [0.3529412  0.3529412  0.3529412 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   ...\n",
            "   [0.25882354 0.25882354 0.25882354]\n",
            "   [0.23137255 0.23137255 0.23137255]\n",
            "   [0.20392157 0.20392157 0.20392157]]\n",
            "\n",
            "  [[0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   ...\n",
            "   [0.2509804  0.2509804  0.2509804 ]\n",
            "   [0.21960784 0.21960784 0.21960784]\n",
            "   [0.2        0.2        0.2       ]]\n",
            "\n",
            "  [[0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   ...\n",
            "   [0.24705882 0.24705882 0.24705882]\n",
            "   [0.21568628 0.21568628 0.21568628]\n",
            "   [0.2        0.2        0.2       ]]]]\n",
            "prob Tensor(\"norm_dense_2/Softmax:0\", shape=(?, 2), dtype=float32)\n",
            "tf.one_hot([label] Tensor(\"one_hot:0\", shape=(1, 2), dtype=float32)\n",
            "loss Tensor(\"Mul_1:0\", shape=(?, 2), dtype=float32)\n",
            "reduced_loss Tensor(\"Sum:0\", shape=(), dtype=float32)\n",
            "conv_output Tensor(\"conv5_block3_1_conv/convolution:0\", shape=(?, 15, 15, 304), dtype=float32)\n",
            "images Tensor(\"Placeholder:0\", shape=(?, 224, 224, 3), dtype=float32)\n",
            "grads Tensor(\"gradients/AddN_2:0\", shape=(?, 15, 15, 304), dtype=float32)\n",
            "output.shape (1, 15, 15, 304)\n",
            "grads_val [[[[-3.14525416e-04  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00 -1.19945835e-04 ...  1.07892347e-03\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00 -2.91066273e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   ...\n",
            "   [ 0.00000000e+00  0.00000000e+00  2.22340474e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  2.32855411e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  3.68257461e-05 ... -2.19962705e-04\n",
            "     0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            "  [[ 0.00000000e+00  0.00000000e+00 -8.90215160e-04 ...  1.00077933e-03\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   ...\n",
            "   [ 0.00000000e+00  0.00000000e+00  7.71735678e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  4.86834411e-04 ...  2.50375422e-04\n",
            "     0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            "  [[ 0.00000000e+00  0.00000000e+00 -1.19602544e-06 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   ...\n",
            "   [ 0.00000000e+00  0.00000000e+00  4.87139012e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00 -3.66599968e-04 ... -2.37017710e-04\n",
            "     0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  1.85568089e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   ...\n",
            "   [ 0.00000000e+00  0.00000000e+00 -8.05719697e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00 -5.79904183e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  2.62193407e-05\n",
            "     0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            "  [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  5.92484896e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   ...\n",
            "   [ 0.00000000e+00  0.00000000e+00 -6.69467088e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00 -8.95936566e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00 -6.00262429e-04 ... -9.23479165e-05\n",
            "     0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            "  [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00  2.81600142e-03 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00 -7.85064185e-04 ...  0.00000000e+00\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   ...\n",
            "   [ 0.00000000e+00  0.00000000e+00 -3.32850934e-04 ... -3.28170019e-04\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00 -5.42340858e-04 ...  5.54679136e-05\n",
            "     0.00000000e+00  0.00000000e+00]\n",
            "   [ 0.00000000e+00  0.00000000e+00 -3.85844090e-04 ...  8.01671049e-05\n",
            "     0.00000000e+00  0.00000000e+00]]]]\n",
            "cams [[[-4.55511108e-05 -2.64814335e-05 -3.49949405e-05 -5.17506196e-05\n",
            "   -1.41192286e-04 -8.30253703e-05 -9.03414621e-05 -8.67195849e-05\n",
            "   -9.90913904e-05 -1.49456318e-04 -9.60042671e-05 -5.12572369e-05\n",
            "   -6.04046691e-05 -5.78453873e-05 -6.16106554e-05]\n",
            "  [-6.32214360e-05 -1.82077474e-05 -1.98288762e-05 -1.46496081e-04\n",
            "   -1.22432772e-04 -1.27217252e-04 -3.49109250e-05  7.05549610e-05\n",
            "   -4.05613537e-05 -1.73005057e-04 -1.69164719e-04 -9.07723152e-05\n",
            "   -4.38447678e-05 -6.82519167e-05 -8.29081109e-05]\n",
            "  [ 1.48519375e-05  3.39173785e-05 -3.95160750e-05 -1.51683576e-04\n",
            "   -1.65019519e-04 -8.34851926e-06  1.04505860e-04  1.17254560e-04\n",
            "    1.35019291e-04 -6.31887960e-05 -9.74715804e-05 -5.54690196e-05\n",
            "    8.23356531e-05 -4.92105755e-06  9.21358151e-05]\n",
            "  [-2.78894386e-05 -2.97084225e-05 -9.92047717e-05 -1.25618040e-04\n",
            "   -2.15171807e-04 -7.72375424e-05  7.97323955e-05  1.41692188e-04\n",
            "    2.41527814e-04  1.52925524e-04  3.33079224e-05 -3.83786719e-05\n",
            "   -1.16048141e-05 -3.37556485e-05 -4.88102160e-06]\n",
            "  [-4.38721218e-05 -2.93694029e-05 -7.26890576e-05 -1.53782501e-04\n",
            "   -2.12291197e-04 -1.30687113e-04 -8.17609980e-05  5.14285712e-05\n",
            "    2.40444642e-04  1.07836589e-04 -7.75715162e-05 -3.88759363e-05\n",
            "   -4.91355313e-05  4.11370347e-05  5.70723932e-06]\n",
            "  [-6.96547067e-05 -4.09993299e-05 -9.28779773e-05 -1.06682259e-04\n",
            "   -1.62887591e-04 -1.68077429e-04 -1.18656753e-04  2.56394706e-05\n",
            "    2.49429635e-04  7.94394145e-05 -8.50505530e-05 -1.02674290e-04\n",
            "    1.43383349e-05  8.14503364e-05  5.02503717e-05]\n",
            "  [-6.60805963e-05 -2.98494197e-05 -5.90737909e-06 -1.24121172e-04\n",
            "   -1.70171348e-04 -9.20937164e-05  2.04540811e-05  1.40518765e-04\n",
            "    2.45168078e-04  1.76325892e-04  6.14302371e-06 -4.74600165e-06\n",
            "   -2.55098348e-05  6.93327747e-05  9.95474547e-05]\n",
            "  [-8.02316208e-05  1.19921169e-05 -4.38769057e-05 -7.54290522e-05\n",
            "    5.22292612e-05  3.00011725e-05  4.59125513e-05  1.24950922e-04\n",
            "    2.00259979e-04  1.44098813e-04  5.05191638e-05 -1.26848725e-04\n",
            "   -9.33614720e-05  8.62801389e-05  9.18090736e-05]\n",
            "  [-8.09966368e-05  7.49994069e-05  1.00217381e-04  2.22549919e-04\n",
            "    1.51245680e-04  9.35788703e-05  8.31965444e-05  1.34041446e-04\n",
            "    1.40522345e-04  1.23524820e-04  1.80053263e-04  1.10192399e-04\n",
            "    5.69544209e-05  1.73506109e-04  9.85845036e-05]\n",
            "  [ 6.84698534e-06  1.23353675e-04  4.54329769e-04  4.43685072e-04\n",
            "    3.12869030e-04  1.11275833e-04  4.89107588e-05  1.52813853e-04\n",
            "    1.42823774e-04  2.04028140e-04  2.10403741e-04  2.30327249e-04\n",
            "    2.84317241e-04  3.50701797e-04  1.33965485e-04]\n",
            "  [ 2.31256108e-05  1.16819108e-04  3.65607499e-04  2.96596700e-04\n",
            "    1.81317126e-04  2.10270446e-04  2.18663641e-04  1.58287017e-04\n",
            "    1.66502679e-04  1.93688393e-04  2.05694305e-04  1.45090307e-04\n",
            "    1.05061968e-04  2.02292722e-04  6.43085950e-05]\n",
            "  [-6.85689156e-06  9.23517218e-05  1.56336362e-04  6.06582907e-05\n",
            "    5.54770704e-05  1.31171357e-04  1.22576559e-04  1.25862061e-04\n",
            "    1.50500389e-04  1.70716536e-04  1.73296547e-04  1.18429409e-04\n",
            "    7.29995372e-05  1.02917416e-04  2.13511521e-05]\n",
            "  [-4.55869013e-05 -2.78158550e-05  7.63258795e-05  1.02758910e-04\n",
            "    8.92934622e-06 -6.82648970e-05 -3.99360360e-05  8.53715392e-06\n",
            "    4.35359398e-05  3.37510937e-05  3.28430215e-05  1.13919668e-07\n",
            "    8.66647169e-05  1.23337726e-04  1.39185577e-05]\n",
            "  [-2.13487765e-05  1.39109761e-05  1.29065244e-04  1.92717329e-04\n",
            "    9.72196067e-05  9.31245449e-06  5.43277565e-07  4.20752585e-05\n",
            "    1.20696932e-04  1.03942068e-04  7.00717792e-05  6.15079625e-05\n",
            "    7.86310702e-05  7.40006653e-05  4.59062867e-05]\n",
            "  [ 5.86586793e-05  9.85545194e-05  1.51125336e-04  1.47729850e-04\n",
            "    1.56947295e-04  1.29974243e-04  7.56460067e-05  1.20785422e-04\n",
            "    1.48585401e-04  1.38298376e-04  2.03752657e-04  1.24244325e-04\n",
            "    1.22964542e-04  1.07280925e-04  9.81652774e-05]]]\n",
            "cam [[[-4.55511108e-05 -2.64814335e-05 -3.49949405e-05 -5.17506196e-05\n",
            "   -1.41192286e-04 -8.30253703e-05 -9.03414621e-05 -8.67195849e-05\n",
            "   -9.90913904e-05 -1.49456318e-04 -9.60042671e-05 -5.12572369e-05\n",
            "   -6.04046691e-05 -5.78453873e-05 -6.16106554e-05]\n",
            "  [-6.32214360e-05 -1.82077474e-05 -1.98288762e-05 -1.46496081e-04\n",
            "   -1.22432772e-04 -1.27217252e-04 -3.49109250e-05  7.05549610e-05\n",
            "   -4.05613537e-05 -1.73005057e-04 -1.69164719e-04 -9.07723152e-05\n",
            "   -4.38447678e-05 -6.82519167e-05 -8.29081109e-05]\n",
            "  [ 1.48519375e-05  3.39173785e-05 -3.95160750e-05 -1.51683576e-04\n",
            "   -1.65019519e-04 -8.34851926e-06  1.04505860e-04  1.17254560e-04\n",
            "    1.35019291e-04 -6.31887960e-05 -9.74715804e-05 -5.54690196e-05\n",
            "    8.23356531e-05 -4.92105755e-06  9.21358151e-05]\n",
            "  [-2.78894386e-05 -2.97084225e-05 -9.92047717e-05 -1.25618040e-04\n",
            "   -2.15171807e-04 -7.72375424e-05  7.97323955e-05  1.41692188e-04\n",
            "    2.41527814e-04  1.52925524e-04  3.33079224e-05 -3.83786719e-05\n",
            "   -1.16048141e-05 -3.37556485e-05 -4.88102160e-06]\n",
            "  [-4.38721218e-05 -2.93694029e-05 -7.26890576e-05 -1.53782501e-04\n",
            "   -2.12291197e-04 -1.30687113e-04 -8.17609980e-05  5.14285712e-05\n",
            "    2.40444642e-04  1.07836589e-04 -7.75715162e-05 -3.88759363e-05\n",
            "   -4.91355313e-05  4.11370347e-05  5.70723932e-06]\n",
            "  [-6.96547067e-05 -4.09993299e-05 -9.28779773e-05 -1.06682259e-04\n",
            "   -1.62887591e-04 -1.68077429e-04 -1.18656753e-04  2.56394706e-05\n",
            "    2.49429635e-04  7.94394145e-05 -8.50505530e-05 -1.02674290e-04\n",
            "    1.43383349e-05  8.14503364e-05  5.02503717e-05]\n",
            "  [-6.60805963e-05 -2.98494197e-05 -5.90737909e-06 -1.24121172e-04\n",
            "   -1.70171348e-04 -9.20937164e-05  2.04540811e-05  1.40518765e-04\n",
            "    2.45168078e-04  1.76325892e-04  6.14302371e-06 -4.74600165e-06\n",
            "   -2.55098348e-05  6.93327747e-05  9.95474547e-05]\n",
            "  [-8.02316208e-05  1.19921169e-05 -4.38769057e-05 -7.54290522e-05\n",
            "    5.22292612e-05  3.00011725e-05  4.59125513e-05  1.24950922e-04\n",
            "    2.00259979e-04  1.44098813e-04  5.05191638e-05 -1.26848725e-04\n",
            "   -9.33614720e-05  8.62801389e-05  9.18090736e-05]\n",
            "  [-8.09966368e-05  7.49994069e-05  1.00217381e-04  2.22549919e-04\n",
            "    1.51245680e-04  9.35788703e-05  8.31965444e-05  1.34041446e-04\n",
            "    1.40522345e-04  1.23524820e-04  1.80053263e-04  1.10192399e-04\n",
            "    5.69544209e-05  1.73506109e-04  9.85845036e-05]\n",
            "  [ 6.84698534e-06  1.23353675e-04  4.54329769e-04  4.43685072e-04\n",
            "    3.12869030e-04  1.11275833e-04  4.89107588e-05  1.52813853e-04\n",
            "    1.42823774e-04  2.04028140e-04  2.10403741e-04  2.30327249e-04\n",
            "    2.84317241e-04  3.50701797e-04  1.33965485e-04]\n",
            "  [ 2.31256108e-05  1.16819108e-04  3.65607499e-04  2.96596700e-04\n",
            "    1.81317126e-04  2.10270446e-04  2.18663641e-04  1.58287017e-04\n",
            "    1.66502679e-04  1.93688393e-04  2.05694305e-04  1.45090307e-04\n",
            "    1.05061968e-04  2.02292722e-04  6.43085950e-05]\n",
            "  [-6.85689156e-06  9.23517218e-05  1.56336362e-04  6.06582907e-05\n",
            "    5.54770704e-05  1.31171357e-04  1.22576559e-04  1.25862061e-04\n",
            "    1.50500389e-04  1.70716536e-04  1.73296547e-04  1.18429409e-04\n",
            "    7.29995372e-05  1.02917416e-04  2.13511521e-05]\n",
            "  [-4.55869013e-05 -2.78158550e-05  7.63258795e-05  1.02758910e-04\n",
            "    8.92934622e-06 -6.82648970e-05 -3.99360360e-05  8.53715392e-06\n",
            "    4.35359398e-05  3.37510937e-05  3.28430215e-05  1.13919668e-07\n",
            "    8.66647169e-05  1.23337726e-04  1.39185577e-05]\n",
            "  [-2.13487765e-05  1.39109761e-05  1.29065244e-04  1.92717329e-04\n",
            "    9.72196067e-05  9.31245449e-06  5.43277565e-07  4.20752585e-05\n",
            "    1.20696932e-04  1.03942068e-04  7.00717792e-05  6.15079625e-05\n",
            "    7.86310702e-05  7.40006653e-05  4.59062867e-05]\n",
            "  [ 5.86586793e-05  9.85545194e-05  1.51125336e-04  1.47729850e-04\n",
            "    1.56947295e-04  1.29974243e-04  7.56460067e-05  1.20785422e-04\n",
            "    1.48585401e-04  1.38298376e-04  2.03752657e-04  1.24244325e-04\n",
            "    1.22964542e-04  1.07280925e-04  9.81652774e-05]]]\n",
            "cam - cv.applyColorMap [[[128   0   0]\n",
            "  [128   0   0]\n",
            "  [128   0   0]\n",
            "  ...\n",
            "  [128   0   0]\n",
            "  [128   0   0]\n",
            "  [128   0   0]]\n",
            "\n",
            " [[128   0   0]\n",
            "  [128   0   0]\n",
            "  [128   0   0]\n",
            "  ...\n",
            "  [128   0   0]\n",
            "  [128   0   0]\n",
            "  [128   0   0]]\n",
            "\n",
            " [[128   0   0]\n",
            "  [128   0   0]\n",
            "  [128   0   0]\n",
            "  ...\n",
            "  [128   0   0]\n",
            "  [128   0   0]\n",
            "  [128   0   0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255   4   0]\n",
            "  [255   4   0]\n",
            "  [255   4   0]\n",
            "  ...\n",
            "  [255  92   0]\n",
            "  [255  92   0]\n",
            "  [255  92   0]]\n",
            "\n",
            " [[255   4   0]\n",
            "  [255   4   0]\n",
            "  [255   4   0]\n",
            "  ...\n",
            "  [255  92   0]\n",
            "  [255  92   0]\n",
            "  [255  92   0]]\n",
            "\n",
            " [[255   4   0]\n",
            "  [255   4   0]\n",
            "  [255   4   0]\n",
            "  ...\n",
            "  [255  92   0]\n",
            "  [255  92   0]\n",
            "  [255  92   0]]]\n",
            "heatmap [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.1302309  0.1302309  0.1302309  ... 0.21794137 0.21794137 0.21794137]\n",
            " [0.1302309  0.1302309  0.1302309  ... 0.21794137 0.21794137 0.21794137]\n",
            " [0.1302309  0.1302309  0.1302309  ... 0.21794137 0.21794137 0.21794137]]\n",
            "image [[[ 72  72  72]\n",
            "  [ 73  73  73]\n",
            "  [ 73  73  73]\n",
            "  ...\n",
            "  [ 87  87  87]\n",
            "  [ 87  87  87]\n",
            "  [ 87  87  87]]\n",
            "\n",
            " [[ 69  69  69]\n",
            "  [ 72  72  72]\n",
            "  [ 75  75  75]\n",
            "  ...\n",
            "  [ 87  87  87]\n",
            "  [ 87  87  87]\n",
            "  [ 87  87  87]]\n",
            "\n",
            " [[ 66  66  66]\n",
            "  [ 70  70  70]\n",
            "  [ 73  73  73]\n",
            "  ...\n",
            "  [149 149 149]\n",
            "  [149 149 149]\n",
            "  [149 149 149]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]\n",
            "\n",
            " [[255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  ...\n",
            "  [255 255 255]\n",
            "  [255 255 255]\n",
            "  [255 255 255]]]\n",
            "np.float32(image) [[[ 72.  72.  72.]\n",
            "  [ 73.  73.  73.]\n",
            "  [ 73.  73.  73.]\n",
            "  ...\n",
            "  [ 87.  87.  87.]\n",
            "  [ 87.  87.  87.]\n",
            "  [ 87.  87.  87.]]\n",
            "\n",
            " [[ 69.  69.  69.]\n",
            "  [ 72.  72.  72.]\n",
            "  [ 75.  75.  75.]\n",
            "  ...\n",
            "  [ 87.  87.  87.]\n",
            "  [ 87.  87.  87.]\n",
            "  [ 87.  87.  87.]]\n",
            "\n",
            " [[ 66.  66.  66.]\n",
            "  [ 70.  70.  70.]\n",
            "  [ 73.  73.  73.]\n",
            "  ...\n",
            "  [149. 149. 149.]\n",
            "  [149. 149. 149.]\n",
            "  [149. 149. 149.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[255. 255. 255.]\n",
            "  [255. 255. 255.]\n",
            "  [255. 255. 255.]\n",
            "  ...\n",
            "  [255. 255. 255.]\n",
            "  [255. 255. 255.]\n",
            "  [255. 255. 255.]]\n",
            "\n",
            " [[255. 255. 255.]\n",
            "  [255. 255. 255.]\n",
            "  [255. 255. 255.]\n",
            "  ...\n",
            "  [255. 255. 255.]\n",
            "  [255. 255. 255.]\n",
            "  [255. 255. 255.]]\n",
            "\n",
            " [[255. 255. 255.]\n",
            "  [255. 255. 255.]\n",
            "  [255. 255. 255.]\n",
            "  ...\n",
            "  [255. 255. 255.]\n",
            "  [255. 255. 255.]\n",
            "  [255. 255. 255.]]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-675c078a9f1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mdst_m\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'm'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# cam, img_m, 'm'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dst_m_v'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdst_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-675c078a9f1f>\u001b[0m in \u001b[0;36msave_cam\u001b[0;34m(cam, image, save_path)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"np.float32(image)\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# everlay heatmap onto the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcam\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (224,224,3) (659,651,3) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o08IF8gjxd0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
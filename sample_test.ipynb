{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yXBTtPVJDnCqtYdX10Ma8izpcsTxZTMR",
      "authorship_tag": "ABX9TyO2YCUHkadXAheIKAFPDq9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jellyXuuuuu/CovidNetDeepLearning/blob/main/sample_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp 'drive/MyDrive/covid/requirements.txt' ."
      ],
      "metadata": {
        "id": "ka3aaDvyTbW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install -r requirements.txt"
      ],
      "metadata": {
        "id": "Mkneg79ITy9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw_ZL_iCLX2l",
        "outputId": "6b03a6a8-362f-48b2-8153-606ba44f8a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-gdcm in /usr/local/lib/python3.7/dist-packages (3.0.19)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install python-gdcm\n",
        "!pip3 install pydicom"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m32Q0IImVN8o",
        "outputId": "e3d6e0b3-3252-4770-b4a4-5df8ce74c1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.7/dist-packages (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.49.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (2.0.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.9.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import pydicom\n",
        "# import dicom\n",
        "from pydicom.pixel_data_handlers import apply_modality_lut, apply_voi_lut\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AkT__BReLfUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.py\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def crop_top(img, percent=0.15):\n",
        "    offset = int(img.shape[0] * percent)\n",
        "    return img[offset:]\n",
        "\n",
        "def central_crop(img):\n",
        "    size = min(img.shape[0], img.shape[1])\n",
        "    offset_h = int((img.shape[0] - size) / 2)\n",
        "    offset_w = int((img.shape[1] - size) / 2)\n",
        "    return img[offset_h:offset_h + size, offset_w:offset_w + size]\n",
        "\n",
        "def process_image_file(filepath, size, top_percent=0.08, crop=True):\n",
        "    img = cv2.imread(filepath)\n",
        "    # print(\"filepath\", filepath)\n",
        "    img = crop_top(img, percent=top_percent)\n",
        "    if crop:\n",
        "        img = central_crop(img)\n",
        "    img = cv2.resize(img, (size, size))\n",
        "    return img\n",
        "\n",
        "def process_image_file_medusa(filepath, size):\n",
        "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, (size, size))\n",
        "    img = img.astype('float64')\n",
        "    img -= img.mean()\n",
        "    img /= img.std()\n",
        "    return np.expand_dims(img, -1)\n",
        "\n",
        "def random_ratio_resize(img, prob=0.3, delta=0.1):\n",
        "    if np.random.rand() >= prob:\n",
        "        return img\n",
        "    ratio = img.shape[0] / img.shape[1]\n",
        "    ratio = np.random.uniform(max(ratio - delta, 0.01), ratio + delta)\n",
        "\n",
        "    if ratio * img.shape[1] <= img.shape[1]:\n",
        "        size = (int(img.shape[1] * ratio), img.shape[1])\n",
        "    else:\n",
        "        size = (img.shape[0], int(img.shape[0] / ratio))\n",
        "\n",
        "    dh = img.shape[0] - size[1]\n",
        "    top, bot = dh // 2, dh - dh // 2\n",
        "    dw = img.shape[1] - size[0]\n",
        "    left, right = dw // 2, dw - dw // 2\n",
        "\n",
        "    if size[0] > 480 or size[1] > 480:\n",
        "        print(img.shape, size, ratio)\n",
        "\n",
        "    img = cv2.resize(img, size)\n",
        "    img = cv2.copyMakeBorder(img, top, bot, left, right, cv2.BORDER_CONSTANT,\n",
        "                             (0, 0, 0))\n",
        "\n",
        "    if img.shape[0] != 480 or img.shape[1] != 480:\n",
        "        raise ValueError(img.shape, size)\n",
        "    return img\n",
        "\n",
        "_augmentation_transform = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=(0.9, 1.1),\n",
        "    zoom_range=(0.85, 1.15),\n",
        "    fill_mode='constant',\n",
        "    cval=0.,\n",
        ")\n",
        "\n",
        "def apply_augmentation(img):\n",
        "    img = random_ratio_resize(img)\n",
        "    img = _augmentation_transform.random_transform(img)\n",
        "    return img\n",
        "\n",
        "def _process_csv_file(file):\n",
        "    with open(file, 'r') as fr:\n",
        "        files = fr.readlines()\n",
        "    return files\n",
        "\n",
        "\n",
        "class BalanceCovidDataset(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            data_dir,\n",
        "            csv_file,\n",
        "            is_training=True,\n",
        "            batch_size=8,\n",
        "            medusa_input_shape=(256, 256),\n",
        "            input_shape=(480, 480),\n",
        "            n_classes=2,\n",
        "            num_channels=3,\n",
        "            mapping={\n",
        "                'negative': 0,\n",
        "                'positive': 1,\n",
        "            },\n",
        "            shuffle=True,\n",
        "            augmentation=apply_augmentation,\n",
        "            covid_percent=0.5,\n",
        "            class_weights=[1., 1.],\n",
        "            top_percent=0.08,\n",
        "            is_severity_model=False,\n",
        "            is_medusa_backbone=False,\n",
        "    ):\n",
        "        'Initialization'\n",
        "        self.datadir = data_dir\n",
        "        self.dataset = _process_csv_file(csv_file)\n",
        "        self.is_training = is_training\n",
        "        self.batch_size = batch_size\n",
        "        self.N = len(self.dataset)\n",
        "        self.medusa_input_shape = medusa_input_shape\n",
        "        self.input_shape = input_shape\n",
        "        self.n_classes = n_classes\n",
        "        self.num_channels = num_channels\n",
        "        self.mapping = mapping\n",
        "        self.shuffle = shuffle\n",
        "        self.covid_percent = covid_percent\n",
        "        self.class_weights = class_weights\n",
        "        self.n = 0\n",
        "        self.augmentation = augmentation\n",
        "        self.top_percent = top_percent\n",
        "        self.is_severity_model = is_severity_model\n",
        "        self.is_medusa_backbone = is_medusa_backbone\n",
        "\n",
        "        # If using MEDUSA backbone load images without crop\n",
        "        if self.is_medusa_backbone:\n",
        "            self.load_image = partial(process_image_file, top_percent=0, crop=False)\n",
        "        else:\n",
        "            self.load_image = process_image_file\n",
        "\n",
        "        datasets = {}\n",
        "        for key in self.mapping.keys():\n",
        "            datasets[key] = []\n",
        "\n",
        "        for l in self.dataset:\n",
        "            datasets[l.split()[2]].append(l)\n",
        "        \n",
        "        if self.is_severity_model:\n",
        "            self.datasets = [\n",
        "                datasets['level2'], datasets['level1']\n",
        "            ]\n",
        "        elif self.n_classes == 2:\n",
        "            self.datasets = [\n",
        "                datasets['negative'], datasets['positive']\n",
        "            ]\n",
        "        elif self.n_classes == 3:\n",
        "            self.datasets = [\n",
        "                datasets['normal'] + datasets['pneumonia'],\n",
        "                datasets['COVID-19'],\n",
        "            ]\n",
        "        else:\n",
        "            raise Exception('Only binary or 3 class classification currently supported.')\n",
        "        print(len(self.datasets[0]), len(self.datasets[1]))\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __next__(self):\n",
        "        # Get one batch of data\n",
        "        model_inputs = self.__getitem__(self.n)\n",
        "        # Batch index\n",
        "        self.n += 1\n",
        "\n",
        "        # If we have processed the entire dataset then\n",
        "        if self.n >= self.__len__():\n",
        "            self.on_epoch_end()\n",
        "            self.n = 0\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.datasets[0]) / float(self.batch_size)))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        if self.shuffle == True:\n",
        "            for v in self.datasets:\n",
        "                np.random.shuffle(v)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = np.zeros((self.batch_size, *self.input_shape, self.num_channels))\n",
        "        batch_y = np.zeros(self.batch_size)\n",
        "\n",
        "        if self.is_medusa_backbone:\n",
        "            batch_sem_x = np.zeros((self.batch_size, *self.medusa_input_shape, 1))\n",
        "\n",
        "        batch_files = self.datasets[0][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        # upsample covid cases\n",
        "        covid_size = max(int(len(batch_files) * self.covid_percent), 1)\n",
        "        covid_inds = np.random.choice(np.arange(len(batch_files)),\n",
        "                                      size=covid_size,\n",
        "                                      replace=False)\n",
        "        covid_files = np.random.choice(self.datasets[1],\n",
        "                                       size=covid_size,\n",
        "                                       replace=False)\n",
        "        for i in range(covid_size):\n",
        "            batch_files[covid_inds[i]] = covid_files[i]\n",
        "\n",
        "        for i in range(len(batch_files)):\n",
        "            sample = batch_files[i].split()\n",
        "\n",
        "            if self.is_training:\n",
        "                folder = 'train'\n",
        "            else:\n",
        "                folder = 'test'\n",
        "\n",
        "            image_file = os.path.join(self.datadir, folder, sample[1])\n",
        "            x = self.load_image(\n",
        "                image_file,\n",
        "                self.input_shape[0],\n",
        "                top_percent=self.top_percent,\n",
        "            )\n",
        "\n",
        "            if self.is_training and hasattr(self, 'augmentation'):\n",
        "                x = self.augmentation(x)\n",
        "\n",
        "            x = x.astype('float32') / 255.0\n",
        "\n",
        "            if self.is_medusa_backbone:\n",
        "                sem_x = process_image_file_medusa(image_file, self.medusa_input_shape[0])\n",
        "                batch_sem_x[i] = sem_x\n",
        "            \n",
        "            y = self.mapping[sample[2]]\n",
        "\n",
        "            batch_x[i] = x\n",
        "            batch_y[i] = y\n",
        "\n",
        "        class_weights = self.class_weights\n",
        "        weights = np.take(class_weights, batch_y.astype('int64'))\n",
        "        batch_y = keras.utils.to_categorical(batch_y, num_classes=self.n_classes)\n",
        "\n",
        "        if self.is_medusa_backbone:\n",
        "            return batch_sem_x, batch_x, batch_y, weights, self.is_training\n",
        "        else:\n",
        "            return batch_x, batch_y, weights, self.is_training\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19x8J7S1SbCC",
        "outputId": "5785eebf-321a-4e67-d09f-898791055d6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r 'drive/MyDrive/covid/labels/' ."
      ],
      "metadata": {
        "id": "OFgXNpMmSpSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r 'drive/MyDrive/covid/models/' ."
      ],
      "metadata": {
        "id": "ira5CKpiV3MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os, argparse\n",
        "\n",
        "# from data import (\n",
        "#     process_image_file, \n",
        "#     process_image_file_medusa,\n",
        "# )\n",
        "\n",
        "# To remove TF Warnings\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "\n",
        "def print_metrics(y_test, pred, mapping):\n",
        "    matrix = confusion_matrix(y_test, pred)\n",
        "    matrix = matrix.astype('float')\n",
        "    print(matrix)\n",
        "\n",
        "    class_acc = [matrix[i,i]/np.sum(matrix[i,:]) if np.sum(matrix[i,:]) else 0 for i in range(len(matrix))]\n",
        "    ppvs = [matrix[i,i]/np.sum(matrix[:,i]) if np.sum(matrix[:,i]) else 0 for i in range(len(matrix))]\n",
        "\n",
        "    print('Sens', ', '.join('{}: {:.3f}'.format(cls.capitalize(), class_acc[i]) for cls, i in mapping.items()))\n",
        "    print('PPV', ', '.join('{}: {:.3f}'.format(cls.capitalize(), ppvs[i]) for cls, i in mapping.items()))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # args = []\n",
        "    '''\n",
        "    python eval.py \\\n",
        "    --weightspath models/COVIDNet-CXR-3 \\\n",
        "    --metaname model.meta \\\n",
        "    --ckptname model \\\n",
        "    --n_classes 2 \\\n",
        "    --testfile labels/test_COVIDx9B.txt \\\n",
        "    --out_tensorname softmax/Softmax:0 \\\n",
        "    --is_medusa_backbone\n",
        "    '''\n",
        "\n",
        "    args_weightspath = 'models/COVIDNet-CXR-2' \n",
        "    args_metaname = 'model.meta'\n",
        "    args_ckptname = 'model'\n",
        "    args_n_classes = 2\n",
        "\n",
        "    args_testfolder = 'data/test'\n",
        "\n",
        "    args_input_size = 480\n",
        "\n",
        "    args_trainfile = 'labels/train_COVIDx9B.txt'\n",
        "    args_testfile = 'labels/test_COVIDx9B.txt'\n",
        "    args_in_tensorname = 'input_1:0'\n",
        "    args_out_tensorname = 'norm_dense_2/Softmax:0'\n",
        "    args_logit_tensorname = 'norm_dense_2/MatMul:0'\n",
        "    args_is_severity_model = True\n",
        "    args_is_medusa_backbone = True\n",
        "    \n",
        "    sess = tf.compat.v1.Session()\n",
        "    # sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "    # tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "    # tf.get_default_graph()\n",
        "    tf.compat.v1.get_default_graph()\n",
        "    saver = tf.compat.v1.train.import_meta_graph(os.path.join(args_weightspath, args_metaname))\n",
        "    saver.restore(sess, os.path.join(args_weightspath, args_ckptname))\n",
        "\n",
        "    graph = tf.get_default_graph()\n",
        "\n",
        "    # print(\"GRAPH: -------------------\")\n",
        "    # for op in graph.get_operations():\n",
        "    #   print(op.name)\n",
        "\n",
        "    file = open(args_testfile, 'r')\n",
        "    testfile = file.readlines()\n",
        "\n",
        "    if args_is_severity_model:\n",
        "        # For COVIDNet CXR-S training with COVIDxSev level 1 and level 2 air space seveirty grading\n",
        "        mapping = {\n",
        "            'level2': 0,\n",
        "            'level1': 1\n",
        "        }\n",
        "    elif args_n_classes == 2:\n",
        "        # For COVID-19 positive/negative detection\n",
        "        mapping = {\n",
        "            'negative': 0,\n",
        "            'positive': 1,\n",
        "        }\n",
        "    elif args_n_classes == 3:\n",
        "        # For detection of no pneumonia/non-COVID-19 pneumonia/COVID-19 pneumonia\n",
        "        mapping = {\n",
        "            'normal': 0,\n",
        "            'pneumonia': 1,\n",
        "            'COVID-19': 2\n",
        "        }\n",
        "    else:\n",
        "        raise Exception('''COVID-Net currently only supports 2 class COVID-19 positive/negative detection\n",
        "            or 3 class detection of no pneumonia/non-COVID-19 pneumonia/COVID-19 pneumonia''')\n",
        "\n",
        "\n",
        "\n",
        "    # //////////////////////////////////\n",
        "\n",
        "\n",
        "    y_test = []\n",
        "    pred = []\n",
        "\n",
        "    # for i in range(len(testfile)):\n",
        "    line = 0\n",
        "    # image_file = os.path.join(testfolder, line[1])\n",
        "    image_file = os.path.join(args_testfolder, \"0a8d486f-1aa6-4fcf-b7be-4bf04fc8628b.png\")\n",
        "    print(\"testfolder line1, image\", image_file)\n",
        "    \n",
        "    # y_test.append(mapping[line[2]])\n",
        "\n",
        "    # if args.is_medusa_backbone:\n",
        "    #     x = process_image_file(image_file, args.input_size, top_percent=0, crop=False)\n",
        "    #     x = x.astype('float32') / 255.0\n",
        "    #     medusa_x = process_image_file_medusa(image_file, args.input_size_medusa)\n",
        "    #     data_tensor = tf.get_default_graph().get_tensor_by_name(args.in_tensorname_medusa)\n",
        "    #     feed_dict = {\n",
        "    #         data_tensor: np.expand_dims(medusa_x, axis=0),   # /////////////////////????input_1:0\n",
        "    #         args.in_tensorname: np.expand_dims(x, axis=0),\n",
        "    #     }\n",
        "\n",
        "    #     print(\"is_medusa_backbone\")\n",
        "\n",
        "    # else:\n",
        "    x = process_image_file(image_file, args_input_size, top_percent=0.08)\n",
        "    x = x.astype('float32') / 255.0\n",
        "    data_tensor = tf.get_default_graph().get_tensor_by_name(\"input_1:0\")  # /////////////////////????input_2:0\n",
        "    feed_dict = {data_tensor: np.expand_dims(x, axis=0)}\n",
        "\n",
        "    # print(\"Not is_medusa_backbone\")\n",
        "\n",
        "    print(\"feed_dict\", feed_dict)\n",
        "    \n",
        "    \n",
        "    pred.append(np.array(sess.run(args_out_tensorname, feed_dict=feed_dict)).argmax(axis=1))  ############\n",
        "    print(\"pred\", pred)\n",
        "    print(\"mapping\", mapping)\n",
        "    \n",
        "    # y_test = np.array(y_test)\n",
        "    # pred = np.array(pred)\n",
        "\n",
        "    # print_metrics(y_test, pred, mapping)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpUH23ZFQ77s",
        "outputId": "f70da0eb-ccb1-4be5-9f3e-b97a68cab8eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testfolder line1, image data/test/0a8d486f-1aa6-4fcf-b7be-4bf04fc8628b.png\n",
            "feed_dict {<tf.Tensor 'input_1:0' shape=(?, 480, 480, 3) dtype=float32>: array([[[[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.00784314, 0.00784314, 0.00784314],\n",
            "         ...,\n",
            "         [0.01568628, 0.01568628, 0.01568628],\n",
            "         [0.01568628, 0.01568628, 0.01568628],\n",
            "         [0.01568628, 0.01568628, 0.01568628]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.00784314, 0.00784314, 0.00784314],\n",
            "         ...,\n",
            "         [0.01568628, 0.01568628, 0.01568628],\n",
            "         [0.01568628, 0.01568628, 0.01568628],\n",
            "         [0.01568628, 0.01568628, 0.01568628]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.00784314, 0.00784314, 0.00784314],\n",
            "         ...,\n",
            "         [0.01568628, 0.01568628, 0.01568628],\n",
            "         [0.01568628, 0.01568628, 0.01568628],\n",
            "         [0.01568628, 0.01568628, 0.01568628]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.03137255, 0.03137255, 0.03137255],\n",
            "         [0.01568628, 0.01568628, 0.01568628],\n",
            "         [0.00784314, 0.00784314, 0.00784314]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.02352941, 0.02352941, 0.02352941],\n",
            "         [0.01568628, 0.01568628, 0.01568628],\n",
            "         [0.01176471, 0.01176471, 0.01176471]],\n",
            "\n",
            "        [[0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         [0.        , 0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.01176471, 0.01176471, 0.01176471],\n",
            "         [0.00784314, 0.00784314, 0.00784314],\n",
            "         [0.00784314, 0.00784314, 0.00784314]]]], dtype=float32)}\n",
            "pred [array([0])]\n",
            "mapping {'level2': 0, 'level1': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7RPiCUtUuoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print( tf.__version__ )"
      ],
      "metadata": {
        "id": "aei1EgnVU938"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JbSdegjgVtFg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}